{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Pelagia documentation","text":""},{"location":"#introduction","title":"Introduction","text":"<p>The Pelagia Controller is a Kubernetes operator that implements lifecycle management for Ceph clusters managed by Rook.</p> <p>The Pelagia is written in Go lang using Cluster API to build Kubernetes controllers.</p> <p>Pelagia solution provides two main controllers:</p> <ul> <li>Deployment Controller monitors changes in the <code>CephDeployment</code> Kubernetes custom resource    and handles these changes by creating, updating, or deleting appropriate resources in Kubernetes.</li> <li>Ceph OSD Lifecycle Management Controller monitors changes in the   <code>CephOsdRemoveTask</code> custom resource and runs automated Ceph OSD disk or node removal.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>To get started with Pelagia, follow the Quick Start Guide.</p> <p>To install Pelagia in automated LCM mode only, follow the Quick Start LCM-only Guide.</p>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>File a bug: https://github.com/Mirantis/pelagia/issues</li> <li>Discuss with us: https://github.com/Mirantis/pelagia/discussions</li> </ul>"},{"location":"#developer-resources","title":"Developer Resources","text":"<ul> <li>Contributing: https://github.com/Mirantis/pelagia/pulls</li> <li>Developer Guide: https://mirantis.github.io/pelagia/developer/</li> <li>Reference Architecture: https://mirantis.github.io/pelagia</li> </ul>"},{"location":"developer/","title":"Developer Guide","text":"<p>Pelagia uses Makefile to build, test, and deploy the controller. This document provides a guide for developers with the most used Makefile targets.</p>"},{"location":"developer/#code-style","title":"Code style","text":"<p>Pelagia uses golangci-lint code formatter. To check your changes and format them: <pre><code>make check\n</code></pre></p>"},{"location":"developer/#tests","title":"Tests","text":"<p>Each commit requires all <code>PASS</code> unit tests. To run unit tests locally: <pre><code>make unit\n</code></pre></p>"},{"location":"developer/#build-the-controller-image-and-chart","title":"Build the controller image and chart","text":"<p>Pelagia is deployed as a Helm chart into the Kubernetes cluster.</p> <p>To build the controller image for the <code>linux/amd64</code> platform: <pre><code>make build image\n</code></pre></p> <p>To build the Helm chart after the controller image is built: <pre><code>make\n</code></pre></p>"},{"location":"architecture/addressing-ceph-devices/","title":"Addressing Ceph storage devices","text":"<p>There are several formats to use when specifying and addressing storage devices of a Ceph cluster. The default and recommended one is the <code>/dev/disk/by-id</code> format. This format is reliable and unaffected by the disk controller actions, for example, device name shuffling on boot.</p>"},{"location":"architecture/addressing-ceph-devices/#difference-between-by-id-name-and-by-path-formats","title":"Difference between <code>by-id</code>, <code>name</code>, and <code>by-path</code> formats","text":"<p>The storage device <code>/dev/disk/by-id</code> format mostly bases on a disk serial number, which is unique for each disk. A <code>by-id</code> symlink is created by the <code>udev</code> rules in the following format, where <code>&lt;BusID&gt;</code> is an ID of the bus to which the disk is attached and <code>&lt;DiskSerialNumber&gt;</code> stands for a unique disk serial number:</p> <pre><code>/dev/disk/by-id/&lt;BusID&gt;-&lt;DiskSerialNumber&gt;\n</code></pre> <p>Typical <code>by-id</code> symlinks for storage devices look as follows:</p> <pre><code>/dev/disk/by-id/nvme-SAMSUNG_MZ1LB3T8HMLA-00007_S46FNY0R394543\n/dev/disk/by-id/scsi-SATA_HGST_HUS724040AL_PN1334PEHN18ZS\n/dev/disk/by-id/ata-WDC_WD4003FZEX-00Z4SA0_WD-WMC5D0D9DMEH\n</code></pre> <p>In the example above, symlinks contain the following IDs:</p> <ul> <li>Bus IDs: <code>nvme</code>, <code>scsi-SATA</code> and <code>ata</code></li> <li>Disk serial numbers: <code>SAMSUNG_MZ1LB3T8HMLA-00007_S46FNY0R394543</code>,   <code>HGST_HUS724040AL_PN1334PEHN18ZS</code> and   <code>WDC_WD4003FZEX-00Z4SA0_WD-WMC5D0D9DMEH</code>.</li> </ul> <p>An exception to this rule is the <code>wwn</code> <code>by-id</code> symlinks, which are programmatically generated at boot. They are not solely based on disk serial numbers but also include other node information. This can lead to the <code>wwn</code> being recalculated when the node reboots. As a result, this symlink type cannot guarantee a persistent disk identifier and should not be used as a stable storage device symlink in a Ceph cluster.</p> <p>The storage device <code>name</code> format cannot be considered persistent because the sequence in which block devices are added during boot is semi-arbitrary. This means that block device names, for example, <code>nvme0n1</code> and <code>sdc</code>, are assigned to physical disks during discovery, which may vary inconsistently from the previous node state.</p> <p>The storage device <code>by-path</code> format is supported, but we recommend using <code>by-id</code> symlinks instead of <code>by-path</code> symlinks due to <code>by-id</code> symlinks directly refer to the disk serial number.</p> <p>Therefore, we are highly recommending using storage device <code>by-id</code> symlinks that contain disk serial numbers. This approach enables you to use a persistent device identifier addressed in the Ceph cluster specification.</p>"},{"location":"architecture/addressing-ceph-devices/#example-cephdeployment-with-device-by-id-identifiers","title":"Example <code>CephDeployment</code> with device <code>by-id</code> identifiers","text":"<p>Below is an example <code>CephDeployment</code> custom resource using the <code>/dev/disk/by-id</code> format for storage devices specification:</p> <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephDeployment\nmetadata:\n  name: pelagia-ceph\n  namespace: pelagia\nspec:\n  nodes:\n    # Add the exact node names.\n    # Obtain the name from the \"kubectl get node\" list.\n    - name: cluster-storage-worker-0\n      roles:\n      - mgr\n      - mon\n      devices:\n      - config:\n          deviceClass: ssd\n        fullPath: /dev/disk/by-id/scsi-1ATA_WDC_WDS100T2B0A-00SM50_200231440912\n    - name: cluster-storage-worker-1\n      roles:\n      - mgr\n      - mon\n      devices:\n      - config:\n          deviceClass: ssd\n        fullPath: /dev/disk/by-id/nvme-SAMSUNG_MZ1LB3T8HMLA-00007_S46FNY0R394543\n    - name: cluster-storage-worker-2\n      roles:\n      - mgr\n      - mon\n      devices:\n      - config:\n          deviceClass: ssd\n        fullPath: /dev/disk/by-id/nvme-SAMSUNG_ML1EB3T8HMLA-00007_S46FNY1R130423\n  pools:\n  - default: true\n    deviceClass: ssd\n    name: kubernetes\n    replicated:\n      size: 3\n</code></pre>"},{"location":"architecture/limitations/","title":"Pelagia Limitations","text":"<p>A Pelagia <code>CephDeployment</code> configuration includes but is not limited to the following limitations:</p> <ul> <li>The replication size for any Ceph pool must be set to more than 1.</li> <li>Only one CRUSH tree per cluster. The separation of devices per Ceph pool is   supported through Ceph Device Classes   with only one pool of each type for a device class.</li> <li> <p>Only the following types of CRUSH buckets are supported:</p> <ul> <li><code>topology.kubernetes.io/region</code></li> <li><code>topology.kubernetes.io/zone</code></li> <li><code>topology.rook.io/datacenter</code></li> <li><code>topology.rook.io/room</code></li> <li><code>topology.rook.io/pod</code></li> <li><code>topology.rook.io/pdu</code></li> <li><code>topology.rook.io/row</code></li> <li><code>topology.rook.io/rack</code></li> <li><code>topology.rook.io/chassis</code></li> </ul> </li> <li> <p>Only IPv4 is supported.</p> </li> <li>If two or more Ceph OSDs are located on the same device, there must be no   dedicated WAL or DB for this class.</li> <li>Only full collocation or dedicated WAL and DB configurations are supported.</li> <li>The minimum size of any defined Ceph OSD device is <code>5G</code>.</li> <li>Ceph OSDs support only raw disks as data devices meaning that no <code>dm</code> or   <code>lvm</code> devices are allowed.</li> <li>Ceph cluster does not support removable devices (with hotplug enabled) for   deploying Ceph OSDs.</li> <li>When adding a Ceph node with the Ceph Monitor role, if any issues occur with   the Ceph Monitor, Rook Ceph Operator removes it and adds a new Ceph Monitor instead,   named using the next alphabetic character in order. Therefore, the Ceph Monitor   names may not follow the alphabetic order. For example, <code>a</code>, <code>b</code>, <code>d</code>,   instead of <code>a</code>, <code>b</code>, <code>c</code>.</li> <li>Reducing the number of Ceph Monitors is not supported and causes the Ceph   Monitor daemons removal from random nodes.</li> <li>Removal of the <code>mgr</code> role in the <code>nodes</code> section of the   <code>CephDeployment</code> CR does not remove Ceph Managers. To remove a Ceph   Manager from a node, remove it from the <code>nodes</code> spec and manually delete   the <code>rook-ceph-mgr</code> pod in the Rook namespace.</li> </ul>"},{"location":"architecture/overview/","title":"Pelagia Overview","text":"<p>Pelagia Helm chart deploys the following components:</p> <ul> <li>Pelagia Deployment Controller</li> <li>Pelagia Lifecycle Management Controller</li> <li>Rook Ceph Operator from Rook.</li> </ul>"},{"location":"architecture/overview/#pelagia-deployment-controller","title":"Pelagia Deployment Controller","text":"<p>Pelagia Controller contains the following containers:</p>"},{"location":"architecture/overview/#pelagia-deployment-controller_1","title":"Pelagia Deployment Controller","text":"<p>A Kubernetes Cluster API controller that fetches parameters from the <code>CephDeployment</code> custom resource, creates Rook custom resources, and updates <code>CephDeployment</code> and <code>CephDeploymentHealth</code> status based on the Ceph cluster deployment progress.</p> <p>Pelagia Deployment Controller operations include:</p> <ul> <li>Transforming user parameters from the <code>CephDeployment</code> resource into Rook resources   and deploying a Ceph cluster using Rook.</li> <li>Providing single entrypoint management of the Ceph cluster with Kubernetes.</li> <li>Integrating with Rockoon and providing data for OpenStack   to integrate with the deployed Ceph cluster.</li> </ul> <p>Also, Pelagia Controller eventually obtains the data from the OpenStack Controller (Rockoon) for the Keystone integration and updates the Ceph Object Gateway services configurations to use Kubernetes for user authentication.</p>"},{"location":"architecture/overview/#pelagia-secret-controller","title":"Pelagia Secret Controller","text":"<p>A Kubernetes Cluster API controller that fetches parameters from the corresponding secrets of Rook <code>CephClient</code> and <code>CephObjectStoreUser</code> and updates the <code>CephDeploymentSecret</code> status with the secret references. It can be used in custom applications to access the Ceph cluster with defined RBD, RadosGW, or CephFS credentials.</p>"},{"location":"architecture/overview/#pelagia-lifecycle-management-controller","title":"Pelagia Lifecycle Management Controller","text":"<p>Pelagia Lifecycle Management (LCM) Controller contains the following containers:</p>"},{"location":"architecture/overview/#pelagia-health-controller","title":"Pelagia Health Controller","text":"<p>A Kubernetes controller that collects all valuable parameters from the current Ceph cluster, its daemons, and Rook resources and exposes them into the <code>CephDeploymentHealth</code> status.</p> <p>Pelagia Health Controller operations include:</p> <ul> <li>Collecting all statuses from a Ceph cluster and corresponding Rook resources.</li> <li>Collecting additional information on the health of Ceph daemons.</li> <li>Providing information for the <code>status</code> section of the <code>CephDeploymentHealth</code>   custom resource.</li> </ul>"},{"location":"architecture/overview/#pelagia-task-controller","title":"Pelagia Task Controller","text":"<p>A Kubernetes controller that manages Ceph OSD LCM operations. It allows for a safe Ceph OSD removal from the Ceph cluster. It uses the <code>CephOsdRemoveTask</code> custom resource to perform the removal operations.</p> <p>Pelagia Task Controller operations include:</p> <ul> <li>Providing an ability to perform Ceph OSD LCM operations</li> <li>Fetching parameters from the <code>CephOsdRemoveTask</code> resource to remove Ceph OSDs and execute them</li> </ul>"},{"location":"architecture/overview/#pelagia-infra-controller","title":"Pelagia Infra Controller","text":"<p>A Kubernetes controller that manages the Pelagia infrastructure resources. It creates and manages Ceph toolbox with Ceph CLI tools and deploys LCM infrastructure pods that allow processing disk cleanup.</p> <p>Pelagia Task Controller operations include:</p> <ul> <li>Creating and managing the Ceph toolbox pod with the Ceph CLI tools</li> <li>Creating and managing the <code>disk-daemon</code> DaemonSet that is used to perform disk cleanup   operations on the Ceph OSD disks</li> <li>Pausing the regular Rook Ceph Operator orchestration until all requests are finished</li> </ul>"},{"location":"architecture/overview/#rook-ceph-operator","title":"Rook Ceph Operator","text":"<p>Rook is a storage orchestrator that deploys Ceph on top of a Kubernetes cluster. Also known as <code>Rook</code>, <code>Rook Ceph Operator</code> or <code>Rook Operator</code>.</p> <p>Rook operations include:</p> <ul> <li>Deploying and managing a Ceph cluster based on provided Rook CRs such as   <code>CephCluster</code>, <code>CephBlockPool</code>, <code>CephObjectStore</code>, and so on</li> <li>Orchestrating the state of the Ceph cluster and all its daemons</li> </ul> <p>For more information about Rook, see the official Rook documentation.</p> <p>Note that Pelagia Helm chart deploys the Rook Operator according to the Rook chart templates.</p> <p>A typical Ceph cluster deployed by Rook consists of the following components:</p> <ul> <li>Two Ceph Managers (<code>mgr</code>).</li> <li>Three or, in rare cases, five Ceph Monitors (<code>mon</code>).</li> <li>Ceph OSDs (<code>osd</code>). The number of Ceph OSDs may vary depending on deployment needs.</li> <li>Optionally three Ceph Object Gateway (<code>radosgw</code>) daemons.</li> <li>Optionally two Ceph Metadata Servers (<code>mds</code>) for CephFS: one active and one stand-by.</li> </ul> <p>Warning</p> <p>A Ceph cluster with 3 Ceph nodes does not provide hardware fault tolerance and is not eligible for recovery operations, such as a disk or an entire Ceph node replacement.</p>"},{"location":"architecture/overview/#pelagia-data-flow","title":"Pelagia Data Flow","text":"<p>The following diagram illustrates the processes within Pelagia and Rook Ceph Operator that are involved in the Ceph cluster deployment and lifecycle management:</p> <p></p>"},{"location":"architecture/overview/#see-also","title":"See also","text":"<ul> <li>Ceph documentation</li> <li>Rook documentation</li> </ul>"},{"location":"architecture/rockoon-integration/","title":"Pelagia integration with Rockoon","text":"<p>The integration between Pelagia and Rockoon OpenStack Controllers is implemented through the shared Kubernetes <code>openstack-ceph-shared</code> namespace. Both controllers have access to this namespace to read and write the Kubernetes <code>kind: Secret</code> objects.</p> <p></p> <p>To integrate Pelagia with Rockoon, all necessary Ceph pools must be specified in the configuration of the <code>CephDeployment</code> custom resource as part of the deployment. For the required pools, see Ops Guide: Integrate Pelagia with Rockoon.</p> <p>Once the Ceph cluster is deployed, Pelagia Deployment Controller posts the information required by Rockoon OpenStack services to be properly configured as a <code>kind: Secret</code> object into the <code>openstack-ceph-shared</code> namespace. Rockoon Controller watches this namespace. Once the corresponding secret is created, Rockoon Controller transforms this secret to the data structures expected by the OpenStack Helm charts. Even if an OpenStack installation is triggered at the same time as a Ceph cluster deployment, Rockoon Controller halts the deployment of the OpenStack services that depend on Ceph availability until Pelagia Deployment Controller creates the secret in the shared namespace.</p> <p>For the configuration of Ceph Object Gateway as an OpenStack Object Storage, the reverse process takes place. Rockoon Controller waits for the OpenStack Helm to create a secret with OpenStack Identity (Keystone) credentials that Ceph Object Gateway must use to validate the OpenStack Identity tokens, and posts it back to the same <code>openstack-ceph-shared</code> namespace in the format suitable for consumption by Pelagia. Pelagia Deployment Controller then reads this secret and reconfigures Ceph Object Gateway accordingly.</p>"},{"location":"architecture/custom-resources/cephdeployment/","title":"CephDeployment Custom Resource","text":"<p>This section describes how to configure a Ceph cluster using the <code>CephDeployment</code> (<code>cephdeployments.lcm.mirantis.com</code>) custom resource (CR).</p> <p>The <code>CephDeployment</code> CR spec specifies the nodes to deploy as Ceph components. Based on the roles definitions in the <code>CephDeployment</code> CR, Pelagia Deployment Controller automatically labels nodes for Ceph Monitors and Managers. Ceph OSDs are deployed based on the <code>devices</code> parameter defined for each Ceph node.</p> <p>For the default <code>CephDeployment</code> CR, see the following example:</p> Example configuration of Ceph specification <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephDeployment\nmetadata:\n  name: pelagia-ceph\n  namespace: pelagia\nspec:\n  nodes:\n  - name: cluster-storage-controlplane-0\n    roles:\n    - mgr\n    - mon\n    - mds\n  - name: cluster-storage-controlplane-1\n    roles:\n    - mgr\n    - mon\n    - mds\n  - name: cluster-storage-controlplane-2\n    roles:\n    - mgr\n    - mon\n    - mds\n  - name: cluster-storage-worker-0\n    roles: []\n    devices:\n    - config:\n        deviceClass: ssd\n      fullPath: /dev/disk/by-id/scsi-1ATA_WDC_WDS100T2B0A-00SM50_200231434939\n  - name: cluster-storage-worker-1\n    roles: []\n    devices:\n    - config:\n        deviceClass: ssd\n      fullPath: /dev/disk/by-id/scsi-1ATA_WDC_WDS100T2B0A-00SM50_200231440912\n  - name: cluster-storage-worker-2\n    roles: []\n    devices:\n    - config:\n        deviceClass: ssd\n      fullPath: /dev/disk/by-id/scsi-1ATA_WDC_WDS100T2B0A-00SM50_200231443409\n  pools:\n  - default: true\n    deviceClass: ssd\n    name: kubernetes\n    replicated:\n      size: 3\n  objectStorage:\n    rgw:\n      name: rgw-store\n      dataPool:\n        deviceClass: ssd\n        replicated:\n          size: 3\n      metadataPool:\n        deviceClass: ssd\n        replicated:\n          size: 3\n      gateway:\n        allNodes: false\n        instances: 3\n        port: 8081\n        securePort: 8443\n      preservePoolsOnDelete: false\n  sharedFilesystem:\n    cephFS:\n    - name: cephfs-store\n      dataPools:\n      - name: cephfs-pool-1\n        deviceClass: ssd\n        replicated:\n          size: 3\n      metadataPool:\n        deviceClass: ssd\n        replicated:\n          size: 3\n      metadataServer:\n        activeCount: 1\n        activeStandby: false\n</code></pre>"},{"location":"architecture/custom-resources/cephdeployment/#configure-a-ceph-cluster-with-cephdeployment","title":"Configure a Ceph cluster with CephDeployment","text":"<ol> <li> <p>Select from the following options:</p> <ul> <li>If you do not have a Ceph cluster yet, create <code>cephdeployment.yaml</code> for editing.</li> <li>If the Ceph cluster is already deployed, open the <code>CephDeployment</code> CR for editing:</li> </ul> <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre> </li> <li> <p>Using the tables below, configure the Ceph cluster as required:</p> <ul> <li>General parameters</li> <li>Network parameters</li> <li>Nodes parameters</li> <li>Pools parameters</li> <li>Clients parameters</li> <li>Object Storage parameters</li> <li>Object Storage Multisite parameters</li> <li>CephFS parameters</li> <li>RookConfig parameters</li> <li>Health check parameters</li> <li>Extra options</li> <li>Ceph Manager modules parameters</li> <li>RBD Mirroring parameters</li> </ul> </li> <li> <p>Select from the following options:</p> <ul> <li>If you are creating Ceph cluster, save the updated   <code>CephDeployment</code> template to the corresponding file and apply the file to a cluster:   <pre><code>kubectl apply -f cephdeployment.yaml\n</code></pre></li> <li>If you are editing <code>CephDeployment</code> , save the changes and exit the text editor to apply it.</li> </ul> </li> <li> <p>Verify <code>CephDeployment</code> reconcile status with Status fields.</p> </li> </ol>"},{"location":"architecture/custom-resources/cephdeployment/#cephdeployment-configuration-options","title":"CephDeployment configuration options","text":"<p>The following subsections contain a description of <code>CephDeployment</code> parameters for an advanced configuration.</p>"},{"location":"architecture/custom-resources/cephdeployment/#general-parameters","title":"General parameters","text":"Parameter Description <code>network</code> Specifies access and public networks for the Ceph cluster. For details, see Network parameters. <code>nodes</code> Specifies the list of Ceph nodes. For details, see Node parameters. The <code>nodes</code> parameter is a list with Ceph node specifications. List item could define Ceph node specification for a single node or a group of nodes listed or defined by label. It could be also combined. <code>pools</code> Specifies the list of Ceph pools. For details, see Pool parameters. <code>clients</code> List of Ceph clients. For details, see Clients parameters. <code>objectStorage</code> Specifies the parameters for Object Storage, such as RADOS Gateway, the Ceph Object Storage. Also specifies the RADOS Gateway Multisite configuration. For details, see RADOS Gateway parameters and Multisite parameters. <code>ingressConfig</code> Enables a custom ingress rule for public access on Ceph services, for example, Ceph RADOS Gateway. For details, see Configure Ceph Object Gateway TLS. <code>sharedFilesystem</code> Enables Ceph Filesystem. For details, see CephFS parameters. <code>rookConfig</code> String key-value parameter that allows overriding Ceph configuration options. For details, see RookConfig parameters. <code>healthCheck</code> Configures health checks and liveness probe settings for Ceph daemons. For details, see Health check parameters. <code>extraOpts</code> Enables specification of extra options for a setup, includes the <code>deviceLabels</code> parameter. Refer to Extra options for details. <code>mgr</code> Specifies a list of Ceph Manager modules to be enabled or disabled. For details, see Manager modules parameters. Modules <code>balancer</code> and <code>pg_autoscaler</code> are enabled by default. <code>dashboard</code> Enables Ceph dashboard. Currently, Pelagia has no support of Ceph Dashboard. Defaults to <code>false</code>. <code>rbdMirror</code> Specifies the parameters for RBD Mirroring. For details, see RBD Mirroring parameters. <code>external</code> Enables external Ceph cluster mode. If enabled, Pelagia will read a special <code>Secret</code> with external Ceph cluster credentials data connect to."},{"location":"architecture/custom-resources/cephdeployment/#network-parameters","title":"Network parameters","text":"<ul> <li> <p><code>clusterNet</code> - specifies a Classless Inter-Domain Routing (CIDR)   for the Ceph OSD replication network.</p> <p>Warning</p> <p>To avoid ambiguous behavior of Ceph daemons, do not specify <code>0.0.0.0/0</code> in <code>clusterNet</code>. Otherwise, Ceph daemons can select an incorrect public interface that can cause the Ceph cluster to become unavailable.</p> <p>Note</p> <p>The <code>clusterNet</code> and <code>publicNet</code> parameters support multiple IP networks. For details, see Ops Guide: Enable Multinetworking.</p> </li> <li> <p><code>publicNet</code> - specifies a CIDR for communication between   the service and operator.</p> <p>Warning</p> <p>To avoid ambiguous behavior of Ceph daemons, do not specify <code>0.0.0.0/0</code> in <code>publicNet</code>. Otherwise, Ceph daemons can select an incorrect public interface that can cause the Ceph cluster to become unavailable.</p> <p>Note</p> <p>The <code>clusterNet</code> and <code>publicNet</code> parameters support multiple IP networks. For details, see Ops Guide: Enable Multinetworking.</p> </li> </ul> <p>Example configuration: <pre><code>spec:\n  network:\n    clusterNet: 10.10.0.0/24\n    publicNet:  192.100.0.0/24\n</code></pre></p>"},{"location":"architecture/custom-resources/cephdeployment/#nodes-parameters","title":"Nodes parameters","text":"<ul> <li> <p><code>name</code> - Mandatory. Specifies the following:</p> <ul> <li>If node spec implies to be deployed on a single node, <code>name</code>   stands for node name where Ceph node should be deployed. For example,   it could be <code>cluster-storage-worker-0</code>.</li> <li>If node spec implies to be deployed on a group of nodes, <code>name</code>   stands for group name, for example <code>group-rack-1</code>. In that case,   Ceph node specification must contain either <code>nodeGroup</code> or   <code>nodesByLabel</code> fields defined.</li> </ul> </li> <li> <p><code>nodeGroup</code> - Optional. Specifies the list of nodes and used for specifying Ceph   node specification for a group of nodes from the list. For example:</p> <pre><code>spec:\n  nodes:\n  - name: group-1\n    nodeGroup: [node-X, node-Y]\n</code></pre> </li> <li> <p><code>nodesByLabel</code> - Optional. Specifies label expression and used for specifying Ceph   node specification for a group of nodes found by label. For example:</p> <pre><code>spec:\n  nodes:\n  - name: group-1\n    nodesByLabel: \"ceph-storage-node=true,!ceph-control-node\"\n</code></pre> </li> <li> <p><code>roles</code> - Optional. Specifies the <code>mon</code>, <code>mgr</code>, <code>rgw</code> or <code>mds</code> daemon   to be installed on a Ceph node. You can place the daemons on any nodes   upon your decision. Consider the following recommendations:</p> <ul> <li>The recommended number of Ceph Monitors in a Ceph cluster is 3.   Therefore, at least three Ceph nodes must contain the <code>mon</code> item in   the <code>roles</code> parameter.</li> <li>The number of Ceph Monitors must be odd.</li> <li>Do not add more than three Ceph Monitors at a time and wait until the   Ceph cluster is <code>Ready</code> before adding more daemons.</li> <li>For better HA and fault tolerance, the number of <code>mgr</code> roles   must equal the number of <code>mon</code> roles. Therefore, we recommend   labeling at least three Ceph nodes with the <code>mgr</code> role.</li> <li>If <code>rgw</code> roles are not specified, all <code>rgw</code> daemons will spawn   on the same nodes with <code>mon</code> daemons.</li> </ul> <p>If a Ceph node contains a <code>mon</code> role, the Ceph Monitor Pod deploys on this node.</p> <p>If a Ceph node contains a <code>mgr</code> role, it informs the Ceph Controller that a Ceph Manager can be deployed on the node. Rook Operator selects the first available node to deploy the Ceph Manager on it. Pelagia supports deploying two Ceph Managers in total: one active and one stand-by.</p> <p>If you assign the <code>mgr</code> role to three recommended Ceph nodes, one back-up Ceph node is available to redeploy a failed Ceph Manager in a case of a node outage.</p> </li> <li> <p><code>monitorIP</code> - Optional. If defined, specifies a custom IP address for Ceph Monitor which   should be placed on the node. If not set, Ceph Monitor on the node will use   default <code>hostNetwork</code> address of a node. General recommendation is to use IP   address from Ceph public network address range.</p> <p>Note</p> <p>To update <code>monitorIP</code> the corresponding Ceph Monitor daemon should be re-created.</p> </li> <li> <p><code>config</code> - Mandatory. Specifies a map of device configurations that must contain a   mandatory <code>deviceClass</code> parameter set to <code>hdd</code>, <code>ssd</code>, or <code>nvme</code>.   Applied for all OSDs on the current node. Can be overridden by the   <code>config</code> parameter of each device defined in the <code>devices</code> parameter.</p> <p>For details, see Rook documentation: OSD config settings.</p> </li> <li> <p><code>devices</code> - Optional. Specifies the list of devices to use for Ceph OSD deployment.   Includes the following parameters:</p> <p>Note</p> <p>Recommending to use the <code>fullPath</code> field for defining <code>by-id</code> symlinks as persistent device identifiers. For details, see Addressing Ceph devices.</p> <ul> <li> <p><code>fullPath</code> - a storage device symlink. Accepts the following values:</p> <ul> <li> <p>The device <code>by-id</code> symlink that contains the serial number of the   physical device and does not contain <code>wwn</code>. For example,   <code>/dev/disk/by-id/nvme-SAMSUNG_MZ1LB3T8HMLA-00007_S46FNY0R394543</code>.</p> </li> <li> <p>The device <code>by-path</code> symlink. For example,   <code>/dev/disk/by-path/pci-0000:00:11.4-ata-3</code>. We do not recommend   specifying storage devices with device <code>by-path</code> symlinks   because such identifiers are not persistent and can change at node boot.</p> </li> </ul> <p>This parameter is mutually exclusive with <code>name</code>.</p> </li> <li> <p><code>name</code> - a storage device name. Accepts the following values:</p> <ul> <li>The device name, for example, <code>sdc</code>. We do not recommend   specifying storage devices with device names because such identifiers   are not persistent and can change at node boot.</li> <li>The device <code>by-id</code> symlink that contains the serial number of the   physical device and does not contain <code>wwn</code>. For example,   <code>/dev/disk/by-id/nvme-SAMSUNG_MZ1LB3T8HMLA-00007_S46FNY0R394543</code>.</li> <li>The device label from <code>extraOpts.deviceLabels</code> section which is   generally used for templating Ceph node specification for node groups.   For details, see Extra options.</li> </ul> <p>This parameter is mutually exclusive with <code>fullPath</code>.</p> </li> <li> <p><code>config</code> - a map of device configurations that must contain a   mandatory <code>deviceClass</code> parameter set to <code>hdd</code>, <code>ssd</code>, or   <code>nvme</code>. The device class must be defined in a pool and can   optionally contain a metadata device, for example:</p> <pre><code>spec:\n  nodes:\n  - name: &lt;node-a&gt;\n    devices:\n    - fullPath: /dev/disk/by-id/scsi-SATA_HGST_HUS724040AL_PN1334PEHN18ZS\n      config:\n        deviceClass: hdd\n        metadataDevice: /dev/meta-1/nvme-dev-1\n        osdsPerDevice: \"2\"\n</code></pre> <p>The underlying storage format to use for Ceph OSDs is BlueStore.</p> <p>The <code>metadataDevice</code> parameter accepts a device name or logical volume path for the BlueStore device. We recommend using logical volume paths created on <code>nvme</code> devices.</p> <p>The <code>osdsPerDevice</code> parameter accepts the string-type natural numbers and allows splitting one device on several Ceph OSD daemons. We recommend using this parameter only for <code>ssd</code> or <code>nvme</code> disks.</p> </li> </ul> </li> <li> <p><code>deviceFilter</code> - Optional. Specifies regexp by names of devices to use for Ceph OSD   deployment. Mutually exclusive with <code>devices</code> and <code>devicePathFilter</code>.   Requires the <code>config</code> parameter with <code>deviceClass</code> specified. For example:</p> <pre><code>spec:\n  nodes:\n  - name: &lt;node-a&gt;\n    deviceFilter: \"^sd[def]$\"\n    config:\n      deviceClass: hdd\n</code></pre> <p>For more details, see Rook documentation: Storage selection settings.</p> </li> <li> <p><code>devicePathFilter</code> - Optional. Specifies regexp by paths of devices to use for Ceph OSD   deployment. Mutually exclusive with <code>devices</code> and <code>deviceFilter</code>.   Requires the <code>config</code> parameter with <code>deviceClass</code> specified. For example:</p> <pre><code>spec:\n  nodes:\n  - name: &lt;node-a&gt;\n    devicePathFilter: \"^/dev/disk/by-id/scsi-SATA.+$\"\n    config:\n      deviceClass: hdd\n</code></pre> <p>For more details, see Rook documentation: Storage selection settings.</p> </li> <li> <p><code>crush</code> - Optional. Specifies the explicit key-value CRUSH topology for a node.   For details, see Ceph documentation: CRUSH maps.   Includes the following parameters:</p> <ul> <li><code>datacenter</code> - a physical data center that consists of rooms and   handles data.</li> <li><code>room</code> - a room that accommodates one or more racks with hosts.</li> <li><code>pdu</code> - a power distribution unit (PDU) device that has multiple   outputs and distributes electric power to racks located within a   data center.</li> <li><code>row</code> - a row of computing racks inside <code>room</code>.</li> <li><code>rack</code> - a computing rack that accommodates one or more hosts.</li> <li><code>chassis</code> - a bare metal structure that houses or physically   assembles hosts.</li> <li><code>region</code> - the geographic location of one or more Ceph Object   instances within one or more zones.</li> <li><code>zone</code> - a logical group that consists of one or more Ceph Object   instances.</li> </ul> <p>Example configuration:</p> <pre><code>spec:\n  nodes:\n  - name: &lt;node-a&gt;\n    crush:\n      datacenter: dc1\n      room: room1\n      pdu: pdu1\n      row: row1\n      rack: rack1\n      chassis: ch1\n      region: region1\n      zone: zone1\n</code></pre> </li> </ul>"},{"location":"architecture/custom-resources/cephdeployment/#pools-parameters","title":"Pools parameters","text":"<ul> <li><code>name</code> - Mandatory. Specifies the pool name as a prefix for each Ceph block pool.   The resulting Ceph block pool name will be <code>&lt;name&gt;-&lt;deviceClass&gt;</code>.</li> <li><code>useAsFullName</code> - Optional. Enables Ceph block pool to use only the <code>name</code> value as a name.   The resulting Ceph block pool name will be <code>&lt;name&gt;</code> without the <code>deviceClass</code> suffix.</li> <li><code>role</code> - Optional. Specifies the pool role for Rockoon integration.</li> <li><code>preserveOnDelete</code> - Optional. Enables skipping Ceph pool delete on <code>pools</code> section item removal.   If <code>pools</code> section item removed with this flag enabled, related <code>CephBlockPool</code> object would be   kept untouched and will require manual deletion on demand. Defaulted to <code>false</code>.</li> <li> <p><code>storageClassOpts</code> - Optional. Allows to configure parameters for storage class, created for RBD pool.   Includes the following parameters:</p> <ul> <li><code>default</code> - Optional. Defines whether the pool and dependent StorageClass must be set   as default. Must be enabled only for one pool. Defaults to <code>false</code>.</li> <li><code>mapOptions</code> - Optional. Not updatable as it applies only once. Specifies custom   <code>rbd device map</code> options to use with <code>StorageClass</code> of a corresponding pool. Allows customizing   the Kubernetes CSI driver interaction with Ceph RBD for the defined <code>StorageClass</code>. For   available options, see Ceph documentation: Kernel RBD (KRBD) options.</li> <li><code>unmapOptions</code> - Optional. Not updatable as it applies only once. Specifies custom   <code>rbd device unmap</code> options to use with <code>StorageClass</code> of a corresponding pool. Allows customizing   the Kubernetes CSI driver interaction with Ceph RBD for the defined <code>StorageClass</code>. For   available options, see Ceph documentation: Kernel RBD (KRBD) options.</li> <li><code>imageFeatures</code> - Optional. Not updatable as it applies only once. Specifies is a comma-separated   list of RBD image features, see Ceph documentation: Manage Rados block device (RBD) images.</li> <li><code>reclaimPolicy</code> - Optional. Specifies reclaim policy for the underlying <code>StorageClass</code> of   the pool. Accepts <code>Retain</code> and <code>Delete</code> values. Default is <code>Delete</code> if not set.</li> <li> <p><code>allowVolumeExpansion</code> - Optional. Not updatable as it applies only once. Enables expansion of   persistent volumes based on <code>StorageClass</code> of a corresponding pool. For details, see   Kubernetes documentation: Resizing persistent volumes using Kubernetes.</p> <p>Note</p> <p>A Kubernetes cluster only supports increase of storage size.</p> </li> </ul> </li> <li> <p><code>deviceClass</code> - Mandatory. Specifies the device class for the defined pool. Common possible   values are <code>hdd</code>, <code>ssd</code> and <code>nvme</code>. Also allows customized device classes, refers to Extra options.</p> </li> <li> <p><code>replicated</code> - The <code>replicated</code> parameter is mutually exclusive with <code>erasureCoded</code>   and includes the following parameters:</p> <ul> <li><code>size</code> - the number of pool replicas.</li> <li> <p><code>targetSizeRatio</code> - A float percentage from <code>0.0</code> to <code>1.0</code>, which   specifies the expected consumption of the total Ceph cluster capacity.   The default values are as follows:</p> <ul> <li>The default ratio of the Ceph Object Storage <code>dataPool</code> 10.0%.</li> <li>Target ratios for the pools required for Rockoon, described in   Ops Guide: Integrate Pelagia with Rockoon.</li> </ul> <p>Note</p> <p>Mirantis recommends defining target ratio with the <code>parameters.target_size_ratio</code> string field instead.</p> </li> </ul> </li> <li> <p><code>erasureCoded</code> - Enables the erasure-coded pool. For details, see Rook documentation: Erasure Coded RBD Pool.   and Ceph documentation: Erasure coded pool. The   <code>erasureCoded</code> parameter is mutually exclusive with <code>replicated</code>.</p> </li> <li> <p><code>failureDomain</code> - Optional. The failure domain across which the replicas or chunks   of data will be spread. Set to <code>host</code> by default. The list of   possible recommended values includes: <code>host</code>, <code>rack</code>, <code>room</code>,   and <code>datacenter</code>.</p> <p>Caution</p> <p>We do not recommend using the following intermediate topology keys: <code>pdu</code>, <code>row</code>, <code>chassis</code>. Consider the <code>rack</code> topology instead. The <code>osd</code> failure domain is prohibited.</p> </li> <li> <p><code>mirroring</code> - Optional. Enables the mirroring feature for the defined pool.   Includes the <code>mode</code> parameter that can be set to <code>pool</code> or <code>image</code>. For details, see Ops Guide: Enable RBD Mirroring.</p> </li> <li><code>parameters</code> - Optional. Specifies the key-value map for the parameters of the Ceph pool.   For details, see Ceph documentation: Set Pool values.</li> <li><code>enableCrushUpdates</code> - Optional. Enables automatic updates of the CRUSH map   when the pool is created or updated. Defaulted to <code>false</code>.</li> </ul> Example configuration of Pools specification <pre><code>spec:\n  pools:\n  - name: kubernetes\n    deviceClass: hdd\n    replicated:\n      size: 3\n    parameters:\n      target_size_ratio: \"10.0\"\n    storageClassOpts:\n      default: true\n    preserveOnDelete: true\n  - name: kubernetes\n    deviceClass: nvme\n    erasureCoded:\n      codingChunks: 1\n      dataChunks: 2\n    failureDomain: host\n  - name: archive\n    useAsFullName: true\n    deviceClass: hdd\n    failureDomain: rack\n    replicated:\n      size: 3\n</code></pre> <p>As a result, the following Ceph pools will be created: <code>kubernetes-hdd</code>, <code>kubernetes-nvme</code>, and <code>archive</code>.</p> <p>To configure additional required pools for Rockoon, see Ops Guide: Integrate Pelagia with Rockoon.</p> <p>Caution</p> <p>Since Ceph Pacific, Ceph CSI driver does not propagate the <code>777</code> permission on the mount point of persistent volumes based on any <code>StorageClass</code> of the Ceph pool.</p>"},{"location":"architecture/custom-resources/cephdeployment/#clients-parameters","title":"Clients parameters","text":"<ul> <li><code>name</code> - Mandatory. Ceph client name.</li> <li><code>caps</code> - Mandatory. Key-value parameter with Ceph client capabilities. For details about   <code>caps</code>, refer to Ceph documentation: Authorization (capabilities).</li> </ul> Example configuration of Clients specification <pre><code>spec:\n  clients:\n  - name: test-client\n    caps:\n      mon: allow r, allow command \"osd blacklist\"\n      osd: profile rbd pool=kubernetes-nvme\n</code></pre>"},{"location":"architecture/custom-resources/cephdeployment/#rados-gateway-parameters","title":"RADOS Gateway parameters","text":"<ul> <li><code>name</code> - Required. Ceph Object Storage instance name.</li> <li> <p><code>dataPool</code> - Required if <code>zone.name</code> is not specified. Mutually exclusive with   <code>zone</code>. Must be used together with <code>metadataPool</code>.</p> <p>Object storage data pool spec that must only contain <code>replicated</code> or <code>erasureCoded</code>, <code>deviceClass</code> and <code>failureDomain</code> parameters. The <code>failureDomain</code> parameter may be set to <code>host</code>, <code>rack</code>, <code>room</code>, or <code>datacenter</code>, defining the failure domain across which the data will be spread. The <code>deviceClass</code> must be explicitly defined. For <code>dataPool</code>, We recommend using an <code>erasureCoded</code> pool.</p> <pre><code>spec:\n   objectStorage:\n     rgw:\n       dataPool:\n         deviceClass: hdd\n         failureDomain: host\n         erasureCoded:\n           codingChunks: 1\n           dataChunks: 2\n</code></pre> </li> <li> <p><code>metadataPool</code> - Required if <code>zone.name</code> is not specified. Mutually exclusive with   <code>zone</code>. Must be used together with <code>dataPool</code>.</p> <p>Object storage metadata pool spec that must only contain <code>replicated</code>, <code>deviceClass</code> and <code>failureDomain</code> parameters. The <code>failureDomain</code> parameter may be set to <code>host</code>, <code>rack</code>, <code>room</code>, or <code>datacenter</code>, defining the failure domain across which the data will be spread. The <code>deviceClass</code> must be explicitly defined. Can use only <code>replicated</code> settings. For example:</p> <pre><code>spec:\n   objectStorage:\n     rgw:\n       metadataPool:\n         deviceClass: hdd\n         failureDomain: host\n         replicated:\n           size: 3\n</code></pre> <p>where <code>replicated.size</code> is the number of full copies of data on multiple nodes.</p> <p>Warning</p> <p>When using the non-recommended Ceph pools <code>replicated.size</code> of less than <code>3</code>, Ceph OSD removal cannot be performed. The minimal replica size equals a rounded up half of the specified <code>replicated.size</code>.</p> <p>For example, if <code>replicated.size</code> is <code>2</code>, the minimal replica size is <code>1</code>, and if <code>replicated.size</code> is <code>3</code>, then the minimal replica size is <code>2</code>. The replica size of <code>1</code> allows Ceph having PGs with only one Ceph OSD in the <code>acting</code> state, which may cause a <code>PG_TOO_DEGRADED</code> health warning that blocks Ceph OSD removal. We recommend setting <code>replicated.size</code> to <code>3</code> for each Ceph pool.</p> </li> <li> <p><code>gateway</code> - Required. The gateway settings corresponding to the <code>rgw</code> daemon   settings. Includes the following parameters:</p> <ul> <li><code>port</code> - the port on which the Ceph RGW service will be listening on   HTTP.</li> <li><code>securePort</code> - the port on which the Ceph RGW service will be   listening on HTTPS.</li> <li> <p><code>instances</code> - the number of pods in the Ceph RGW ReplicaSet. If   <code>allNodes</code> is set to <code>true</code>, a DaemonSet is created instead.</p> <p>Note</p> <p>We recommend using 3 instances for Ceph Object Storage.</p> </li> <li> <p><code>allNodes</code> - defines whether to start the Ceph RGW pods as a   DaemonSet on all nodes. The <code>instances</code> parameter is ignored if   <code>allNodes</code> is set to <code>true</code>.</p> </li> <li><code>splitDaemonForMultisiteTrafficSync</code> - Optional. For multisite setup defines   whether to split RGW daemon on daemon responsible for sync between zones and daemon   for serving clients request.</li> <li><code>rgwSyncPort</code> - Optional. Port the rgw multisite traffic service will be listening on (http).   Has effect only for multisite configuration.</li> <li><code>resources</code> - Optional. Represents Kubernetes resource requirements for Ceph RGW pods. For details   see: Kubernetes docs: Resource Management for Pods and Containers.</li> <li> <p><code>externalRgwEndpoint</code> - Required for external Ceph cluster Setup. Represents external RGW Endpoint to use,   only when external Ceph cluster is used. Contains the following parameters:</p> <ul> <li><code>ip</code> - represents the IP address of RGW endpoint.</li> <li><code>hostname</code> - represents the DNS-addressable hostname of RGW endpoint.   This field will be preferred over IP if both are given.</li> </ul> <pre><code>spec:\n  objectStorage:\n    rgw:\n      gateway:\n        allNodes: false\n        instances: 3\n        port: 80\n        securePort: 8443\n</code></pre> </li> </ul> </li> <li> <p><code>preservePoolsOnDelete</code> - Optional. Defines whether to delete the data and metadata pools in   the <code>rgw</code> section if the Object Storage is deleted. Set this parameter   to <code>true</code> if you need to store data even if the object storage is   deleted. However, we recommend setting this parameter to <code>false</code>.</p> </li> <li> <p><code>objectUsers</code> and <code>buckets</code> - Optional. To create new Ceph RGW resources, such as buckets or users,   specify the following keys. Ceph Controller will automatically create   the specified object storage users and buckets in the Ceph cluster.</p> <ul> <li> <p><code>objectUsers</code> - a list of user specifications to create for object   storage. Contains the following fields:</p> <ul> <li><code>name</code> - a user name to create.</li> <li><code>displayName</code> - the Ceph user name to display.</li> <li> <p><code>capabilities</code> - user capabilities:</p> <ul> <li><code>user</code> - admin capabilities to read/write Ceph Object Store   users.</li> <li><code>bucket</code> - admin capabilities to read/write Ceph Object Store   buckets.</li> <li><code>metadata</code> - admin capabilities to read/write Ceph Object Store   metadata.</li> <li><code>usage</code> - admin capabilities to read/write Ceph Object Store   usage.</li> <li><code>zone</code> - admin capabilities to read/write Ceph Object Store   zones.</li> </ul> <p>The available options are <code>*</code>, <code>read</code>, <code>write</code>, <code>read, write</code>. For details, see Ceph documentation: Add/remove admin capabilities.</p> </li> <li> <p><code>quotas</code> - user quotas:</p> <ul> <li><code>maxBuckets</code> - the maximum bucket limit for the Ceph user.   Integer, for example, <code>10</code>.</li> <li><code>maxSize</code> - the maximum size limit of all objects across all the   buckets of a user. String size, for example, <code>10G</code>.</li> <li><code>maxObjects</code> - the maximum number of objects across all buckets   of a user. Integer, for example, <code>10</code>.</li> </ul> </li> </ul> <pre><code>spec:\n  objectStorage:\n    rgw:\n      objectUsers:\n      - name: test-user\n         displayName: test-user\n         capabilities:\n           bucket: '*'\n           metadata: read\n           user: read\n         quotas:\n           maxBuckets: 10\n           maxSize: 10G\n</code></pre> </li> <li> <p><code>buckets</code> - a list of strings that contain bucket names to create   for object storage.</p> </li> </ul> </li> <li> <p><code>zone</code> - Required if <code>dataPool</code> and <code>metadataPool</code> are not specified. Mutually     exclusive with these parameters.</p> <p>Defines the Ceph Multisite zone where the object storage must be placed.   Includes the <code>name</code> parameter that must be set to one of the <code>zones</code>   items. For details, see the Ops Guide: Enable Multisite for Ceph Object Storage.</p> <pre><code>spec:\n  objectStorage:\n    multisite:\n      zones:\n      - name: master-zone\n        ...\n    rgw:\n      zone:\n        name: master-zone\n</code></pre> </li> <li> <p><code>SSLCert</code> - Optional. Custom TLS certificate parameters used to access the Ceph RGW     endpoint. If not specified, a self-signed certificate will be generated.</p> <pre><code>spec:\n  objectStorage:\n    rgw:\n      SSLCert:\n        cacert: |\n          -----BEGIN CERTIFICATE-----\n          ca-certificate here\n          -----END CERTIFICATE-----\n        tlsCert: |\n          -----BEGIN CERTIFICATE-----\n          private TLS certificate here\n          -----END CERTIFICATE-----\n        tlsKey: |\n          -----BEGIN RSA PRIVATE KEY-----\n          private TLS key here\n          -----END RSA PRIVATE KEY-----\n</code></pre> </li> <li> <p><code>SSLCertInRef</code> - Optional. Flag to determine that a TLS     certificate for accessing the Ceph RGW endpoint is used but not exposed     in <code>spec</code>. For example:</p> <pre><code>spec:\n  objectStorage:\n    rgw:\n      SSLCertInRef: true\n</code></pre> <p>The operator must manually provide TLS configuration using the   <code>rgw-ssl-certificate</code> secret in the <code>rook-ceph</code> namespace of the   managed cluster. The secret object must have the following structure:</p> <pre><code>data:\n  cacert: &lt;base64encodedCaCertificate&gt;\n  cert: &lt;base64encodedCertificate&gt;\n</code></pre> <p>When removing an already existing <code>SSLCert</code> block, no additional actions   are required, because this block uses the same <code>rgw-ssl-certificate</code> secret   in the <code>rook-ceph</code> namespace.</p> <p>When adding a new secret directly without exposing it in <code>spec</code>, the following   rules apply:</p> <ul> <li><code>cert</code> - base64 representation of a file with the server TLS key,     server TLS cert, and CA certificate.</li> <li><code>cacert</code> - base64 representation of a CA certificate only.</li> </ul> </li> </ul> Example configuration of RADOS gateway specification <pre><code>spec:\n  objectStorage:\n    rgw:\n      name: rgw-store\n      dataPool:\n        deviceClass: hdd\n        erasureCoded:\n          codingChunks: 1\n          dataChunks: 2\n        failureDomain: host\n      metadataPool:\n        deviceClass: hdd\n        failureDomain: host\n        replicated:\n          size: 3\n      gateway:\n        allNodes: false\n        instances: 3\n        port: 80\n        securePort: 8443\n      preservePoolsOnDelete: false\n</code></pre>"},{"location":"architecture/custom-resources/cephdeployment/#rados-gateway-multisite-parameters","title":"RADOS Gateway Multisite parameters","text":"<p>Technical Preview</p> <ul> <li> <p><code>realms</code> - Required. List of realms to use, represents the realm namespaces.   Includes the following parameters:</p> <ul> <li><code>name</code> - required, the realm name.</li> <li> <p><code>pullEndpoint</code> - optional, required only when the master zone is in   a different storage cluster. The endpoint, access key, and system key   of the system user from the realm to pull from. Includes the   following parameters:</p> <ul> <li><code>endpoint</code> - the endpoint of the master zone in the master zone   group.</li> <li><code>accessKey</code> - the access key of the system user from the realm to   pull from.</li> <li><code>secretKey</code> - the system key of the system user from the realm to   pull from.</li> </ul> </li> </ul> </li> <li> <p><code>zoneGroups</code> - Required. The list of zone groups for realms. Includes the following   parameters:</p> <ul> <li><code>name</code> - required, the zone group name.</li> <li><code>realmName</code> - required, the realm namespace name to which   the zone group belongs to.</li> </ul> </li> <li> <p><code>zones</code> - Required. The list of zones used within one zone group. Includes   the following parameters:</p> <ul> <li><code>name</code> - required, the zone name.</li> <li><code>metadataPool</code> - required, the settings used to create the Object   Storage metadata pools. Must use replication. For details, see   description of Pool parameters.</li> <li><code>dataPool</code> - required, the settings used to create the Object Storage   data pool. Can use replication or erasure coding. For details, see   Pool parameters.</li> <li><code>zoneGroupName</code> - required, the zone group name.</li> <li><code>endpointsForZone</code> - optional. The list of all endpoints in the zone group.   If you use ingress proxy for RGW,   the list of endpoints must contain that FQDN/IP address to access RGW.   By default, if no ingress proxy is used, the list of endpoints is set   to the IP address of the RGW external service. Endpoints must follow   the HTTP URL format.</li> </ul> </li> </ul> <p>For configuration example, see Ops Guide: Enable Multisite for Ceph Object Storage.</p>"},{"location":"architecture/custom-resources/cephdeployment/#cephfs-parameters","title":"CephFS parameters","text":"<p><code>sharedFilesystem</code> contains a list of Ceph Filesystems <code>cephFS</code>. Each <code>cephFS</code> item contains the following parameters:</p> <ul> <li><code>name</code> - Mandatory. CephFS instance name.</li> <li> <p><code>dataPools</code> - A list of CephFS data pool specifications. Each spec contains the   <code>name</code>, <code>replicated</code> or <code>erasureCoded</code>, <code>deviceClass</code>, and   <code>failureDomain</code> parameters. The first pool in the list is treated   as the default data pool for CephFS and must always be   <code>replicated</code>. The number of data pools is unlimited, but the default   pool must always be present. For example:</p> <pre><code>spec:\n  sharedFilesystem:\n    cephFS:\n    - name: cephfs-store\n      dataPools:\n      - name: default-pool\n        deviceClass: ssd\n        replicated:\n          size: 3\n        failureDomain: host\n      - name: second-pool\n        deviceClass: hdd\n        failureDomain: rack\n        erasureCoded:\n          dataChunks: 2\n          codingChunks: 1\n</code></pre> <p>where <code>replicated.size</code> is the number of full copies of data on   multiple nodes.</p> <p>Warning</p> <p>When using the non-recommended Ceph pools <code>replicated.size</code> of   less than <code>3</code>, Ceph OSD removal cannot be performed. The minimal replica   size equals a rounded up half of the specified <code>replicated.size</code>.</p> <p>For example, if <code>replicated.size</code> is <code>2</code>, the minimal replica size is   <code>1</code>, and if <code>replicated.size</code> is <code>3</code>, then the minimal replica size   is <code>2</code>. The replica size of <code>1</code> allows Ceph having PGs with only one   Ceph OSD in the <code>acting</code> state, which may cause a <code>PG_TOO_DEGRADED</code>   health warning that blocks Ceph OSD removal. We recommend setting   <code>replicated.size</code> to <code>3</code> for each Ceph pool.</p> <p>Warning</p> <p>Modifying of <code>dataPools</code> on a deployed CephFS has no   effect. You can manually adjust pool settings through the Ceph   CLI. However, for any changes in <code>dataPools</code>, we recommend re-creating CephFS.</p> </li> <li> <p><code>metadataPool</code> - CephFS metadata pool spec that should only contain <code>replicated</code>,   <code>deviceClass</code>, and <code>failureDomain</code> parameters. Can use only <code>replicated</code> settings.   For example:</p> <pre><code>spec:\n  sharedFilesystem:\n    cephFS:\n    - name: cephfs-store\n      metadataPool:\n        deviceClass: nvme\n        replicated:\n          size: 3\n        failureDomain: host\n</code></pre> <p>where <code>replicated.size</code> is the number of full copies of data on   multiple nodes.</p> <p>Warning</p> <p>Modifying of <code>metadataPool</code> on a deployed CephFS has   no effect. You can manually adjust pool settings through the   Ceph CLI. However, for any changes in <code>metadataPool</code>,   we recommend re-creating CephFS.</p> </li> <li> <p><code>preserveFilesystemOnDelete</code> - Defines whether to delete the data and metadata pools if CephFS is   deleted. Set to <code>true</code> to avoid occasional data loss in case of   human error. However, for security reasons, we recommend   setting <code>preserveFilesystemOnDelete</code> to <code>false</code>.</p> </li> <li> <p><code>metadataServer</code> - Metadata Server settings correspond to the Ceph MDS daemon settings.   Contains the following fields:</p> <ul> <li><code>activeCount</code> - the number of active Ceph MDS instances. As load   increases, CephFS will automatically partition the file system   across the Ceph MDS instances. Rook will create double the number   of Ceph MDS instances as requested by <code>activeCount</code>. The extra   instances will be in the standby mode for failover. We   recommend specifying this parameter to <code>1</code> and increasing the   MDS daemons count only in case of high load.</li> <li><code>activeStandby</code> - defines whether the extra Ceph MDS instances   will be in active standby mode and will keep a warm cache of the   file system metadata for faster failover. The instances will be   assigned by CephFS in failover pairs. If <code>false</code>, the extra   Ceph MDS instances will all be in passive standby mode and will   not maintain a warm cache of the metadata. The default value is   <code>false</code>.</li> <li><code>resources</code> - represents Kubernetes resource requirements for Ceph MDS pods.   For details see: Kubernetes docs: Resource Management for Pods and Containers.</li> </ul> <pre><code>spec:\n  sharedFilesystem:\n    cephFS:\n    - name: cephfs-store\n      metadataServer:\n        activeCount: 1\n        activeStandby: false\n      resources: # example, non-prod values\n        requests:\n          memory: 1Gi\n          cpu: 1\n      limits:\n        memory: 2Gi\n        cpu: 2\n</code></pre> </li> </ul> Example configuration of shared Filesystem specification <pre><code>spec:\n  sharedFilesystem:\n    cephFS:\n    - name: cephfs-store\n      dataPools:\n      - name: cephfs-pool-1\n        deviceClass: hdd\n        replicated:\n          size: 3\n        failureDomain: host\n      metadataPool:\n        deviceClass: nvme\n        replicated:\n          size: 3\n        failureDomain: host\n      metadataServer:\n        activeCount: 1\n        activeStandby: false\n</code></pre>"},{"location":"architecture/custom-resources/cephdeployment/#rookconfig-parameters","title":"RookConfig parameters","text":"<p>String key-value parameter that allows overriding Ceph configuration options.</p> <p>Use the <code>|</code> delimiter to specify the section where a parameter must be placed. For example, <code>mon</code> or <code>osd</code>. And, if required, use the <code>.</code> delimiter to specify the exact number of the Ceph OSD or Ceph Monitor to apply an option to a specific <code>mon</code> or <code>osd</code> and override the configuration of the corresponding section.</p> <p>The use of this option enables restart of only specific daemons related to the corresponding section. If you do not specify the section, a parameter is set in the <code>global</code> section, which includes restart of all Ceph daemons except Ceph OSD.</p> <pre><code>spec:\n  rookConfig:\n    \"osd_max_backfills\": \"64\"\n    \"mon|mon_health_to_clog\":  \"true\"\n    \"osd|osd_journal_size\": \"8192\"\n    \"osd.14|osd_journal_size\": \"6250\"\n</code></pre>"},{"location":"architecture/custom-resources/cephdeployment/#healthcheck-parameters","title":"HealthCheck parameters","text":"<ul> <li> <p><code>daemonHealth</code> - Optional. Specifies health check settings for Ceph daemons. Contains   the following parameters:</p> <ul> <li><code>status</code> - configures health check settings for Ceph health</li> <li><code>mon</code> - configures health check settings for Ceph Monitors</li> <li><code>osd</code> - configures health check settings for Ceph OSDs</li> </ul> <p>Each parameter allows defining the following settings:</p> <ul> <li><code>disabled</code> - a flag that disables the health check.</li> <li><code>interval</code> - an interval in seconds or minutes for the health     check to run. For example, <code>60s</code> for 60 seconds.</li> <li><code>timeout</code> - a timeout for the health check in seconds or minutes.     For example, <code>60s</code> for 60 seconds.</li> </ul> </li> <li> <p><code>livenessProbe</code> - Optional. Key-value parameter with liveness probe settings for   the defined daemon types. Can be one of the following: <code>mgr</code>,   <code>mon</code>, <code>osd</code>, or <code>mds</code>. Includes the <code>disabled</code> flag and   the <code>probe</code> parameter. The <code>probe</code> parameter accepts   the following options:</p> <ul> <li><code>initialDelaySeconds</code> - the number of seconds after the container   has started before the liveness probes are initiated. Integer.</li> <li><code>timeoutSeconds</code> - the number of seconds after which the probe   times out. Integer.</li> <li><code>periodSeconds</code> - the frequency (in seconds) to perform the   probe. Integer.</li> <li><code>successThreshold</code> - the minimum consecutive successful probes   for the probe to be considered successful after a failure. Integer.</li> <li><code>failureThreshold</code> - the minimum consecutive failures for the   probe to be considered failed after having succeeded. Integer.</li> </ul> <p>Note</p> <p>Pelagia Deployment Controller specifies the following <code>livenessProbe</code> defaults for <code>mon</code>, <code>mgr</code>, <code>osd</code>, and <code>mds</code> (if CephFS is enabled):</p> <ul> <li><code>5</code> for <code>timeoutSeconds</code></li> <li><code>5</code> for <code>failureThreshold</code></li> </ul> </li> <li> <p><code>startupProbe</code> - Optional. Key-value parameter with startup probe settings for   the defined daemon types. Can be one of the following: <code>mgr</code>,   <code>mon</code>, <code>osd</code>, or <code>mds</code>. Includes the <code>disabled</code> flag and   the <code>probe</code> parameter. The <code>probe</code> parameter accepts   the following options:</p> <ul> <li><code>timeoutSeconds</code> - the number of seconds after which the probe   times out. Integer.</li> <li><code>periodSeconds</code> - the frequency (in seconds) to perform the   probe. Integer.</li> <li><code>successThreshold</code> - the minimum consecutive successful probes   for the probe to be considered successful after a failure. Integer.</li> <li><code>failureThreshold</code> - the minimum consecutive failures for the   probe to be considered failed after having succeeded. Integer.</li> </ul> </li> </ul> Example configuration of health check specification <pre><code>spec:\n  healthCheck:\n    daemonHealth:\n      mon:\n        disabled: false\n        interval: 45s\n        timeout: 600s\n      osd:\n        disabled: false\n        interval: 60s\n      status:\n        disabled: true\n    livenessProbe:\n      mon:\n        disabled: false\n        probe:\n          timeoutSeconds: 10\n          periodSeconds: 3\n          successThreshold: 3\n      mgr:\n        disabled: false\n        probe:\n          timeoutSeconds: 5\n          failureThreshold: 5\n      osd:\n        probe:\n          initialDelaySeconds: 5\n          timeoutSeconds: 10\n          failureThreshold: 7\n    startupProbe:\n      mon:\n        disabled: true\n      mgr:\n        probe:\n          successThreshold: 3\n</code></pre>"},{"location":"architecture/custom-resources/cephdeployment/#extraopts-parameters","title":"ExtraOpts parameters","text":"<ul> <li> <p><code>deviceLabels</code> - Optional. A key-value mapping which is used to assign a specification label to any   available device on a specific node. These labels can then be used for the   <code>nodes</code> section items with <code>nodeGroup</code> or <code>nodesByLabel</code> defined to   eliminate the need to specify different devices for each node individually.   Additionally, it helps in avoiding the use of device names, facilitating   the grouping of nodes with similar labels.</p> <p>Example usage:</p> <pre><code>spec:\n  extraOpts:\n    deviceLabels:\n      &lt;node-name&gt;:\n        &lt;dev-label&gt;: /dev/disk/by-id/&lt;unique_ID&gt;\n        ...\n        &lt;dev-label-n&gt;: /dev/disk/by-id/&lt;unique_ID&gt;\n      ...\n      &lt;node-name-n&gt;:\n        &lt;dev-label&gt;: /dev/disk/by-id/&lt;unique_ID&gt;\n        ...\n        &lt;dev-label-n&gt;: /dev/disk/by-id/&lt;unique_ID&gt;\n  nodes:\n  - name: &lt;group-name&gt;\n    devices:\n    - name: &lt;dev_label&gt;\n    - name: &lt;dev_label_n&gt;\n    nodes:\n    - &lt;node_name&gt;\n    - &lt;node_name_n&gt;\n</code></pre> </li> <li> <p><code>customDeviceClasses</code> - Optional. TechPreview. A list of custom device class names to use in the   specification. Enables you to specify the custom names different from   the default ones, which include <code>ssd</code>, <code>hdd</code>, and <code>nvme</code>, and use   them in nodes and pools definitions.</p> <p>Example usage:</p> <pre><code>spec:\n  extraOpts:\n    customDeviceClasses:\n    - &lt;custom_class_name&gt;\n  nodes:\n  - name: kaas-node-5bgk6\n    devices:\n      - config: # existing item\n        deviceClass: &lt;custom_class_name&gt;\n        fullPath: /dev/disk/by-id/&lt;unique_ID&gt;\n  pools:\n  - default: false\n    deviceClass: &lt;custom_class_name&gt;\n    erasureCoded:\n    codingChunks: 1\n    dataChunks: 2\n    failureDomain: host\n</code></pre> </li> </ul>"},{"location":"architecture/custom-resources/cephdeployment/#manager-modules-parameters","title":"Manager modules parameters","text":"<p><code>CephDeployment</code> specification <code>mgr</code> section contains <code>mgrModules</code> parameter. It includes the following parameters:</p> <ul> <li><code>name</code> - Ceph Manager module name.</li> <li> <p><code>enabled</code> - Flag that defines whether the Ceph Manager module   is enabled.</p> <p>For example:</p> <pre><code>spec:\n  mgr:\n    mgrModules:\n    - name: balancer\n      enabled: true\n    - name: pg_autoscaler\n      enabled: true\n</code></pre> <p>The <code>balancer</code> and <code>pg_autoscaler</code> Ceph Manager modules are  enabled by default and cannot be disabled.</p> </li> </ul> <p>Note</p> <p>Most Ceph Manager modules require additional configuration that you can perform through the <code>pelagia-lcm-tooblox</code> pod.</p>"},{"location":"architecture/custom-resources/cephdeployment/#rbd-mirroring-parameters","title":"RBD Mirroring parameters","text":"<ul> <li><code>daemonsCount</code> - Count of <code>rbd-mirror</code> daemons to spawn. We recommend using one instance of the <code>rbd-mirror</code> daemon.                                                                                                                                                                                                                                                                                                                                                                                                                  |</li> <li> <p><code>peers</code> - Optional. List of mirroring peers of an external cluster to connect to. Only a single peer is supported.    The <code>peer</code> section includes the following parameters:</p> <ul> <li><code>site</code> - the label of a remote Ceph cluster associated with the token.</li> <li><code>token</code> - the token that will be used by one site (Ceph cluster) to pull images from the other site.    To obtain the token, use the rbd mirror pool peer bootstrap create command.</li> <li><code>pools</code> - optional, a list of pool names to mirror.</li> </ul> </li> </ul>"},{"location":"architecture/custom-resources/cephdeployment/#status-fields","title":"Status fields","text":"Field Description <code>phase</code> Current handling phase of the applied Ceph cluster spec. Can equal to <code>Creating</code>, <code>Deploying</code>, <code>Validation</code>, <code>Ready</code>, <code>Deleting</code>, <code>OnHold</code> or <code>Failed</code>. <code>message</code> Detailed description of the current phase or an error message if the phase is <code>Failed</code>. <code>lastRun</code> <code>DateTime</code> when previous spec reconcile occurred. <code>clusterVersion</code> Current Ceph cluster version, for example, <code>v19.2.3</code>. <code>validation</code> Validation result (<code>Succeed</code> or <code>Failed</code>) of the spec with a list of messages, if any. The <code>validation</code> section includes the following fields:<ul><li><code>result</code> - Succeed or Failed</li><li><code>messages</code> - the list of error messages</li><li><code>lastValidatedGeneration</code> - the last validated <code>metadata.generation</code> of <code>CephDeployment</code></li></ul> <code>objRefs</code> Pelagia API object refereneces such as <code>CephDeploymentHealth</code> and <code>CephDeploymentSecret</code>."},{"location":"architecture/custom-resources/cephdeploymenthealth/","title":"CephDeploymentHealth Custom Resource","text":"<p>Verifying Ceph cluster state is an entry point for issues investigation. <code>CephDeploymentHealth</code> (<code>cephdeploymenthealths.lcm.mirantis.com</code>) custom resource (CR) allows you to verify the current health of a Ceph cluster and identify potentially problematic components. To obtain the detailed status for a particular Rook resources and Ceph cluster, which Pelagia manages:</p> <pre><code>kubectl -n pelagia get cephdeploymenthealth -o yaml\n</code></pre> <p>Example output:</p> Example CephDeploymentHealth status <pre><code>apiVersion: v1\nitems:\n- apiVersion: lcm.mirantis.com/v1alpha1\n  kind: CephDeploymentHealth\n  metadata:\n    name: pelagia-ceph\n    namespace: pelagia\n  status:\n    healthReport:\n      cephDaemons:\n        cephCSIPluginDaemons:\n          csi-cephfsplugin:\n            info:\n            - 3/3 ready\n            status: ok\n          csi-rbdplugin:\n            info:\n            - 3/3 ready\n            status: ok\n        cephDaemons:\n          mds:\n            info:\n            - 'mds active: 1/1 (cephfs ''cephfs-store'')'\n            status: ok\n          mgr:\n            info:\n            - 'a is active mgr, standbys: [b]'\n            status: ok\n          mon:\n            info:\n            - 3 mons, quorum [a b c]\n            status: ok\n          osd:\n            info:\n            - 3 osds, 3 up, 3 in\n            status: ok\n          rgw:\n            info:\n            - '2 rgws running, daemons: [21273 38213]'\n            status: ok\n      clusterDetails:\n        cephEvents:\n          PgAutoscalerDetails:\n            state: Idle\n          rebalanceDetails:\n            state: Idle\n        rgwInfo:\n          publicEndpoint: https://192.10.1.101:443\n        usageDetails:\n          deviceClasses:\n            hdd:\n              availableBytes: \"159676964864\"\n              totalBytes: \"161048690688\"\n              usedBytes: \"1371725824\"\n          pools:\n            .mgr:\n              availableBytes: \"75660169216\"\n              totalBytes: \"75661557760\"\n              usedBytes: \"1388544\"\n              usedBytesPercentage: \"0.001\"\n            .rgw.root:\n              availableBytes: \"75661426688\"\n              totalBytes: \"75661557760\"\n              usedBytes: \"131072\"\n              usedBytesPercentage: \"0.000\"\n            cephfs-store-cephfs-pool-1:\n              availableBytes: \"75661557760\"\n              totalBytes: \"75661557760\"\n              usedBytes: \"0\"\n              usedBytesPercentage: \"0.000\"\n            cephfs-store-metadata:\n              availableBytes: \"75660517376\"\n              totalBytes: \"75661557760\"\n              usedBytes: \"1040384\"\n              usedBytesPercentage: \"0.001\"\n            kubernetes-hdd:\n              availableBytes: \"75661549568\"\n              totalBytes: \"75661557760\"\n              usedBytes: \"8192\"\n              usedBytesPercentage: \"0.000\"\n            rgw-store.rgw.buckets.data:\n              availableBytes: \"75661557760\"\n              totalBytes: \"75661557760\"\n              usedBytes: \"0\"\n              usedBytesPercentage: \"0.000\"\n            rgw-store.rgw.buckets.index:\n              availableBytes: \"75661557760\"\n              totalBytes: \"75661557760\"\n              usedBytes: \"0\"\n              usedBytesPercentage: \"0.000\"\n            rgw-store.rgw.buckets.non-ec:\n              availableBytes: \"75661557760\"\n              totalBytes: \"75661557760\"\n              usedBytes: \"0\"\n              usedBytesPercentage: \"0.000\"\n            rgw-store.rgw.control:\n              availableBytes: \"75661557760\"\n              totalBytes: \"75661557760\"\n              usedBytes: \"0\"\n              usedBytesPercentage: \"0.000\"\n            rgw-store.rgw.log:\n              availableBytes: \"75660230656\"\n              totalBytes: \"75661557760\"\n              usedBytes: \"1327104\"\n              usedBytesPercentage: \"0.001\"\n            rgw-store.rgw.meta:\n              availableBytes: \"75661557760\"\n              totalBytes: \"75661557760\"\n              usedBytes: \"0\"\n              usedBytesPercentage: \"0.000\"\n            rgw-store.rgw.otp:\n              availableBytes: \"75661557760\"\n              totalBytes: \"75661557760\"\n              usedBytes: \"0\"\n              usedBytesPercentage: \"0.000\"\n      osdAnalysis:\n        cephClusterSpecGeneration: 1\n        diskDaemon:\n          info:\n          - 3/3 ready\n          status: ok\n        specAnalysis:\n          cluster-storage-worker-0:\n            status: ok\n          cluster-storage-worker-1:\n            status: ok\n          cluster-storage-worker-2:\n            status: ok\n      rookCephObjects:\n        blockStorage:\n          cephBlockPools:\n            builtin-mgr:\n              info:\n                failureDomain: host\n                type: Replicated\n              observedGeneration: 1\n              phase: Ready\n              poolID: 11\n            builtin-rgw-root:\n              info:\n                failureDomain: host\n                type: Replicated\n              observedGeneration: 1\n              phase: Ready\n              poolID: 1\n            kubernetes-hdd:\n              info:\n                failureDomain: host\n                type: Replicated\n              observedGeneration: 1\n              phase: Ready\n              poolID: 10\n        cephCluster:\n          ceph:\n            capacity:\n              bytesAvailable: 159676964864\n              bytesTotal: 161048690688\n              bytesUsed: 1371725824\n              lastUpdated: \"2025-08-15T12:10:39Z\"\n            fsid: 92d56f80-b7a8-4a35-80ef-eb6a877c2a73\n            health: HEALTH_OK\n            lastChanged: \"2025-08-14T14:07:43Z\"\n            lastChecked: \"2025-08-15T12:10:39Z\"\n            previousHealth: HEALTH_WARN\n            versions:\n              mds:\n                ceph version 19.2.3 (c92aebb279828e9c3c1f5d24613efca272649e62) squid (stable): 2\n              mgr:\n                ceph version 19.2.3 (c92aebb279828e9c3c1f5d24613efca272649e62) squid (stable): 2\n              mon:\n                ceph version 19.2.3 (c92aebb279828e9c3c1f5d24613efca272649e62) squid (stable): 3\n              osd:\n                ceph version 19.2.3 (c92aebb279828e9c3c1f5d24613efca272649e62) squid (stable): 3\n              overall:\n                ceph version 19.2.3 (c92aebb279828e9c3c1f5d24613efca272649e62) squid (stable): 12\n              rgw:\n                ceph version 19.2.3 (c92aebb279828e9c3c1f5d24613efca272649e62) squid (stable): 2\n          conditions:\n          - lastHeartbeatTime: \"2025-08-15T12:10:40Z\"\n            lastTransitionTime: \"2025-08-12T09:35:27Z\"\n            message: Cluster created successfully\n            reason: ClusterCreated\n            status: \"True\"\n            type: Ready\n          message: Cluster created successfully\n          observedGeneration: 1\n          phase: Ready\n          state: Created\n          storage:\n            deviceClasses:\n            - name: hdd\n            osd:\n              migrationStatus: {}\n              storeType:\n                bluestore: 3\n          version:\n            image: 127.0.0.1/ceph/ceph:v19.2.3\n            version: 19.2.3-0\n        objectStorage:\n          cephObjectStore:\n            rgw-store:\n              endpoints:\n                insecure:\n                - http://rook-ceph-rgw-rgw-store.rook-ceph.svc:8081\n                secure:\n                - https://rook-ceph-rgw-rgw-store.rook-ceph.svc:8443\n              info:\n                endpoint: http://rook-ceph-rgw-rgw-store.rook-ceph.svc:8081\n                secureEndpoint: https://rook-ceph-rgw-rgw-store.rook-ceph.svc:8443\n              observedGeneration: 1\n              phase: Ready\n        sharedFilesystem:\n          cephFilesystems:\n            cephfs-store:\n              observedGeneration: 1\n              phase: Ready\n      rookOperator:\n        status: ok\n    lastHealthCheck: \"2025-08-15T12:11:00Z\"\n    lastHealthUpdate: \"2025-08-15T12:11:00Z\"\n    state: Ok\nkind: List\nmetadata:\n  resourceVersion: \"\"\n</code></pre> <p>To understand the status of a <code>CephDeploymentHealth</code>, learn the following:</p> <ul> <li>High-level status fields</li> <li>Health report status fields</li> </ul>"},{"location":"architecture/custom-resources/cephdeploymenthealth/#high-level-status-fields","title":"High-level status fields","text":"Field Description <code>healthReport</code> Complete information about Ceph cluster including cluster, Ceph resources, and daemon health. It helps reveal potentially problematic components. <code>lastHealthCheck</code> <code>DateTime</code> when previous cluster state check occurred. <code>lastHealthUpdate</code> <code>DateTime</code> when previous cluster state update occurred. <code>issues</code> List of strings of all issues found during cluster state check. <code>state</code> Cluster state that can be <code>Ok</code> or <code>Failed</code> depending on the Ceph cluster state check."},{"location":"architecture/custom-resources/cephdeploymenthealth/#health-report-status-fields","title":"Health report status fields","text":"<ul> <li> <p><code>rookOperator</code> - State of the Rook Ceph Operator pod which contains the following fields:</p> <ul> <li><code>status</code> - contains short state of current Rook Ceph Operator pod;</li> <li><code>issues</code> - represents found Rook Operator issues, otherwise it is empty;</li> </ul> </li> <li> <p><code>rookCephObjects</code> - General information from Rook about the Ceph cluster health and current   state. Contains the following fields:</p> <ul> <li><code>cephCluster</code> - Contains Ceph cluster status information.</li> <li><code>blockStorage</code> - Contains status of block-storage related objects status information.</li> <li><code>cephClients</code> - Represents a key-value mapping of Ceph client's name and its status.</li> <li><code>objectStorage</code> - Contains status of object-storage related objects status information.</li> <li><code>sharedFilesystems</code> - Contains status of shared filesystems related objects status information.</li> </ul> <p> Example rookCephObjects status <pre><code>status:\n  healthReport:\n    rookCephObjects:\n      cephCluster:\n        state: &lt;rook ceph cluster common status&gt;\n        phase: &lt;rook ceph cluster spec reconcile phase&gt;\n        message: &lt;rook ceph cluster phase details&gt;\n        conditions: &lt;history of rook ceph cluster reconcile steps&gt;\n        ceph: &lt;ceph cluster health&gt;\n        storage:\n          deviceClasses: &lt;list of used device classes in ceph cluster&gt;\n        version:\n          image: &lt;ceph image used in ceph cluster&gt;\n          version: &lt;ceph version of ceph cluster&gt;\n    blockStorage:\n      cephBlockPools:\n        &lt;cephBlockPoolName&gt;:\n          ...\n          phase: &lt;rook ceph block pool resource phase&gt;\n    cephClients:\n      &lt;cephClientName&gt;:\n        ...\n        phase: &lt;rook ceph client resource phase&gt;\n    objectStorage:\n      cephObjectStore:\n        &lt;cephObjectStoreName&gt;:\n          ...\n          phase: &lt;rook ceph object store resource phase&gt;\n      cephObjectStoreUsers:\n        &lt;rgwUserName&gt;:\n          ...\n          phase: &lt;rook ceph object store user resource phase&gt;\n      objectBucketClaims:\n        &lt;bucketName&gt;:\n          ...\n          phase: &lt;rook ceph object bucket claims resource phase&gt;\n      cephObjectRealms:\n        &lt;realmName&gt;:\n          ...\n          phase: &lt;rook ceph object store realm resource phase&gt;\n      cephObjectZoneGroups:\n        &lt;zonegroupName&gt;:\n          ...\n          phase: &lt;rook ceph object store zonegroup resource phase&gt;\n      cephObjectZones:\n        &lt;zoneName&gt;:\n          ...\n          phase: &lt;rook ceph object store zone resource phase&gt;\n    sharedFilesystems:\n      cephFilesystems:\n        &lt;cephFSName&gt;:\n          ...\n          phase: &lt;rook ceph filesystem resource phase&gt;\n</code></pre> </p> </li> <li> <p><code>cephDaemons</code> - Contains information about the state of the Ceph and Ceph CSI daemons in the cluster.   Includes the following fields:</p> <ul> <li><code>cephDaemons</code> - Map of statuses for each Ceph cluster daemon type. Indicates the   expected and actual number of Ceph daemons on the cluster. Available   daemon types are: <code>mgr</code>, <code>mon</code>, <code>osd</code>, and <code>rgw</code>.</li> <li><code>cephCSIPluginDaemons</code> - Contains information, similar to the <code>daemonsStatus</code> format, for each   Ceph CSI plugin deployed in the Ceph cluster: <code>rbd</code> and <code>cephfs</code>.</li> </ul> <p> Example cephDaemons status <pre><code>status:\n  healthReport:\n    cephDaemons:\n      cephCSIPluginDaemons:\n        csi-cephfsplugin:\n          info:\n          - 3/3 ready\n          status: ok\n        csi-rbdplugin:\n          info:\n          - 3/3 ready\n          status: ok\n      cephDaemons:\n        mds:\n          info:\n          - 'mds active: 1/1 (cephfs ''cephfs-store'')'\n          status: ok\n        mgr:\n          info:\n          - 'a is active mgr, standbys: [b]'\n          status: ok\n        mon:\n          info:\n          - 3 mons, quorum [a b c]\n          status: ok\n        osd:\n          info:\n          - 3 osds, 3 up, 3 in\n          status: ok\n        rgw:\n          info:\n          - '2 rgws running, daemons: [21273 38213]'\n          status: ok\n</code></pre> </p> </li> <li> <p><code>clusterDetails</code> - Verbose details of the Ceph cluster state. Contains the following fields:</p> <ul> <li><code>usageDetails</code> - Describes the used, available, and total storage size for each   <code>deviceClass</code> and <code>pool</code>.</li> <li><code>cephEvents</code> - Contains info about current ceph events happen in Ceph cluster   if progress events module is enabled.</li> <li><code>rgwInfo</code> - represents additional Ceph Object Storage Multisite information like public endpoint   to connect external zone and sync statuses.</li> </ul> <p> Example clusterDetails status <pre><code>status:\n  healthReport:\n    clusterDetails:\n      cephEvents:\n        PgAutoscalerDetails:\n          state: Idle\n        rebalanceDetails:\n          state: Idle\n      rgwInfo:\n        publicEndpoint: https://192.10.1.101:443\n      usageDetails:\n        deviceClasses:\n          hdd:\n            availableBytes: \"159681224704\"\n            totalBytes: \"161048690688\"\n            usedBytes: \"1367465984\"\n        pools:\n          .mgr:\n            availableBytes: \"75660169216\"\n            totalBytes: \"75661557760\"\n            usedBytes: \"1388544\"\n            usedBytesPercentage: \"0.001\"\n          .rgw.root:\n            availableBytes: \"75661426688\"\n            totalBytes: \"75661557760\"\n            usedBytes: \"131072\"\n            usedBytesPercentage: \"0.000\"\n          cephfs-store-cephfs-pool-1:\n            availableBytes: \"75661557760\"\n            totalBytes: \"75661557760\"\n            usedBytes: \"0\"\n            usedBytesPercentage: \"0.000\"\n          cephfs-store-metadata:\n            availableBytes: \"75660517376\"\n            totalBytes: \"75661557760\"\n            usedBytes: \"1040384\"\n            usedBytesPercentage: \"0.001\"\n          kubernetes-hdd:\n            availableBytes: \"75661549568\"\n            totalBytes: \"75661557760\"\n            usedBytes: \"8192\"\n            usedBytesPercentage: \"0.000\"\n          rgw-store.rgw.buckets.data:\n            availableBytes: \"75661557760\"\n            totalBytes: \"75661557760\"\n            usedBytes: \"0\"\n            usedBytesPercentage: \"0.000\"\n          ...\n          rgw-store.rgw.otp:\n            availableBytes: \"75661557760\"\n            totalBytes: \"75661557760\"\n            usedBytes: \"0\"\n            usedBytesPercentage: \"0.000\"\n</code></pre> </p> </li> <li> <p><code>osdAnalysis</code> - Ceph OSD analysis results based on Rook <code>CephCluster</code> specification and <code>disk-daemon</code> reports.   Contains the following fields:</p> <ul> <li><code>diskDaemon</code> - Disk daemon status. Disk daemon is Pelagia LCM component that provides information about   nodes' devices and their usage by Ceph OSDs.</li> <li><code>cephClusterSpecGeneration</code> - Last validated Rook <code>CephCluster</code> specification generation.</li> <li><code>specAnalysis</code> - Map of per-node analysis results based on the Rook <code>CephCluster</code> specification.</li> </ul> <p> Example osdAnalysis status <pre><code>status:\n  healthReport:\n    osdAnalysis:\n      cephClusterSpecGeneration: 1\n      diskDaemon:\n        info:\n        - 3/3 ready\n        status: ok\n      specAnalysis:\n        cluster-storage-worker-0:\n          status: ok\n        cluster-storage-worker-1:\n          status: ok\n        cluster-storage-worker-2:\n          status: ok\n</code></pre> </p> </li> </ul>"},{"location":"architecture/custom-resources/cephdeploymentsecret/","title":"CephDeploymentSecret Custom Resource","text":"<p><code>CephDeploymentSecret</code> (<code>cephdeploymentsecrets.lcm.mirantis.com</code>) custom resource (CR) contains the information about all Ceph RBD/RGW credential secrets to be used for the access to Ceph cluster. To obtain the resource, run the following command:</p> <pre><code>kubectl -n pelagia get cephdeploymentsecret -o yaml\n</code></pre> <p>Example output:</p> Example CephDeploymentSecret output <pre><code>apiVersion: v1\nitems:\n- apiVersion: lcm.mirantis.com/v1alpha1\n  kind: CephDeploymentSecret\n  metadata:\n    name: pelagia-ceph\n    namespace: pelagia\n  status:\n    lastSecretCheck: \"2025-08-15T12:22:11Z\"\n    lastSecretUpdate: \"2025-08-15T12:22:11Z\"\n    secretInfo:\n      clientSecrets:\n      - name: client.admin\n        secretName: rook-ceph-admin-keyring\n        secretNamespace: rook-ceph\n      rgwUserSecrets:\n      - name: test-user\n        secretName: rook-ceph-object-user-rgw-store-test-user\n        secretNamespace: rook-ceph\n    state: Ok\nkind: List\nmetadata:\n  resourceVersion: \"\"\n</code></pre> <p>To understand the status of a <code>CephDeploymentHealth</code>, learn the following:</p> <ul> <li>High-level status fields</li> <li>Secret info fields</li> </ul>"},{"location":"architecture/custom-resources/cephdeploymentsecret/#high-level-status-fields","title":"High-level status fields","text":"<p>The <code>CephDeploymentSecret</code> custom resource contains the following high-level status fields:</p> Field Description <code>state</code> Current state of the secret collector on the Ceph cluster:  - <code>Ready</code> - information about secrets is collected successfully  - <code>Failed</code> - information about secrets fails to be collected <code>lastSecretCheck</code> <code>DateTime</code> when the Ceph cluster secrets were verified last time. <code>lastSecretUpdate</code> <code>DateTime</code> when the Ceph cluster secrets were updated last time. <code>secretsInfo</code> List of secrets for Ceph <code>authx</code> clients and RADOS Gateway users. For details, see Secret info fields. <code>messages</code> List of error or warning messages, if any, found when collecting information about the Ceph cluster."},{"location":"architecture/custom-resources/cephdeploymentsecret/#secret-info-fields","title":"Secret info fields","text":"<p>The <code>secretsInfo</code> field contains the following fields:</p> <ul> <li><code>clientSecrets</code> - Details on secrets for Ceph clients such as <code>name</code>, <code>secretName</code>, and <code>secretNamespace</code>   for each client secret.</li> <li><code>rgwUserSecrets</code> - Details on secrets for Ceph RADOS Gateway users such as <code>name</code>, <code>secretName</code>, and   <code>secretNamespace</code>.</li> </ul> <p>Example of the <code>secretsInfo</code> field:</p> Example *secretsInfo* field <pre><code>status:\n  secretInfo:\n    clientSecrets:\n    - name: client.admin\n      secretName: rook-ceph-admin-keyring\n      secretNamespace: rook-ceph\n    rgwUserSecrets:\n    - name: test-user\n      secretName: rook-ceph-object-user-rgw-store-test-user\n      secretNamespace: rook-ceph\n</code></pre>"},{"location":"architecture/custom-resources/cephosdremovetask/","title":"CephOsdRemoveTask custom resource","text":"<p>This section describes the <code>CephOsdRemoveTask</code> custom resource specification. For the procedure workflow, see Creating a Ceph OSD removal task.</p> <p>The following sections describe the <code>CephOsdRemoveTask</code> custom resource specification:</p> <ul> <li> <p>Spec parameters</p> <ul> <li>Nodes parameters</li> </ul> </li> <li> <p>Status fields</p> <ul> <li>Remove info fields</li> <li>Remove info examples</li> </ul> </li> </ul>"},{"location":"architecture/custom-resources/cephosdremovetask/#spec-parameters","title":"Spec parameters","text":"Parameter Description <code>nodes</code> Map of Kubernetes nodes that specifies how to remove Ceph OSDs: by host-devices or OSD IDs. For details, see Nodes parameters. <code>approve</code> Flag that indicates whether a request is ready to execute removal. Can only be manually enabled by the Operator. Defaults to <code>false</code>. <code>resolved</code> Optional. Flag that marks a finished request, even if it failed, to keep it in historydo not block any further operations."},{"location":"architecture/custom-resources/cephosdremovetask/#nodes-parameters","title":"Nodes parameters","text":"<ul> <li><code>completeCleanUp</code> - Flag used to clean up an entire node and drop it from the CRUSH map.   Mutually exclusive with <code>cleanupByDevice</code> and <code>cleanupByOsd</code>.</li> <li><code>dropFromCrush</code> - Flag which is the same to CompleteCleanup, but without device cleanup on node   May be useful when node is going to be reprovisioned and no need to spend time for device cleanup.</li> <li> <p><code>cleanupByDevice</code> - List that describes devices to clean up by name or device symlink as they   were specified in Rook <code>CephCluster</code> (or Pelagia <code>CephDeployment</code>). Mutually exclusive with   <code>completeCleanUp</code> and <code>cleanupByOsd</code>. Includes the following parameters:</p> <ul> <li><code>device</code> - Physical device name, e.g. <code>sdb</code>, <code>/dev/nvme1e0</code>, or   device symlink (<code>by-path</code> or <code>by-id</code>), e.g. <code>/dev/disk/by-path/pci-0000:00:1c.5</code>   or <code>/dev/disk/by-id/scsi-SATA_HGST_HUS724040AL_PN1334PEHN18ZS</code>.</li> <li><code>skipDeviceCleanup</code> - Optional. Flag that indicates whether to skip the device cleanup.   Defaults to <code>false</code>. If set to <code>true</code>, the device will not be cleaned up, but   OSDs running on this device will be removed from the CRUSH map and deleted.</li> </ul> <p>Warning</p> <p>We do not recommend using device name or device <code>by-path</code> symlink in the <code>cleanupByDevice</code> field as these identifiers are not persistent and can change at node boot. Remove Ceph OSDs with device <code>by-id</code> symlinks or use <code>cleanupByOsd</code> instead.</p> </li> <li> <p><code>cleanupByOsd</code> - List of Ceph OSD IDs to remove. Mutually exclusive with   <code>completeCleanUp</code> and <code>cleanupByDevice</code>. Includes the following parameters:</p> <ul> <li><code>id</code> - Ceph OSD ID to remove.</li> <li><code>skipDeviceCleanup</code> - Optional. Flag that indicates whether to skip the device cleanup.   Defaults to <code>false</code>. If set to <code>true</code>, the device will not be cleaned up, but   OSDs running on this device will be removed from the CRUSH map and deleted.</li> </ul> </li> <li> <p><code>cleanupStrayPartitions</code> - Flag used to for cleaning disks with osd lvm partitions   which not belong to current cluster, for example, when disk was not cleaned up previously.</p> </li> </ul> <p>Example of <code>CephOsdRemoveTask</code> with <code>spec.nodes</code>:</p> Example CephOsdRemoveTask nodes specification <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephOsdRemoveTask\nmetadata:\n  name: remove-osd-task\n  namespace: pelagia\nspec:\n  nodes:\n    \"node-a\":\n      completeCleanUp: true\n    \"node-b\":\n      cleanupByOsd:\n      - id: 1\n      - id: 15\n      - id: 25\n    \"node-c\":\n      cleanupByDevice:\n      - device: \"sdb\"\n      - device: \"/dev/disk/by-path/pci-0000:00:1c.5\"\n      - device: \"/dev/disk/by-id/scsi-SATA_HGST_HUS724040AL_PN1334PEHN18ZS\"\n    \"node-d\":\n      dropFromCrush: true\n</code></pre> <p>The example above includes the following actions:</p> <ul> <li>For <code>node-a</code>, full cleanup, including all OSDs on the node, node drop from   the CRUSH map, and cleanup of all disks used for Ceph OSDs on this node.</li> <li>For <code>node-b</code>, cleanup of Ceph OSDs with IDs <code>1</code>, <code>15</code>, and <code>25</code>   along with the related disk information.</li> <li>For <code>node-c</code>, cleanup of the device with name <code>sdb</code>, the device with   path ID <code>/dev/disk/by-path/pci-0000:00:1c.5</code>, and the device with <code>by-id</code> <code>/dev/disk/by-id/scsi-SATA_HGST_HUS724040AL_PN1334PEHN18ZS</code>,   dropping of OSDs running on these devices.</li> <li>For <code>node-d</code>, cleanup, including all OSDs on the node, node drop from   the CRUSH map but skip cleanup of all disks used for Ceph OSDs on this node.</li> </ul>"},{"location":"architecture/custom-resources/cephosdremovetask/#status-fields","title":"Status fields","text":"Field Description <code>phase</code> Describes the current task phase. <code>phaseInfo</code> Additional human-readable message describing task phase. <code>removeInfo</code> The overall information about the Ceph OSDs to remove: final removal map, issues, and warnings. Once the <code>Processing</code> phase succeeds, <code>removeInfo</code> will be extended with the removal status for each node and Ceph OSD. In case of an entire node removal, the status will contain the status itself and an error message, if any. <code>messages</code> Informational messages describing the reason for the request transition to the next phase. <code>conditions</code> History of spec updates for the request. <p><code>CephOsdRemoveTask</code> phases are moving in the following order:</p> <p></p> <p>Here are the following final phases:</p> <ul> <li><code>ValidationFailed</code> - The task is not valid and cannot be processed.</li> <li><code>Aborted</code> - The task detected inappropriate Rook <code>CephCluster</code> spec changes after receiving approval.</li> <li><code>Completed</code> - The task is successfully completed.</li> <li><code>CompletedWithWarnings</code> - The task is completed but some steps are skipped.</li> <li><code>Failed</code> - The task is failed on one of the steps.</li> </ul>"},{"location":"architecture/custom-resources/cephosdremovetask/#remove-info-fields","title":"Remove info fields","text":"Field Description <code>cleanupMap</code> Map of desired nodes and devices to clean up. Based on this map, the cloud operator decides whether to approve the current task or not. After approve, it will contain all statuses and errors happened during cleanup. <code>issues</code> List of error messages found during validation or processing phases <code>warnings</code> List of non-blocking warning messages found during validation or processing phases <p><code>cleanupMap</code> is a map of nodes to devices contains the following fields:</p> <ul> <li><code>completeCleanup</code> - Flag that indicates whether to perform a full cleanup of the node.</li> <li><code>dropFromCrush</code> - Flag that indicates whether to drop the node from the CRUSH map without node cleanup.</li> <li> <p><code>osdMapping</code> - Map of Ceph OSD IDs to the device names or symlink used for the Ceph OSD. It includes device info and   statuses of Ceph OSD remove itself, Rook Ceph OSD deployment remove, Ceph OSD device cleanup job.</p> <p> CephOsdRemoveTask osdMapping example output <pre><code>status:\n  removeInfo:\n    cleanupMap:\n      \"node-a\":\n        completeCleanUp: true\n        osdMapping:\n          \"2\":\n            deviceMapping:\n              \"sdb\":\n                path: \"/dev/disk/by-path/pci-0000:00:0a.0\"\n                partition: \"/dev/ceph-a-vg_sdb/osd-block-b-lv_sdb\"\n                type: \"block\"\n                class: \"hdd\"\n                zapDisk: true\n          \"6\":\n            deviceMapping:\n              \"sdc\":\n                path: \"/dev/disk/by-path/pci-0000:00:0c.0\"\n                partition: \"/dev/ceph-a-vg_sdc/osd-block-b-lv_sdc-1\"\n                type: \"block\"\n                class: \"hdd\"\n                zapDisk: true\n          \"11\":\n            deviceMapping:\n              \"sdc\":\n                path: \"/dev/disk/by-path/pci-0000:00:0c.0\"\n                partition: \"/dev/ceph-a-vg_sdc/osd-block-b-lv_sdc-2\"\n                type: \"block\"\n                class: \"hdd\"\n                zapDisk: true\n</code></pre> </p> </li> <li> <p><code>nodeIsDown</code> - Flag that indicates whether the node is down.</p> </li> <li><code>volumeInfoMissed</code> - Flag that indicates whether <code>disk-daemon</code> collected device info or not.</li> <li><code>hostRemoveStatus</code> - Node removal status if node is marked for complete cleanup.</li> </ul>"},{"location":"architecture/custom-resources/cephosdremovetask/#remove-info-examples","title":"Remove info examples","text":"<p>Example of <code>status.removeInfo</code> after successful <code>Validation</code>:</p> Example output <pre><code>status:\n  removeInfo:\n    cleanupMap:\n      \"node-a\":\n        completeCleanUp: true\n        osdMapping:\n          \"2\":\n            deviceMapping:\n              \"sdb\":\n                path: \"/dev/disk/by-path/pci-0000:00:0a.0\"\n                partition: \"/dev/ceph-a-vg_sdb/osd-block-b-lv_sdb\"\n                type: \"block\"\n                class: \"hdd\"\n                zapDisk: true\n          \"6\":\n            deviceMapping:\n              \"sdc\":\n                path: \"/dev/disk/by-path/pci-0000:00:0c.0\"\n                partition: \"/dev/ceph-a-vg_sdc/osd-block-b-lv_sdc-1\"\n                type: \"block\"\n                class: \"hdd\"\n                zapDisk: true\n          \"11\":\n            deviceMapping:\n              \"sdc\":\n                path: \"/dev/disk/by-path/pci-0000:00:0c.0\"\n                partition: \"/dev/ceph-a-vg_sdc/osd-block-b-lv_sdc-2\"\n                type: \"block\"\n                class: \"hdd\"\n                zapDisk: true\n      \"node-b\":\n        osdMapping:\n          \"1\":\n            deviceMapping:\n              \"sdb\":\n                path: \"/dev/disk/by-path/pci-0000:00:0a.0\"\n                partition: \"/dev/ceph-b-vg_sdb/osd-block-b-lv_sdb\"\n                type: \"block\"\n                class: \"ssd\"\n                zapDisk: true\n          \"15\":\n            deviceMapping:\n              \"sdc\":\n                path: \"/dev/disk/by-path/pci-0000:00:0b.1\"\n                partition: \"/dev/ceph-b-vg_sdc/osd-block-b-lv_sdc\"\n                type: \"block\"\n                class: \"ssd\"\n                zapDisk: true\n          \"25\":\n            deviceMapping:\n              \"sdd\":\n                path: \"/dev/disk/by-path/pci-0000:00:0c.2\"\n                partition: \"/dev/ceph-b-vg_sdd/osd-block-b-lv_sdd\"\n                type: \"block\"\n                class: \"ssd\"\n                zapDisk: true\n      \"node-c\":\n        osdMapping:\n          \"0\":\n            deviceMapping:\n              \"sdb\":\n                path: \"/dev/disk/by-path/pci-0000:00:1t.9\"\n                partition: \"/dev/ceph-c-vg_sdb/osd-block-c-lv_sdb\"\n                type: \"block\"\n                class: \"hdd\"\n                zapDisk: true\n          \"8\":\n            deviceMapping:\n              \"sde\":\n                path: \"/dev/disk/by-path/pci-0000:00:1c.5\"\n                partition: \"/dev/ceph-c-vg_sde/osd-block-c-lv_sde\"\n                type: \"block\"\n                class: \"hdd\"\n                zapDisk: true\n              \"sdf\":\n                path: \"/dev/disk/by-path/pci-0000:00:5a.5\",\n                partition: \"/dev/ceph-c-vg_sdf/osd-db-c-lv_sdf-1\",\n                type: \"db\",\n                class: \"ssd\"\n</code></pre> <p>During the <code>Validation</code> phase, the provided information was validated and reflects the final map of the Ceph OSDs to remove:</p> <ul> <li>For <code>node-a</code>, Ceph OSDs with IDs <code>2</code>, <code>6</code>, and <code>11</code> will be removed   with the related disk and its information: all block devices, names, paths,   and disk class.</li> <li>For <code>node-b</code>, the Ceph OSDs with IDs <code>1</code>, <code>15</code>, and  <code>25</code> will be   removed with the related disk information.</li> <li>For <code>node-c</code>, the Ceph OSD with ID <code>8</code> will be removed, which is placed   on the specified <code>sdb</code> device. The related partition on the <code>sdf</code> disk,   which is used as the BlueStore metadata device, will be cleaned up keeping   the disk itself untouched. Other partitions on that device will not be   touched.</li> </ul> <p>Example of <code>removeInfo</code> with <code>removeStatus</code> succeeded:</p> Example output <pre><code>status:\n  removeInfo:\n    cleanupMap:\n      \"node-a\":\n        completeCleanUp: true\n        hostRemoveStatus:\n          status: Removed\n        osdMapping:\n          \"2\":\n            removeStatus:\n              osdRemoveStatus:\n                status: Removed\n              deploymentRemoveStatus:\n                status: Removed\n                name: \"rook-ceph-osd-2\"\n              deviceCleanUpJob:\n                status: Finished\n                name: \"job-name-for-osd-2\"\n            deviceMapping:\n              \"sdb\":\n                path: \"/dev/disk/by-path/pci-0000:00:0a.0\"\n                partition: \"/dev/ceph-a-vg_sdb/osd-block-b-lv_sdb\"\n                type: \"block\"\n                class: \"hdd\"\n                zapDisk: true\n</code></pre> <p>Example of <code>removeInfo</code> with <code>removeStatus</code> failed:</p> Example output <pre><code>status:\n  removeInfo:\n    cleanupMap:\n      \"node-a\":\n        completeCleanUp: true\n        osdMapping:\n          \"2\":\n            removeStatus:\n              osdRemoveStatus:\n                error: \"retries for cmd \u2018ceph osd ok-to-stop 2\u2019 exceeded\"\n                status: Failed\n            deviceMapping:\n              \"sdb\":\n                path: \"/dev/disk/by-path/pci-0000:00:0a.0\"\n                partition: \"/dev/ceph-a-vg_sdb/osd-block-b-lv_sdb\"\n                type: \"block\"\n                class: \"hdd\"\n                zapDisk: true\n</code></pre> <p>Example of <code>removeInfo</code> with <code>removeStatus</code> failed by timeout:</p> Example output <pre><code>status:\n  removeInfo:\n    cleanupMap:\n      \"node-a\":\n        completeCleanUp: true\n        osdMapping:\n          \"2\":\n            removeStatus:\n              osdRemoveStatus:\n                error: Timeout (30m0s) reached for waiting pg rebalance for osd 2\n                status: Failed\n            deviceMapping:\n              \"sdb\":\n                path: \"/dev/disk/by-path/pci-0000:00:0a.0\"\n                partition: \"/dev/ceph-a-vg_sdb/osd-block-b-lv_sdb\"\n                type: \"block\"\n                class: \"hdd\"\n                zapDisk: true\n</code></pre> <p>Note</p> <p>In case of failures similar to the examples above, review the <code>pelagia-lcm-controller</code> logs and both statuses of Rook <code>CephCluster</code> and Pelagia <code>CephDeploymentHealth</code>. Such failures may simply indicate timeout and retry issues. If no other issues were found, re-create the request with a new name and skip adding successfully removed Ceph OSDS or Ceph nodes.</p>"},{"location":"ops-guide/deployment/cephfs/","title":"Enable Ceph Filesystem","text":""},{"location":"ops-guide/deployment/cephfs/#configure-ceph-shared-file-system-cephfs","title":"Configure Ceph Shared File System (CephFS)","text":"<p>The Ceph Shared File System, or CephFS, provides the ability to create read/write shared file system Persistent Volumes (PVs). These PVs support the <code>ReadWriteMany</code> access mode for the <code>FileSystem</code> volume mode. CephFS deploys its own daemons called MetaData Servers or Ceph MDS. For details, see Ceph Documentation: Ceph File System.</p> <p>Note</p> <p>By design, CephFS data pool and metadata pool must be <code>replicated</code> only.</p>"},{"location":"ops-guide/deployment/cephfs/#cephfs-specification-parameters","title":"CephFS specification parameters","text":"<p>The <code>CephDeployment</code> custom resource (CR) <code>spec</code> includes the <code>sharedFilesystem.cephFS</code> section with the following CephFS parameters:</p> <ul> <li><code>name</code> - CephFS instance name.</li> <li> <p><code>dataPools</code> - A list of CephFS data pool specifications. Each spec contains the <code>name</code>, <code>replicated</code> or   <code>erasureCoded</code>, <code>deviceClass</code>, and <code>failureDomain</code> parameters. The first pool in the list is treated   as the default data pool for CephFS and must always be <code>replicated</code>. The <code>failureDomain</code> parameter   may be set to <code>host</code>, <code>rack</code> and so on, defining the failure domain across which the data will   be spread. The number of data pools is unlimited, but the default pool must always be present. For example:</p> <pre><code>spec:\n  sharedFilesystem:\n    cephFS:\n    - name: cephfs-store\n      dataPools:\n      - name: default-pool\n        deviceClass: ssd\n        replicated:\n          size: 3\n        failureDomain: host\n      - name: second-pool\n        deviceClass: hdd\n        failureDomain: rack\n        erasureCoded:\n          dataChunks: 2\n          codingChunks: 1\n</code></pre> <p>where <code>replicated.size</code> is the number of full copies of data on multiple nodes.</p> <p>Warning</p> <p>When using the non-recommended Ceph pools <code>replicated.size</code> of less than <code>3</code>, Ceph OSD removal cannot be performed. The minimal replica size equals a rounded up half of the specified <code>replicated.size</code>.</p> <p>For example, if <code>replicated.size</code> is <code>2</code>, the minimal replica size is <code>1</code>, and if <code>replicated.size</code> is <code>3</code>, then the minimal replica size is <code>2</code>. The replica size of <code>1</code> allows Ceph having PGs with only one Ceph OSD in the <code>acting</code> state, which may cause a <code>PG_TOO_DEGRADED</code> health warning that blocks Ceph OSD removal. We recommend setting <code>replicated.size</code> to <code>3</code> for each Ceph pool.</p> <p>Warning</p> <p>Modifying of <code>dataPools</code> on a deployed CephFS has no effect. You can manually adjust pool settings through the Ceph CLI. However, for any changes in <code>dataPools</code>, we recommend re-creating CephFS.</p> </li> <li> <p><code>metadataPool</code> - CephFS metadata pool spec that should only contain <code>replicated</code>, <code>deviceClass</code>, and <code>failureDomain</code>   parameters. The <code>failureDomain</code> parameter may be set to <code>host</code>, <code>rack</code> and so on,   defining the failure domain across which the data will be spread. Can use only <code>replicated</code> settings. For example:</p> <pre><code>spec:\n  sharedFilesystem:\n    cephFS:\n    - name: cephfs-store\n      metadataPool:\n        deviceClass: nvme\n        replicated:\n          size: 3\n        failureDomain: host\n</code></pre> <p>where <code>replicated.size</code> is the number of full copies of data on multiple nodes.</p> <p>Warning</p> <p>Modifying of <code>metadataPool</code> on a deployed CephFS has no effect. You can manually adjust pool settings through the Ceph CLI. However, for any changes in <code>metadataPool</code>, we recommend re-creating CephFS.</p> </li> <li> <p><code>preserveFilesystemOnDelete</code> - Defines whether to delete the data and metadata pools if CephFS is   deleted. Set to <code>true</code> to avoid occasional data loss in case of human error. However, for security reasons,   we recommend setting <code>preserveFilesystemOnDelete</code> to <code>false</code>.</p> </li> <li> <p><code>metadataServer</code> - Metadata Server settings correspond to the Ceph MDS daemon settings. Contains the following fields:</p> <ul> <li><code>activeCount</code> - the number of active Ceph MDS instances. As a load   increases, CephFS will automatically partition the file system   across the Ceph MDS instances. Rook will create double the number   of Ceph MDS instances as requested by <code>activeCount</code>. The extra   instances will be in the standby mode for failover. We recommend specifying this parameter to <code>1</code> and   increasing the MDS daemons count only in case of a high load.</li> <li><code>activeStandby</code> - defines whether the extra Ceph MDS instances   will be in active standby mode and will keep a warm cache of the   file system metadata for faster failover. CephFS will assign the instances in failover pairs.   If <code>false</code>, the extra Ceph MDS instances will all be in passive standby mode and will   not maintain a warm cache of the metadata. The default value is <code>false</code>.</li> <li><code>resources</code> - represents Kubernetes resource requirements for Ceph MDS pods.</li> </ul> <p>For example:</p> <pre><code>spec:\n  sharedFilesystem:\n    cephFS:\n    - name: cephfs-store\n      metadataServer:\n        activeCount: 1\n        activeStandby: false\n        resources: # example, non-prod values\n          requests:\n            memory: 1Gi\n            cpu: 1\n          limits:\n            memory: 2Gi\n            cpu: 2\n</code></pre> </li> </ul>"},{"location":"ops-guide/deployment/cephfs/#configure-cephfs","title":"Configure CephFS","text":"<ol> <li> <p>Optional. Override the CSI CephFS gRPC and liveness metrics port. For example, if an application is already using    the default CephFS ports <code>9092</code> and <code>9082</code>, which may cause conflicts on the node. Upgrade Pelagia Helm release    values with desired port numbers:</p> <pre><code>helm upgrade --install pelagia-ceph oci://registry.mirantis.com/pelagia/pelagia-ceph --version 1.0.0 -n pelagia \\\n     --set rookConfig.csiCephFsGPCMetricsPort=&lt;desiredPort&gt;,rookConfig.csiCephFsLivenessMetricsPort=&lt;desiredPort&gt;\n</code></pre> <p>Rook will enable the CephFS CSI plugin and provisioner.</p> </li> <li> <p>Open the <code>CephDeployment</code> CR for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>In the <code>sharedFilesystem</code> section, specify parameters according to CephFS specification. For example:    <pre><code>spec:\n  sharedFilesystem:\n    cephFS:\n    - name: cephfs-store\n      dataPools:\n      - name: cephfs-pool-1\n        deviceClass: hdd\n        replicated:\n          size: 3\n        failureDomain: host\n      metadataPool:\n        deviceClass: nvme\n        replicated:\n          size: 3\n        failureDomain: host\n      metadataServer:\n        activeCount: 1\n        activeStandby: false\n</code></pre></p> </li> <li> <p>Define the <code>mds</code> role for the corresponding nodes where Ceph MDS daemons should be deployed. We recommend    labeling only one node with the <code>mds</code> role. For example:    <pre><code>spec:\n  nodes:\n    ...\n    worker-1:\n      roles:\n      ...\n      - mds\n</code></pre></p> </li> </ol> <p>Once CephFS is specified in the <code>CephDeployment</code> CR, Pelagia Deployment Controller will validate it and request Rook to create CephFS. Then Pelagia Deployment Controller will create a Kubernetes <code>StorageClass</code>, required to start provisioning the storage, which will operate the CephFS CSI driver to create Kubernetes PVs.</p>"},{"location":"ops-guide/deployment/default-ceph-conf/","title":"Ceph Default Config Options","text":""},{"location":"ops-guide/deployment/default-ceph-conf/#ceph-default-configuration-options","title":"Ceph default configuration options","text":"<p>Pelagia Deployment Controller provides the capability to specify configuration options for the Ceph cluster through the <code>rookConfig</code> key-value section of the <code>CephDeployment</code> CR as if they were set in a usual <code>ceph.conf</code> file. For details, see Architecture: CephDeployment.</p> <p>However, if <code>rookConfig</code> is empty, Pelagia Deployment Controller still specifies the following default configuration options for each Ceph cluster:</p> <ul> <li>Required network parameters that you can change through the <code>network</code>   section:   <pre><code>[global]\ncluster network = &lt;spec.network.clusterNet&gt;\npublic network = &lt;spec.network.publicNet&gt;\n</code></pre></li> <li>General default configuration options that you can override using the   <code>rookConfig</code> parameter:   <pre><code>[global]\nmon_max_pg_per_osd = 300\nmon_target_pg_per_osd = 100\n\n[mon]\nmon_warn_on_insecure_global_id_reclaim = false\nmon_warn_on_insecure_global_id_reclaim_allowed = false\n\n[osd]\nosd_class_dir = /usr/lib64/rados-classes\n</code></pre></li> <li>If <code>rookConfig</code> is empty but the <code>objectStore.rgw</code> section is defined, Pelagia   specifies the following Ceph RADOS Gateway default configuration options:   <pre><code>[client.rgw.rgw.store.a]\nrgw_bucket_quota_ttl = 30\nrgw_data_log_backing = omap\nrgw_dns_name = rook-ceph-rgw-rgw-store.rook-ceph.svc\nrgw_max_attr_name_len = 64\nrgw_max_attr_size = 1024\nrgw_max_attrs_num_in_req = 32\nrgw_thread_pool_size = 256\nrgw_trust_forwarded_https = true\nrgw_user_quota_bucket_sync_interval = 30\nrgw_user_quota_sync_interval = 30\n</code></pre></li> </ul>"},{"location":"ops-guide/deployment/default-ceph-conf/#rockoon-related-default-configuration-options","title":"Rockoon-related default configuration options","text":"<p>If Pelagia is integrated with Rockoon and <code>objectStore.rgw</code> section is defined in the <code>CephDeployment</code> custom resource, Pelagia Deployment Controller specifies the OpenStack-related default configuration options for each Ceph cluster:</p> <ul> <li>Ceph Object Gateway options that you can override using the <code>rookConfig</code> parameter:   <pre><code>[client.rgw.rgw.store.a]\nrgw swift account in url = true\nrgw keystone accepted roles = '_member_, Member, member, swiftoperator'\nrgw keystone accepted admin roles = admin\nrgw keystone implicit tenants = true\nrgw swift versioning enabled = true\nrgw enforce swift acls = true\nrgw_max_attr_name_len = 64\nrgw_max_attrs_num_in_req = 32\nrgw_max_attr_size = 1024\nrgw_bucket_quota_ttl = 0\nrgw_user_quota_bucket_sync_interval = 30\nrgw_user_quota_sync_interval = 30\nrgw s3 auth use keystone = true\n</code></pre></li> <li>Additional parameters for the Keystone integration provided by Rockoon in shared secret:   <pre><code>rgw keystone api version = 3\nrgw keystone url = &lt;keystoneAuthURL&gt;\nrgw keystone admin user = &lt;keystoneUser&gt;\nrgw keystone admin password = &lt;keystonePassword&gt;\nrgw keystone admin domain = &lt;keystoneProjectDomain&gt;\nrgw keystone admin project = &lt;keystoneProjectName&gt;\n</code></pre></li> </ul>"},{"location":"ops-guide/deployment/default-ceph-conf/#see-also","title":"SEE ALSO","text":"<p>Architecture: CephDeployment</p>"},{"location":"ops-guide/deployment/move-failure-domain/","title":"Change Ceph Pools Failure Domain","text":""},{"location":"ops-guide/deployment/move-failure-domain/#migrate-ceph-pools-from-one-failure-domain-to-another","title":"Migrate Ceph pools from one failure domain to another","text":"<p>The document describes how to change the failure domain of an already deployed Ceph cluster.</p> <p>Note</p> <p>This document focuses on changing the failure domain from a smaller to wider one, for example, from <code>host</code> to <code>rack</code>. Using the same instruction, you can move the failure domain from a wider to smaller scale.</p> <p>A high-level overview of the procedure includes the following steps:</p> <ol> <li>Set correct labels on the nodes.</li> <li>Create the new bucket hierarchy.</li> <li>Move nodes to new buckets.</li> <li>Scale down Pelagia controllers.</li> <li>Modify the CRUSH rules.</li> <li>Add the manual changes to the <code>CephDeployment</code> custom resource (CR) spec.</li> <li>Scale up Pelagia controllers.</li> </ol>"},{"location":"ops-guide/deployment/move-failure-domain/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Verify that the Ceph cluster has enough space for multiple copies of data to migrate. We highly recommend that    the Ceph cluster has a minimum of 25% free space for the procedure to succeed.</p> <p>Note</p> <p>The migration procedure implies data movement and optional modification of CRUSH rules that cause a large  amount of data (depending on the cluster size) to be first copied to a new location in the Ceph  cluster before data removal.</p> </li> <li> <p>Create a backup of the current <code>CephDeployment</code> CR:    <pre><code>kubectl -n pelagia get cephdpl -o yaml &gt; cephdpl-backup.yaml\n</code></pre></p> </li> <li> <p>In the <code>pelagia-ceph-toolbox</code> pod, obtain a backup of the CRUSH map:    <pre><code>ceph osd getcrushmap -o /tmp/crush-map-orig\ncrushtool -d /tmp/crush-map-orig -o /tmp/crush-map-orig.txt\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/deployment/move-failure-domain/#migrate-ceph-pools","title":"Migrate Ceph pools","text":"<p>This procedure contains an example of moving failure domains of all pools from <code>host</code> to <code>rack</code>. Using the same instruction, you can migrate pools from other types of failure domains, migrate pools separately, and so on.</p> <p>To migrate Ceph pools from one failure domain to another:</p> <ol> <li> <p>Set the required CRUSH topology in the <code>MiraCeph</code> object for each    defined node. For details on the <code>crush</code> parameter, see    Node parameters.</p> <p>Setting the CRUSH topology to each node causes the Pelagia Deployment Controller to set proper Kubernetes  labels on the nodes.</p> <p>Example of adding the <code>rack</code> CRUSH topology key for each node in the <code>nodes</code> section:  <pre><code>spec:\n  nodes:\n    machine1:\n      crush:\n        rack: rack-1\n    machine2:\n      crush:\n        rack: rack-1\n    machine3:\n      crush:\n        rack: rack-2\n    machine4:\n      crush:\n        rack: rack-2\n    machine5:\n      crush:\n        rack: rack-3\n    machine6:\n      crush:\n        rack: rack-3\n</code></pre></p> </li> <li> <p>Verify that the required buckets and bucket types are present in the Ceph hierarchy:</p> <ol> <li> <p>Enter the <code>pelagia-ceph-toolbox</code> pod:     <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- bash\n</code></pre></p> </li> <li> <p>Verify that the required bucket type is present by default:     <pre><code>ceph osd getcrushmap -o /tmp/crush-map\ncrushtool -d /tmp/crush-map -o /tmp/crush-map.txt\ncat /tmp/crush-map.txt # Look for the section named \u2192 \u201c# types\u201d\n</code></pre></p> <p>Example of system response:   <pre><code># types\ntype 0 osd\ntype 1 host\ntype 2 chassis\ntype 3 rack\ntype 4 row\ntype 5 pdu\ntype 6 pod\ntype 7 room\ntype 8 datacenter\ntype 9 zone\ntype 10 region\ntype 11 root\n</code></pre></p> </li> <li> <p>Verify that the buckets with the required bucket type are present:     <pre><code>cat /tmp/crush-map.txt # Look for the section named \u2192 \u201c# buckets\u201d\n</code></pre></p> <p>Example of system response of an existing <code>rack</code> bucket:   <pre><code># buckets\nrack rack-1 {\n  id -15\n  id -16 class hdd\n  # weight 0.00000\n  alg straw2\n  hash 0\n}\n</code></pre></p> </li> <li> <p>If the required buckets are not created, create new ones with the     required bucket type:     <pre><code>ceph osd crush add-bucket &lt;bucketName&gt; &lt;bucketType&gt; root=default\n</code></pre></p> <p>For example:   <pre><code>ceph osd crush add-bucket rack-1 rack root=default\nceph osd crush add-bucket rack-2 rack root=default\nceph osd crush add-bucket rack-3 rack root=default\n</code></pre></p> </li> <li> <p>Exit the <code>pelagia-ceph-toolbox</code> pod.</p> </li> </ol> </li> <li> <p>Optional. Order buckets as required:</p> <ol> <li> <p>Add the first Ceph CRUSH smaller bucket to its respective wider bucket:     <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- bash\nceph osd crush move &lt;smallerBucketName&gt; &lt;bucketType&gt;=&lt;widerBucketName&gt;\n</code></pre></p> <p>Substitute the following parameters:</p> <ul> <li><code>&lt;smallerBucketName&gt;</code> with the name of the smaller bucket, for example, host name;</li> <li><code>&lt;bucketType&gt;</code> with the required bucket type, for example, <code>rack</code>;</li> <li><code>&lt;widerBucketName&gt;</code> with the name of the wider bucket, for example, rack name.</li> </ul> <p>For example:   <pre><code>ceph osd crush move kaas-node-1 rack=rack-1 root=default\n</code></pre></p> <p>Warning</p> <p>We highly recommend moving one bucket at a time.</p> <p>For more details, refer to official Ceph documentation:   CRUHS Maps: Moving a bucket.</p> </li> <li> <p>After the bucket is moved to the new location in the CRUSH hierarchy,     verify that no data rebalancing occurs:     <pre><code>ceph -s\n</code></pre></p> </li> <li> <p>Add the remaining Ceph CRUSH smaller buckets to their respective wider buckets one by one.</p> </li> </ol> </li> <li> <p>Scale the Pelagia Controllers and Rook Ceph Operator deployments to <code>0</code> replicas:    <pre><code>kubectl -n pelagia scale deploy --all --replicas 0\nkubectl -n rook-ceph scale deploy rook-ceph-operator --replicas 0\n</code></pre></p> </li> <li> <p>Manually modify the CRUSH rules for Ceph pools to enable data placement on a new failure domain:</p> <ol> <li> <p>Enter the <code>pelagia-ceph-toolbox</code> pod:     <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- bash\n</code></pre></p> </li> <li> <p>List the CRUSH rules and erasure code profiles for the pools:     <pre><code>ceph osd pool ls detail\n</code></pre></p> <p>Example output:</p> <pre><code>pool 1 'mirablock-k8s-block-hdd' replicated size 2 min_size 1 crush_rule 9 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 1193 lfor 0/0/85 flags hashpspool,selfmanaged_snaps stripe_width 0 application rbd read_balance_score 1.31\npool 2 '.mgr' replicated size 2 min_size 1 crush_rule 1 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 70 flags hashpspool stripe_width 0 pg_num_max 32 pg_num_min 1 application mgr read_balance_score 6.06\npool 3 'openstack-store.rgw.otp' replicated size 2 min_size 1 crush_rule 11 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode on last_change 1197 flags hashpspool stripe_width 0 pg_num_min 8 application rook-ceph-rgw read_balance_score 2.27\npool 4 'openstack-store.rgw.meta' replicated size 2 min_size 1 crush_rule 12 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode on last_change 1197 flags hashpspool stripe_width 0 pg_num_min 8 application rook-ceph-rgw read_balance_score 1.50\npool 5 'openstack-store.rgw.log' replicated size 2 min_size 1 crush_rule 10 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode on last_change 1197 flags hashpspool stripe_width 0 pg_num_min 8 application rook-ceph-rgw read_balance_score 3.00\npool 6 'openstack-store.rgw.buckets.non-ec' replicated size 2 min_size 1 crush_rule 13 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode on last_change 1197 flags hashpspool stripe_width 0 pg_num_min 8 application rook-ceph-rgw read_balance_score 1.50\npool 7 'openstack-store.rgw.buckets.index' replicated size 2 min_size 1 crush_rule 15 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode on last_change 1197 flags hashpspool stripe_width 0 pg_num_min 8 application rook-ceph-rgw read_balance_score 2.25\npool 8 '.rgw.root' replicated size 2 min_size 1 crush_rule 14 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode on last_change 1197 flags hashpspool stripe_width 0 pg_num_min 8 application rook-ceph-rgw read_balance_score 3.75\npool 9 'openstack-store.rgw.control' replicated size 2 min_size 1 crush_rule 16 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode on last_change 1197 flags hashpspool stripe_width 0 pg_num_min 8 application rook-ceph-rgw read_balance_score 3.00\npool 10 'other-hdd' replicated size 2 min_size 1 crush_rule 19 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 1179 lfor 0/0/85 flags hashpspool,selfmanaged_snaps stripe_width 0 application rbd read_balance_score 1.69\npool 11 'openstack-store.rgw.buckets.data' erasure profile openstack-store.rgw.buckets.data_ecprofile size 3 min_size 2 crush_rule 18 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 1198 lfor 0/0/86 flags hashpspool,ec_overwrites stripe_width 8192 application rook-ceph-rgw\npool 12 'vms-hdd' replicated size 2 min_size 1 crush_rule 21 object_hash rjenkins pg_num 256 pgp_num 256 autoscale_mode on last_change 1182 lfor 0/0/95 flags hashpspool,selfmanaged_snaps stripe_width 0 target_size_ratio 0.4 application rbd read_balance_score 1.24\npool 13 'volumes-hdd' replicated size 2 min_size 1 crush_rule 23 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 1185 lfor 0/0/89 flags hashpspool,selfmanaged_snaps stripe_width 0 target_size_ratio 0.2 application rbd read_balance_score 1.31\npool 14 'backup-hdd' replicated size 2 min_size 1 crush_rule 25 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 1188 lfor 0/0/90 flags hashpspool,selfmanaged_snaps stripe_width 0 target_size_ratio 0.1 application rbd read_balance_score 2.06\npool 15 'images-hdd' replicated size 2 min_size 1 crush_rule 27 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 1191 lfor 0/0/90 flags hashpspool,selfmanaged_snaps stripe_width 0 target_size_ratio 0.1 application rbd read_balance_score 1.50\n</code></pre> </li> <li> <p>For each replicated Ceph pool:</p> <ol> <li> <p>Obtain the current CRUSH rule name:      <pre><code>ceph osd crush rule dump &lt;oldCrushRuleName&gt;\n</code></pre></p> </li> <li> <p>Create a new CRUSH rule with the required bucket type using the same root, device class, and new bucket type:      <pre><code>ceph osd crush rule create-replicated &lt;newCrushRuleName&gt; &lt;root&gt; &lt;bucketType&gt; &lt;deviceClass&gt;\n</code></pre></p> <p>For example:    <pre><code>ceph osd crush rule create-replicated images-hdd-rack default rack hdd\n</code></pre></p> <p>For more details, refer to official Ceph documentation:    CRUSH Maps: Creating a rule for a replicated pool.</p> </li> <li> <p>Apply a new crush rule to the Ceph pool:      <pre><code>ceph osd pool set &lt;poolName&gt; crush_rule &lt;newCrushRuleName&gt;\n</code></pre></p> <p>For example:    <pre><code>ceph osd pool set images-hdd crush_rule images-hdd-rack\n</code></pre></p> </li> <li> <p>Wait for data to be rebalanced after moving the Ceph pool under the      new failure domain (bucket type) by monitoring Ceph health:      <pre><code>ceph -s\n</code></pre></p> </li> <li> <p>Verify that the old CRUSH rule is not used anymore:      <pre><code>ceph osd pool ls detail\n</code></pre></p> <p>The rule ID is located in the CRUSH map and must match the rule ID in the output of ceph osd dump.</p> </li> <li> <p>Remove the old unused CRUSH rule and rename the new one to the      original name:      <pre><code>ceph osd crush rule rm &lt;oldCrushRuleName&gt;\nceph osd crush rule rename &lt;newCrushRuleName&gt; &lt;oldCrushRuleName&gt;\n</code></pre></p> </li> </ol> </li> <li> <p>For each erasure-coded Ceph pool:</p> <p>Note</p> <p>Erasure-coded pools require different number of buckets to store data. Instead of the number of replicas  in replicated pools, erasure-coded pools require the <code>coding chunks + data chunks</code> number of buckets  existing in the Ceph cluster. For example, if an erasure-coded pool has 2 coding chunks and 2 data chunks  configured, then the pool requires 4 different buckets, for example, 4 racks, to store data.</p> <ol> <li> <p>Obtain the current parameters of the erasure-coded profile:    <pre><code>ceph osd erasure-code-profile get &lt;ecProfile&gt;\n</code></pre></p> </li> <li> <p>In the profile, add the new bucket type as the failure domain using the <code>crush-failure-domain</code> parameter:    <pre><code>ceph osd erasure-code-profile set &lt;ecProfile&gt; k=&lt;int&gt; m=&lt;int&gt; crush-failure-domain=&lt;bucketType&gt; crush-device-class=&lt;deviceClass&gt;\n</code></pre></p> </li> <li> <p>Create a new CRUSH rule in the profile:    <pre><code>ceph osd crush rule create-erasure &lt;newEcCrushRuleName&gt; &lt;ecProfile&gt;\n</code></pre></p> </li> <li> <p>Apply the new CRUSH rule to the pool:    <pre><code>ceph osd pool set &lt;poolName&gt; crush_rule &lt;newEcCrushRuleName&gt;\n</code></pre></p> </li> <li> <p>Wait for data to be rebalanced after moving the Ceph pool under the new failure domain (bucket type)    by monitoring Ceph health:    <pre><code>ceph -s\n</code></pre></p> </li> <li> <p>Verify that the old CRUSH rule is not used anymore:    <pre><code>ceph osd pool ls detail\n</code></pre></p> <p>The rule ID is located in the CRUSH map and must match the rule ID in the output of ceph osd dump.</p> </li> <li> <p>Remove the old unused CRUSH rule and rename the new one to the original name:    <pre><code>ceph osd crush rule rm &lt;oldCrushRuleName&gt;\nceph osd crush rule rename &lt;newCrushRuleName&gt; &lt;oldCrushRuleName&gt;\n</code></pre></p> <p>Note</p> <p>New erasure-coded profiles cannot be renamed, so they will not be removed automatically during pools cleanup. Remove them manually, if needed.</p> </li> </ol> </li> <li> <p>Exit the <code>pelagia-ceph-toolbox</code> pod.</p> </li> </ol> </li> <li> <p>Update the <code>CephDeployment</code> CR <code>spec</code> by setting the <code>failureDomain: rack</code>    parameter for each pool. The configuration from the Rook perspective must    match the manually created configuration. For example:    <pre><code>spec:\n  pools:\n  - name: images\n    ...\n    failureDomain: rack\n  - name: volumes\n    ...\n    failureDomain: rack\n  ...\n  objectStorage:\n    rgw:\n      dataPool:\n        failureDomain: rack\n        ...\n      metadataPool:\n        failureDomain: rack\n        ...\n</code></pre></p> </li> <li> <p>Monitor the Ceph cluster health and wait until rebalancing is completed:    <pre><code>ceph -s\n</code></pre></p> <p>Example of a successful system response:  <pre><code>HEALTH_OK\n</code></pre></p> </li> <li> <p>Scale back Pelagia Controllers and Rook Ceph Operator deployments:    <pre><code>kubectl -n pelagia scale deploy --all --replicas 3\nkubectl -n rook-ceph scale deploy rook-ceph-operator --replicas 1\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/deployment/move-mon-daemon/","title":"Change Ceph Monitor node","text":""},{"location":"ops-guide/deployment/move-mon-daemon/#move-a-ceph-monitor-daemon-to-another-node","title":"Move a Ceph Monitor daemon to another node","text":"<p>This document describes how to migrate a Ceph Monitor daemon from one node to another without changing the general number of Ceph Monitors in the cluster. In the Pelagia Controllers concept, migration of a Ceph Monitor means manually removing it from one node and adding it to another.</p> <p>Consider the following exemplary placement scheme of Ceph Monitors in the <code>nodes</code> spec of the <code>CephDeployment</code> custom resource (CR): <pre><code>spec:\n  nodes:\n    node-1:\n      roles:\n      - mon\n      - mgr\n    node-2:\n      roles:\n      - mgr\n</code></pre></p> <p>Using the example above, if you want to move the Ceph Monitor from <code>node-1</code> to <code>node-2</code> without changing the number of Ceph Monitors, the <code>roles</code> table of the <code>nodes</code> spec must result as follows: <pre><code>spec:\n  nodes:\n    node-1:\n      roles:\n      - mgr\n    node-2:\n      roles:\n      - mgr\n      - mon\n</code></pre></p> <p>However, due to the Rook limitation related to Kubernetes architecture, once you move the Ceph Monitor through the <code>CephDeployment</code> CR, changes will not apply automatically. This is caused by the following Rook behavior:</p> <ul> <li>Rook creates Ceph Monitor resources as deployments with <code>nodeSelector</code>,   which binds Ceph Monitor pods to a requested node.</li> <li>Rook does not recreate new Ceph Monitors with the new node placement if the   current <code>mon</code> quorum works.</li> </ul> <p>Therefore, to move a Ceph Monitor to another node, you must also manually apply the new Ceph Monitors placement to the Ceph cluster as described below.</p>"},{"location":"ops-guide/deployment/move-mon-daemon/#move-a-ceph-monitor-to-another-node","title":"Move a Ceph Monitor to another node","text":"<ol> <li> <p>Open the <code>CephDeployment</code> CR for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>In the <code>nodes</code> spec of the <code>CephDeployment</code> CR, change the <code>mon</code> roles placement without changing the total    number of <code>mon</code> roles. For details, see the example above. Note the nodes on which the <code>mon</code> roles    have been removed and save the <code>name</code> value of those nodes.</p> </li> <li> <p>Obtain the <code>rook-ceph-mon</code> deployment name placed on the obsolete node    using the previously obtained node name:    <pre><code>kubectl -n rook-ceph get deploy -l app=rook-ceph-mon -o jsonpath=\"{.items[?(@.spec.template.spec.nodeSelector['kubernetes\\.io/hostname'] == '&lt;nodeName&gt;')].metadata.name}\"\n</code></pre></p> <p>Substitute <code>&lt;nodeName&gt;</code> with the name of the node where you removed the <code>mon</code> role.</p> </li> <li> <p>Back up the <code>rook-ceph-mon</code> deployment placed on the obsolete node:    <pre><code>kubectl -n rook-ceph get deploy &lt;rook-ceph-mon-name&gt; -o yaml &gt; &lt;rook-ceph-mon-name&gt;-backup.yaml\n</code></pre></p> </li> <li> <p>Remove the <code>rook-ceph-mon</code> deployment placed on the obsolete node:    <pre><code>kubectl -n rook-ceph delete deploy &lt;rook-ceph-mon-name&gt;\n</code></pre></p> </li> <li> <p>Wait approximately 10 minutes until <code>rook-ceph-operator</code> performs a failover of the <code>Pending</code> <code>mon</code> pod.    Inspect the logs during the failover process:    <pre><code>kubectl -n rook-ceph logs -l app=rook-ceph-operator -f\n</code></pre></p> <p>Example of log extract:  <pre><code>2021-03-15 17:48:23.471978 W | op-mon: mon \"a\" not found in quorum, waiting for timeout (554 seconds left) before failover\n</code></pre></p> </li> <li> <p>If the failover process fails:</p> <ol> <li>Scale down the <code>rook-ceph-operator</code> deployment to <code>0</code> replicas.</li> <li>Apply the backed-up <code>rook-ceph-mon</code> deployment.</li> <li>Scale back the <code>rook-ceph-operator</code> deployment to <code>1</code> replica.</li> </ol> </li> </ol> <p>Once done, Rook removes the obsolete Ceph Monitor from the node and creates a new one on the specified node with a new letter. For example, if the <code>a</code>, <code>b</code>, and <code>c</code> Ceph Monitors were in quorum and <code>mon-c</code> was obsolete, Rook removes <code>mon-c</code> and creates <code>mon-d</code>. In this case, the new quorum includes the <code>a</code>, <code>b</code>, and <code>d</code> Ceph Monitors.</p>"},{"location":"ops-guide/deployment/move-mon-node-replace/","title":"Replace failed Ceph Monitor node","text":""},{"location":"ops-guide/deployment/move-mon-node-replace/#move-ceph-monitor-before-node-replacement","title":"Move Ceph Monitor before node replacement","text":"<p>This document describes how to migrate a Ceph Monitor to another node on bare metal-based clusters before node replacement.</p> <p>Warning</p> <ul> <li>Remove the Ceph Monitor role before the machine removal.</li> <li>Make sure that the Ceph cluster always has an odd number of Ceph Monitors.</li> </ul> <p>The procedure of a Ceph Monitor migration assumes that you temporarily move the Ceph Manager/Monitor to another (for example, worker) node. After a node replacement, we recommend migrating the Ceph Manager/Monitor to the new manager node.</p> <p>To migrate a Ceph Monitor to another machine:</p> <ol> <li>Move the Ceph Manager/Monitor daemon from the affected node to one of    the worker machines as described in    Move a Ceph Monitor daemon to another node.</li> <li>Delete the affected node.</li> <li> <p>Add a new manager node without the Monitor and Manager role.</p> <p>Warning</p> <p>The addition of a new node with the Monitor and Manager role breaks the odd number quorum of Ceph Monitors.</p> </li> <li> <p>Move the previously migrated Ceph Manager/Monitor daemon to the new manager    node as described in    Move a Ceph Monitor daemon to another node.</p> </li> </ol>"},{"location":"ops-guide/deployment/multinetworking/","title":"Enable Ceph Multinetwork","text":""},{"location":"ops-guide/deployment/multinetworking/#enable-ceph-multinetwork","title":"Enable Ceph multinetwork","text":"<p>Ceph allows establishing multiple IP networks and subnet masks for clusters with configured L3 network rules. You can configure multi-network through the <code>network</code> section of the <code>CephDeployment</code> custom resource (CR). Pelagia Deployment Controller uses this section to specify the Ceph networks for external access and internal daemon communication. The parameters in the <code>network</code> section use the CIDR notation, for example, <code>10.0.0.0/24</code>.</p> <p>Before enabling multiple networks for a Ceph cluster, consider the following requirements:</p> <ul> <li>Do not confuse the IP addresses you define with the public-facing IP   addresses the network clients may use to access the services.</li> <li>If you define more than one IP address and subnet mask for the public or   cluster network, ensure that the subnets within the network can route to   each other.</li> <li>Include each IP address or subnet in the <code>network</code> section to IP tables and   open ports for them as necessary.</li> <li>The pods of the Ceph OSD and Ceph RADOS Gateway daemons use cross-pods health checkers   to verify that the entire Ceph cluster is healthy. Therefore, each CIDR must   be accessible inside Ceph pods.</li> <li>Avoid using the <code>0.0.0.0/0</code> CIDR in the <code>network</code> section. With a zero   range in <code>publicNet</code> and/or <code>clusterNet</code>, the Ceph daemons behavior   is unpredictable.</li> </ul>"},{"location":"ops-guide/deployment/multinetworking/#enable-multinetwork-for-ceph","title":"Enable multinetwork for Ceph","text":"<ol> <li> <p>Open <code>CephDeployment</code> CR for editing:       <pre><code>kubectl -n pelagia get cephpdl\n</code></pre></p> </li> <li> <p>In the <code>clusterNet</code> and/or <code>publicNet</code> parameters of the <code>network</code> section, define a comma-separated array of CIDRs.    For example:    <pre><code>spec:\n  network:\n    publicNet:  10.12.0.0/24,10.13.0.0/24\n    clusterNet: 10.10.0.0/24,10.11.0.0/24\n</code></pre></p> </li> <li> <p>Exit the editor and apply the changes.</p> </li> </ol> <p>Once done, the specified network CIDRs will be passed to the Ceph daemons pods through the Rook <code>rook-config-override</code> ConfigMap.</p>"},{"location":"ops-guide/deployment/rbd-mirror/","title":"Enable Ceph RBD mirroring","text":""},{"location":"ops-guide/deployment/rbd-mirror/#enable-ceph-rbd-mirroring","title":"Enable Ceph RBD mirroring","text":"<p>Technical Preview</p> <p>This feature is in Technical Preview, use it on own risk. </p> <p>This section describes how to configure and use RADOS Block Device (RBD) mirroring for Ceph pools using the <code>rbdMirror</code> section in the <code>CephDeployment</code> custom resource (CR). The feature may be useful if, for example, you have two interconnected Rockoon clusters. Once you enable RBD mirroring, the images in the specified pools will be replicated, and if a cluster becomes unreachable, the second one will provide users with instant access to all images. For details, see Ceph Documentation: RBD Mirroring.</p> <p>Note</p> <p>Pelagia only supports bidirectional mirroring.</p>"},{"location":"ops-guide/deployment/rbd-mirror/#rbd-mirror-parameters","title":"RBD mirror parameters","text":"<p>To enable Ceph RBD monitoring, follow the procedure below and use the following <code>rbdMirror</code> parameters description:</p> Parameter Description <code>daemonsCount</code> Count of <code>rbd-mirror</code> daemons to spawn. We recommend using one instance of the <code>rbd-mirror</code> daemon. <code>peers</code> Optional. List of mirroring peers of an external cluster to connect to. Only a single peer is supported. The <code>peer</code> section includes the following parameters:<ul><li><code>site</code> - the label of a remote Ceph cluster associated with the token.</li><li><code>token</code> - the token that will be used by one site (Ceph cluster) to pull images from the other site. To obtain the token, use the rbd mirror pool peer bootstrap create command.</li><li><code>pools</code> - optional, a list of pool names to mirror.</li></ul>"},{"location":"ops-guide/deployment/rbd-mirror/#enable-ceph-rbd-mirroring_1","title":"Enable Ceph RBD mirroring","text":"<ol> <li> <p>In <code>CephDeployment</code> CRs of both Ceph clusters where you want to enable    mirroring, specify positive <code>daemonsCount</code> in the <code>rbdMirror</code> section:    <pre><code>spec:\n  rbdMirror:\n    daemonsCount: 1\n</code></pre></p> </li> <li> <p>On both Ceph clusters where you want to enable mirroring, wait for the Ceph    RBD Mirror daemons to start running:    <pre><code>kubectl -n rook-ceph get pod -l app=rook-ceph-rbd-mirror\n</code></pre></p> </li> <li> <p>In <code>CephDeployment</code> CRs of both Ceph clusters where you want to enable    mirroring, specify the <code>spec.pools.mirroring.mode</code> parameter for all <code>pools</code>    that must be mirrored.    <pre><code>spec:\n  pools:\n  - name: image-hdd\n    ...\n    mirroring:\n      mode: pool\n  - name: volumes-hdd\n    ...\n    mirroring:\n      mode: pool\n</code></pre></p> </li> <li> <p>Obtain the name of an external site to mirror with. On pools with mirroring    enabled, the name is typically <code>ceph fsid</code>:</p> <p><pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- rbd mirror pool info &lt;mirroringPoolName&gt;\n# or\nkubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- ceph fsid\n</code></pre>  Substitute <code>&lt;mirroringPoolName&gt;</code> with the name of a pool to be mirrored.</p> </li> <li> <p>On an external site to mirror with, create a new bootstrap peer token.    Execute the following command within the <code>pelagia-ceph-toolbox</code> pod CLI:</p> <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- rbd mirror pool peer bootstrap create &lt;mirroringPoolName&gt; --site-name &lt;siteName&gt;\n</code></pre> <p>Substitute <code>&lt;mirroringPoolName&gt;</code> with the name of a pool to be mirrored.  In <code>&lt;siteName&gt;</code>, assign a label for the external Ceph cluster that will be  used along with mirroring.</p> <p>For details, see Ceph documentation: Bootstrap peers.</p> </li> <li> <p>In <code>CephDeployment</code> CR on the cluster that should mirror pools, specify    <code>rbdMirror.peers</code> with the obtained peer and pools to mirror:    <pre><code>spec:\n  rbdMirror:\n    peers:\n    - site: &lt;siteName&gt;\n      token: &lt;bootstrapPeer&gt;\n      pools: [&lt;mirroringPoolName1&gt;, &lt;mirroringPoolName2&gt;, ...]\n</code></pre>    Substitute <code>&lt;siteName&gt;</code> with the label assigned to the external Ceph    cluster, <code>&lt;bootstrapPeer&gt;</code> with the token obtained in the previous step,    and <code>&lt;mirroringPoolName&gt;</code> with names of pools that have the    <code>mirroring.mode</code> parameter defined.</p> <p>For example:  <pre><code>spec:\n  rbdMirror:\n    peers:\n    - site: cluster-b\n      token: &lt;base64-string&gt;\n      pools:\n      - images-hdd\n      - volumes-hdd\n      - special-pool-ssd\n</code></pre></p> </li> <li> <p>Verify that mirroring is enabled and each pool with <code>spec.pools.mirroring.mode</code> defined has an external peer    site:    <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- rbd mirror pool info &lt;mirroringPoolName&gt;\n</code></pre>    Substitute <code>&lt;mirroringPoolName&gt;</code> with the name of a pool with mirroring enabled.</p> </li> <li> <p>If you have set the <code>image</code> mirroring mode in the <code>pools</code> section,    explicitly enable mirroring for each image with <code>rbd</code> within the pool:    <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- rbd mirror image enable &lt;poolName&gt;/&lt;imageName&gt; &lt;imageMirroringMode&gt;\n</code></pre>    Substitute <code>&lt;poolName&gt;</code> with the name of a pool with the <code>image</code>    mirroring mode, <code>&lt;imageName&gt;</code> with the name of an image stored in the    specified pool. Substitute <code>&lt;imageMirroringMode&gt;</code> with one of:</p> <ul> <li><code>journal</code> - for mirroring to use the RBD journaling image feature to   replicate the image contents. If the RBD journaling image feature is not   yet enabled on the image, it will be enabled automatically.</li> <li><code>snapshot</code> - for mirroring to use RBD image mirror-snapshots to   replicate the image contents. Once enabled, an initial mirror-snapshot   will automatically be created. To create additional RBD image   mirror-snapshots, use the rbd command.</li> </ul> <p>For details, see Ceph Documentation: Enable image mirroring.</p> </li> </ol>"},{"location":"ops-guide/deployment/rgw-multisite/","title":"Enable Multisite for Ceph Object Storage","text":""},{"location":"ops-guide/deployment/rgw-multisite/#enable-multisite-for-ceph-object-storage","title":"Enable Multisite for Ceph Object Storage","text":"<p>Technical Preview</p> <p>This feature is in Technical Preview, use it on own risk. </p> <p>The Ceph Object Storage Multisite feature allows object storage to replicate its data over multiple Ceph clusters. Using multisite, such object storage is independent and isolated from another object storage in the cluster. Only the multi-zone multisite setup is currently supported. For more details, see Ceph documentation: Multisite.</p>"},{"location":"ops-guide/deployment/rgw-multisite/#multisite-parameters","title":"Multisite parameters","text":"<ul> <li> <p><code>realms</code> - Required. List of realms to use, represents the realm namespaces.   Includes the following parameters:</p> <ul> <li><code>name</code> - required, the realm name.</li> <li> <p><code>pullEndpoint</code> - optional, required only when the master zone is in   a different storage cluster. The endpoint, access key, and system key   of the system user from the realm to pull from. Includes the   following parameters:</p> <ul> <li><code>endpoint</code> - the endpoint of the master zone in the master zone   group.</li> <li><code>accessKey</code> - the access key of the system user from the realm to   pull from.</li> <li><code>secretKey</code> - the system key of the system user from the realm to   pull from.</li> </ul> </li> </ul> </li> <li> <p><code>zoneGroups</code> - Required. The list of zone groups for realms. Includes the following   parameters:</p> <ul> <li><code>name</code> - required, the zone group name.</li> <li><code>realmName</code> - required, the realm namespace name to which   the zone group belongs to.</li> </ul> </li> <li> <p><code>zones</code> - Required. The list of zones used within one zone group. Includes   the following parameters:</p> <ul> <li><code>name</code> - required, the zone name.</li> <li><code>metadataPool</code> - required, the settings used to create the Object   Storage metadata pools. Must use replication. For details, see   description of Pool parameters.</li> <li><code>dataPool</code> - required, the settings used to create the Object Storage   data pool. Can use replication or erasure coding. For details, see   Pool parameters.</li> <li><code>zoneGroupName</code> - required, the zone group name.</li> <li><code>endpointsForZone</code> - optional. The list of all endpoints in the zone group.   If you use ingress proxy for RGW,   the list of endpoints must contain that FQDN/IP address to access RGW.   By default, if no ingress proxy is used, the list of endpoints is set   to the IP address of the RGW external service. Endpoints must follow   the HTTP URL format.</li> </ul> </li> </ul>"},{"location":"ops-guide/deployment/rgw-multisite/#enable-the-multisite-rgw-object-storage","title":"Enable the multisite RGW Object Storage","text":"<ol> <li>Open the <code>CephDeployment</code> custom resource for editing:    <pre><code>kubectl -n pelagia edit cephdpl &lt;name&gt;\n</code></pre></li> <li> <p>Using the parameters from Multisite parameters, update the <code>spec.objectStorage.multiSite</code>    section specification as required.</p> </li> <li> <p>Select from the following options:</p> <ul> <li> <p>If you do not need to replicate data from a different storage cluster,   and the current cluster represents the master zone, modify the current   <code>objectStorage</code> section to use the multisite mode:</p> <ol> <li> <p>Configure the <code>zone</code> RADOS Gateway (RGW) parameter by setting it to    the RGW Object Storage name.</p> <p>Note</p> <p>Leave <code>dataPool</code> and <code>metadataPool</code> empty. These   parameters are ignored because the <code>zone</code> block in the multisite   configuration specifies the pools parameters. Other RGW parameters   do not require changes.</p> <p>For example: <pre><code>spec:\n objectStorage:\n   rgw:\n     gateway:\n       allNodes: false\n       instances: 2\n       port: 80\n       securePort: 8443\n     name: openstack-store\n     preservePoolsOnDelete: false\n     zone:\n       name: openstack-store\n</code></pre></p> </li> <li> <p>Create the <code>multiSite</code> section where the names of realm, zone group,    and zone must match the current RGW name.</p> <p>Specify the <code>endpointsForZone</code> parameter according to your configuration:</p> <ul> <li>If you use ingress proxy, which is defined in the <code>spec.ingressConfig</code>    section, add the FQDN endpoint.</li> <li>If you do not use any ingress proxy and access the RGW API using the    default RGW external service, add the IP address of the external    service or leave this parameter empty.</li> </ul> <p>The following example illustrates a complete <code>objectStorage</code> section: <pre><code>objectStorage:\n  multiSite:\n    realms:\n    - name: openstack-store\n    zoneGroups:\n    - name: openstack-store\n      realmName: openstack-store\n    zones:\n    - name: openstack-store\n      zoneGroupName: openstack-store\n      endpointsForZone: http://10.11.0.75:8080\n      metadataPool:\n        failureDomain: host\n          replicated:\n            size: 3\n      dataPool:\n        erasureCoded:\n          codingChunks: 1\n          dataChunks: 2\n        failureDomain: host\n  rgw:\n    gateway:\n      allNodes: false\n      instances: 2\n      port: 80\n      securePort: 8443\n    name: openstack-store\n    preservePoolsOnDelete: false\n    zone:\n      name: openstack-store\n</code></pre></p> </li> </ol> </li> <li> <p>If you use a different storage cluster, and its object storage data must   be replicated, specify the realm and zone group names along with the   <code>pullEndpoint</code> parameter. Additionally, specify the endpoint, access   key, and system keys of the system user of the realm from which you need   to replicate data. For details, see step 2 of this procedure.</p> <p>Note</p> <p>All commands below should be executed inside <code>pelagia-ceph-toolbox</code> pod.</p> <ul> <li> <p>To obtain the endpoint of the cluster zone that must be replicated, run   the following command by specifying the zone group name of the required   master zone on the master zone side:   <pre><code>radosgw-admin zonegroup get --rgw-zonegroup=&lt;ZONE_GROUP_NAME&gt; | jq -r '.endpoints'\n</code></pre>   The endpoint is located in the <code>endpoints</code> field.</p> </li> <li> <p>To obtain the access key and the secret key of the system user, run   the following command on the required Ceph cluster:   <pre><code>radosgw-admin user list\n</code></pre></p> </li> <li>To obtain the system user name, which has your RGW <code>ObjectStorage</code>   name as prefix:   <pre><code>radosgw-admin user info --uid=\"&lt;USER_NAME&gt;\" | jq -r '.keys'\n</code></pre></li> </ul> <p>For example: <pre><code>spec:\n  objectStorage:\n    multiSite:\n      realms:\n      - name: openstack-store\n        pullEndpoint:\n          endpoint: http://10.11.0.75:8080\n          accessKey: DRND5J2SVC9O6FQGEJJF\n          secretKey: qpjIjY4lRFOWh5IAnbrgL5O6RTA1rigvmsqRGSJk\n      zoneGroups:\n      - name: openstack-store\n        realmName: openstack-store\n      zones:\n      - name: openstack-store-backup\n        zoneGroupName: openstack-store\n        metadataPool:\n          failureDomain: host\n          replicated:\n            size: 3\n        dataPool:\n          erasureCoded:\n            codingChunks: 1\n            dataChunks: 2\n          failureDomain: host\n</code></pre></p> <p>Note</p> <p>We recommend using the same <code>metadataPool</code> and <code>dataPool</code> settings as you use in the master zone.</p> </li> </ul> </li> <li> <p>Configure the <code>zone</code> RGW parameter and leave <code>dataPool</code>    and <code>metadataPool</code> empty. These parameters are ignored because    the <code>zone</code> section in the multisite configuration specifies the pool parameters.</p> <p>Also, you can split the RGW daemon on daemons serving clients and daemons  running synchronization. To enable this option, specify  <code>splitDaemonForMultisiteTrafficSync</code> in the <code>gateway</code> section.</p> <p>For example:  <pre><code>spec:\n  objectStorage:\n    multiSite:\n       realms:\n       - name: openstack-store\n         pullEndpoint:\n           endpoint: http://10.11.0.75:8080\n           accessKey: DRND5J2SVC9O6FQGEJJF\n           secretKey: qpjIjY4lRFOWh5IAnbrgL5O6RTA1rigvmsqRGSJk\n       zoneGroups:\n       - name: openstack-store\n         realmName: openstack-store\n       zones:\n       - name: openstack-store-backup\n         zoneGroupName: openstack-store\n         metadataPool:\n           failureDomain: host\n           replicated:\n             size: 3\n         dataPool:\n           erasureCoded:\n             codingChunks: 1\n             dataChunks: 2\n           failureDomain: host\n    rgw:\n      dataPool: {}\n      gateway:\n        allNodes: false\n        instances: 2\n        splitDaemonForMultisiteTrafficSync: true\n        port: 80\n        securePort: 8443\n      healthCheck: {}\n      metadataPool: {}\n      name: openstack-store-backup\n      preservePoolsOnDelete: false\n      zone:\n        name: openstack-store-backup\n</code></pre></p> </li> <li> <p>Verify the multisite status:    <pre><code>radosgw-admin sync status\n</code></pre></p> </li> </ol> <p>Once done, Pelagia Deployment Controller will create the required resources and Rook will handle the multisite configuration. For details, see: Rook documentation: Object Multisite.</p>"},{"location":"ops-guide/deployment/rgw-multisite/#configure-and-clean-up-a-multisite-configuration","title":"Configure and clean up a multisite configuration","text":"<p>Warning</p> <p>Rook does not handle multisite configuration changes and cleanup. Therefore, once you enable multisite for Ceph RGW Object Storage, perform these operations manually in the <code>pelagia-ceph-toolbox</code> pod. For details, see Rook documentation: Multisite cleanup.</p> <p>Automatic update of zonegroup hostnames is disabled in <code>CephDeployment</code> CR if RADOS Gateway Multisite is enabled or External Ceph cluster mode is enabled, therefore, manually specify all required hostnames and update the zone group. In the <code>pelagia-ceph-toolbox</code> pod, run the following script:</p> <p>Note</p> <p>The script is actual for Rook resources deployed by Pelagia Helm chart. If you're using Rook which is not deployed by Pelagia Helm chart, update zonegroup configuration manually.</p> <pre><code>/usr/local/bin/zonegroup_hostnames_update.sh --rgw-zonegroup &lt;ZONEGROUP_NAME&gt; --hostnames fqdn1[,fqdn2]\n</code></pre> <p>If the multisite setup is completely cleaned up, manually execute the following steps on the <code>pelagia-ceph-toolbox</code> pod:</p> <ol> <li> <p>Due to the Rook issue #16328, verify that <code>.rgw.root</code> pool is removed:</p> <ul> <li> <p>Verify <code>.rgw.root</code> pool does not exist:    <pre><code>ceph osd pool ls | grep .rgw.root\n</code></pre></p> </li> <li> <p>If the pool <code>.rgw.root</code> is not removed, remove it manually:    <pre><code>ceph osd pool rm .rgw.root .rgw.root --yes-i-really-really-mean-it\n</code></pre></p> </li> </ul> <p>Some other RGW pools may also require a removal after cleanup.</p> </li> <li> <p>Remove the related RGW crush rules:    <pre><code>ceph osd crush rule ls | grep rgw | xargs -I% ceph osd crush rule rm %\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/deployment/rgw-sse/","title":"Configure Ceph Object Storage SSE","text":""},{"location":"ops-guide/deployment/rgw-sse/#configure-ceph-object-storage-server-side-encryption","title":"Configure Ceph Object Storage server-side encryption","text":"<p>Technical Preview</p> <p>This feature is in Technical Preview, use it on own risk. </p> <p>Note</p> <p>Pelagia supports Ceph Object Storage storage-side encryption for the clusters with Rockoon installed because Pelagia uses OpenStack Barbican KMS. Other KMS types are not supported yet.</p> <p>When you use Ceph Object Storage server-side encryption (SSE), unencrypted data sent over HTTPS is stored encrypted by the Ceph Object Gateway in the Ceph cluster. The current implementation integrates OpenStack Barbican as a key management service.</p> <p>The Object Storage SSE feature is enabled by default in Pelagia with OpenStack Barbican. To use object storage SSE, the AWS CLI S3 client is used.</p> <p>To use object storage server-side encryption:</p> <ol> <li> <p>Create Amazon Elastic Compute Cloud (EC2) credentials:    <pre><code>openstack ec2 credentials create\n</code></pre></p> </li> <li> <p>Configure AWS CLI with <code>access</code> and <code>secret</code> created in the previous    step:    <pre><code>aws configure\n</code></pre></p> </li> <li> <p>Create a secret key in OpenStack Barbican KMS:    <pre><code>openstack secret order create --name &lt;name&gt; --algorithm &lt;algorithm&gt; --mode &lt;mode&gt; --bit-length 256 --payload-content-type=&lt;payload-content-type&gt; key\n</code></pre></p> <p>Substitute the parameters enclosed in angle brackets:</p> <ul> <li><code>&lt;name&gt;</code> - human-friendly name.</li> <li><code>&lt;algorithm&gt;</code> - algorithm to use with the requested key. For example,    <code>aes</code>.</li> <li><code>&lt;mode&gt;</code> - algorithm mode to use with the requested key. For example,    <code>ctr</code>.</li> <li><code>&lt;payload-content-type&gt;</code> - type/format of the secret to generate. For    example, <code>application/octet-stream</code>.</li> </ul> </li> <li> <p>Verify that the key has been created:    <pre><code>openstack secret order get &lt;order-href&gt;\n</code></pre></p> <p>Substitute <code>&lt;order-href&gt;</code> with the corresponding value from the output of the previous command.</p> </li> <li> <p>Specify the <code>ceph-rgw</code> user in the Barbican secret Access Control List (ACL):</p> <ol> <li> <p>Obtain the list of <code>ceph-rgw</code> users:     <pre><code>openstack user list --domain service  | grep ceph-rgw\n</code></pre></p> <p>Example output:   <pre><code>| c63b70134e0845a2b13c3f947880f66a | ceph-rgwZ6ycK3dY         |\n</code></pre></p> <p>In the output, capture the first value as the <code>&lt;user-id&gt;</code>,   which is <code>c63b70134e0845a2b13c3f947880f66a</code> in the above   example.</p> </li> <li> <p>Specify the <code>ceph-rgw</code> user in the Barbican ACL:     <pre><code>openstack acl user add --user &lt;user-id&gt; &lt;secret-href&gt;\n</code></pre></p> <p>Substitute <code>&lt;user-id&gt;</code> with the corresponding value from the output of   the previous command and <code>&lt;secret-href&gt;</code> with the corresponding value   obtained in step 3.</p> </li> </ol> </li> <li> <p>Create an S3 bucket:    <pre><code>aws --endpoint-url &lt;rgw-endpoint-url&gt; --ca-bundle &lt;ca-bundle&gt; s3api create-bucket --bucket &lt;bucket-name&gt;\n</code></pre></p> <p>Substitute the parameters enclosed in angle brackets:</p> <ul> <li><code>&lt;rgw-endpoint-url&gt;</code> - Ceph Object Gateway endpoint DNS name</li> <li><code>&lt;ca-bundle&gt;</code> - CA Certificate Bundle</li> <li><code>&lt;bucket-name&gt;</code> - human-friendly bucket name</li> </ul> </li> <li> <p>Upload a file using object storage SSE:    <pre><code>aws --endpoint-url &lt;rgw-endpoint-url&gt; --ca-bundle &lt;ca-bundle&gt; s3 cp &lt;path-to-file&gt; \"s3://&lt;bucket-name&gt;/&lt;filename&gt;\" --sse aws:kms --sse-kms-key-id &lt;key-id&gt;\n</code></pre></p> <p>Substitute the parameters enclosed in angle brackets:</p> <ul> <li><code>&lt;path-to-file&gt;</code> - path to the file that you want to upload</li> <li><code>&lt;filename&gt;</code> - name under which the uploaded file will be stored    in the bucket</li> <li><code>&lt;key-id&gt;</code> - Barbican secret key ID</li> </ul> </li> <li> <p>Select one of the following options to download the file:</p> <ul> <li> <p>Download the file using a key:    <pre><code>aws --endpoint-url &lt;rgw-endpoint-url&gt; --ca-bundle &lt;ca-bundle&gt; s3 cp \"s3://&lt;bucket-name&gt;/&lt;filename&gt;\" &lt;path-to-output-file&gt; --sse aws:kms --sse-kms-key-id &lt;key-id&gt;\n</code></pre></p> <p>Substitute <code>&lt;path-to-output-file&gt;</code> with the path to the file you want to download.</p> </li> <li> <p>Download the file without a key:    <pre><code>aws --endpoint-url &lt;rgw-endpoint-url&gt; --ca-bundle &lt;ca-bundle&gt; s3 cp \"s3://&lt;bucket-name&gt;/&lt;filename&gt;\" &lt;output-filename&gt;\n</code></pre></p> </li> </ul> </li> </ol>"},{"location":"ops-guide/deployment/rgw-tls/","title":"Configure Ceph Object Gateway TLS","text":""},{"location":"ops-guide/deployment/rgw-tls/#configure-ceph-object-gateway-tls","title":"Configure Ceph Object Gateway TLS","text":"<p>Once you enable Ceph Object Gateway (<code>radosgw</code>) as described in Enable Ceph RGW Object Storage, you can configure the Transport Layer Security (TLS) protocol for a Ceph Object Gateway public endpoint using custom <code>ingressConfig</code> specified in the <code>CephDeployment</code> custom resource (CR). In this case, Ceph Object Gateway public endpoint will use the public domain specified using the <code>ingressConfig</code> parameters.</p> <p>Note</p> <p>For clusters integrated with Rockoon, Pelagia has an ability to use domain and certificates, defined in Rockoon configuration. Pelagia prioritize <code>ingressConfig</code> data over Rockoon ingress data but if <code>ingressConfig</code> section is not configured, Pelagia will use Rockoon domain and certificates. Mirantis recommends not defining <code>ingressConfig</code> section, if Rockoon has <code>tls-proxy</code> enabled. In that case, common certificates are applied to all ingresses from the <code>OpenStackDeployment</code> object. This implies that Pelagia will use the public domain and the common certificate from the <code>OpenStackDeployment</code> object.</p> <p>This section describes how to specify a custom public endpoint for the Ceph Object Storage.</p>"},{"location":"ops-guide/deployment/rgw-tls/#ingress-config-parameters","title":"Ingress config parameters","text":"Parameter Description <code>tlsConfig</code> Defines TLS configuration for the Ceph Object Gateway public endpoint. <code>controllerClassName</code> Name of Ingress Controller class. The default value for Pelagia integrated Rockoon is <code>openstack-ingress-nginx</code> <code>annotations</code> Extra annotations for the ingress proxy."},{"location":"ops-guide/deployment/rgw-tls/#tlsconfig-section-parameters","title":"<code>tlsConfig</code> section parameters","text":"<ul> <li> <p><code>tlsSecretRefName</code> - Secret name with TLS certs in Rook Ceph namespace, for example, <code>rook-ceph</code>.   Allows avoiding exposure of certs directly in <code>spec</code>. Must contain the following format:</p> <pre><code>data:\n  ca.cert: &lt;base64encodedCaCertificate&gt;\n  tls.crt: &lt;base64encodedTlsCert&gt;\n  tls.key: &lt;base64encodedTlsKey&gt;\n</code></pre> <p>Caution</p> <p>When using <code>tlsSecretRefName</code>, remove <code>certs</code> section.</p> </li> <li> <p><code>certs</code> - TLS configuration for ingress including certificates.   Contains the following parameters:</p> <p>Caution</p> <p><code>certs</code> parameters section is insecure because it stores TLS certificates in plain text. Consider using the <code>tlsSecretRefName</code> parameter instead to avoid exposing TLS certificates in the <code>CephDeployment</code> CR.</p> <ul> <li><code>cacert</code> - The Certificate Authority (CA) certificate, used for the   ingress rule TLS support.</li> <li><code>tlsCert</code> - The TLS certificate, used for the ingress rule TLS support.</li> <li><code>tlsKey</code> - The TLS private key, used for the ingress rule TLS support.</li> </ul> </li> <li> <p><code>publicDomain</code> -  Mandatory. The domain name to use for public endpoints.</p> <p>Caution</p> <p>For Pelagia integrated with Rockoon, the default ingress controller does not support <code>publicDomain</code> values different from the OpenStack ingress public domain. Therefore, if you intend to use the default OpenStack Ingress Controller for your Ceph Object Storage public endpoint, plan to use the same public domain as your OpenStack endpoints.</p> </li> <li> <p><code>hostname</code> - Custom name to override the Ceph Object Storage name for public access. Public RGW endpoint has the   <code>https://&lt;hostname&gt;.&lt;publicDomain&gt;</code> format.</p> </li> </ul>"},{"location":"ops-guide/deployment/rgw-tls/#controllerclassname-parameter","title":"<code>controllerClassName</code> parameter","text":"<p><code>controllClassName</code> defines the name of the custom Ingress Controller. Pelagia does not support deploying Ingress Controllers, so you must deploy the Ingress Controller before configuring the <code>ingressConfig</code> section in the <code>CephDeployment</code> CR.</p> <p>For Pelagia integrated with Rockoon, the default Ingress Controller has <code>openstack-ingress-nginx</code> class name and Ceph uses the Rockoon OpenStack Ingress Controller based on NGINX.</p>"},{"location":"ops-guide/deployment/rgw-tls/#annotations-parameter","title":"<code>annotations</code> parameter","text":"<p><code>annotations</code> parameter defines extra annotations for the ingress proxy that are a key-value mapping of strings to add or override ingress rule annotations. For details, see NGINX Ingress Controller: Annotations.</p> <p>By default, the following annotations are set:</p> <ul> <li><code>nginx.ingress.kubernetes.io/rewrite-target</code> is set to <code>/</code>.</li> <li><code>nginx.ingress.kubernetes.io/upstream-vhost</code> is set to <code>&lt;spec.objectStorage.rgw.name&gt;.rook-ceph.svc</code>.</li> </ul> <p>Optional annotations:</p> <ul> <li><code>nginx.ingress.kubernetes.io/proxy-request-buffering: \"off\"</code> that disables buffering for <code>ingress</code> to prevent the   413 (Request Entity Too Large) error when uploading large files using <code>radosgw</code>.</li> <li><code>nginx.ingress.kubernetes.io/proxy-body-size: &lt;size&gt;</code> that increases the default uploading size limit to prevent the   413 (Request Entity Too Large) error when uploading large files using <code>radosgw</code>. Set the value in MB (<code>m</code>) or KB   (<code>k</code>). For example, <code>100m</code>.</li> </ul> <p>By default, an ingress rule is created with an internal Ceph Object Gateway service endpoint as a backend. Also, <code>rgw dns name</code> is specified by Pelagia Deployment Controller and is set to <code>&lt;spec.objectStorage.rgw.name&gt;.rook-ceph.svc</code> by default.</p> <p>You can override <code>rgw dns name</code> using the <code>rookConfig</code> key-value parameter. In this case, also change the corresponding ingress annotation.</p> Configuration example with the rgw_dns_name override <pre><code>spec:\n  objectStorage:\n    rgw:\n      name: rgw-store\n      ...\n  ingressConfig:\n    tlsConfig:\n      publicDomain: public.domain.name\n      tlsSecretRefName: pelagia-ingress-tls-secret\n    controllerClassName: openstack-ingress-nginx\n    annotations:\n      nginx.ingress.kubernetes.io/rewrite-target: /\n      nginx.ingress.kubernetes.io/upstream-vhost: rgw-store.public.domain.name\n      nginx.ingress.kubernetes.io/proxy-body-size: 100m\n  rookConfig:\n    \"rgw dns name\": rgw-store.public.domain.name\n</code></pre> <p>For clouds with the <code>publicDomain</code> parameter specified, align the <code>upstream-vhost</code> ingress annotation with the name of the Ceph Object Storage and the specified public domain.</p> <p>Pelagia Ceph Object Storage requires the <code>upstream-vhost</code> and <code>rgw dns name</code> parameters to be equal. Therefore, override the default <code>rgw dns name</code> with the corresponding ingress annotation value.</p>"},{"location":"ops-guide/deployment/rgw-tls/#to-configure-ceph-object-gateway-tls","title":"To configure Ceph Object Gateway TLS","text":"<ol> <li>To generate an SSL certificate for internal usage, verify that the    RADOS Gateway <code>spec.objectStorage.rgw.gateway.securePort</code> parameter is specified in the <code>CephDeployment</code> CR.    For details, see Enable Ceph RGW Object Storage.</li> <li> <p>Configure TLS for Ceph Object Gateway using a custom <code>ingressConfig</code>:</p> <ol> <li>Open the <code>CephDeployment</code> CR for editing:     <pre><code>kubectl -n pelagia edit cephdpl &lt;name&gt;\n</code></pre>     Substitute <code>&lt;name&gt;</code> with the name of your <code>CephDeployment</code>.</li> <li>Specify the <code>ingressConfig</code> parameters according Ingress config parameters.</li> <li>Save the changes and close the editor.</li> </ol> <p>Note</p> <p>For Pelagia with Rockoon, you can omit TLS configuration for the default settings provided by Rockoon to be  applied. Just obtain the Rockoon OpenStack CA certificate for a trusted connection:  <pre><code>kubectl -n openstack-ceph-shared get secret openstack-rgw-creds -o jsonpath=\"{.data.ca_cert}\" | base64 -d\n</code></pre></p> </li> <li> <p>If you use the HTTP scheme instead of HTTPS for internal or public Ceph Object Gateway endpoints,    add custom annotations to the <code>ingressConfig.annotations</code> section of the <code>CephDeployment</code> CR:    <pre><code>spec:\n  ingressConfig:\n    annotations:\n      \"nginx.ingress.kubernetes.io/force-ssl-redirect\": \"false\"\n      \"nginx.ingress.kubernetes.io/ssl-redirect\": \"false\"\n</code></pre></p> <p>If both HTTP and HTTPS must be used, apply the following configuration in the <code>CephDeployment</code> object: <pre><code>spec:\n  ingressConfig:\n    tlsConfig:\n      publicDomain: public.domain.name\n      tlsSecretRefName: pelagia-ingress-tls-secret\n    annotations:\n      \"nginx.ingress.kubernetes.io/force-ssl-redirect\": \"false\"\n      \"nginx.ingress.kubernetes.io/ssl-redirect\": \"false\"\n</code></pre></p> </li> <li> <p>Access internal and public Ceph Object Gateway endpoints by selecting one of the following options:</p> <ul> <li> <p>For a public endpoint:</p> <ol> <li>Obtain the Ceph Object Gateway public endpoint:    <pre><code>kubectl -n rook-ceph get ingress\n</code></pre></li> <li>Obtain the public endpoint TLS CA certificate:    <pre><code>kubectl -n rook-ceph get secret $(kubectl -n rook-ceph get ingress -o jsonpath='{.items[0].spec.tls[0].secretName}{\"\\n\"}') -o jsonpath='{.data.ca\\.crt}' | base64 -d; echo\n</code></pre></li> </ol> </li> <li> <p>For an internal endpoint:</p> <ol> <li> <p>Obtain the internal endpoint name for Ceph Object Gateway:    <pre><code>kubectl -n rook-ceph get svc -l app=rook-ceph-rgw\n</code></pre></p> <p>The internal endpoint for Ceph Object Gateway has the following format: <pre><code>https://&lt;internal-svc-name&gt;.rook-ceph.svc:&lt;rgw-secure-port&gt;/\n</code></pre> where <code>&lt;rgw-secure-port&gt;</code> is <code>spec.objectStorage.rgw.gateway.securePort</code> specified in the <code>CephDeployment</code> CR.</p> </li> <li> <p>Obtain the internal endpoint TLS CA certificate:    <pre><code>kubectl -n rook-ceph get secret rgw-ssl-certificate -o jsonpath=\"{.data.cacert}\" | base64 -d\n</code></pre></p> </li> </ol> </li> </ul> </li> <li> <p>Skip this step if one of the following requirements is met:</p> <ul> <li>The public hostname matches the public domain name set by the <code>spec.ingressConfig.tlsConfig.publicDomain</code> field;</li> <li>The OpenStack configuration has been applied.</li> </ul> <p>Otherwise, update the zonegroup <code>hostnames</code> of Ceph Object Gateway:</p> <ol> <li>Enter the <code>pelagia-ceph-toolbox</code> pod:      <pre><code>kubectl -n rook-ceph exec -it deployment/pelagia-ceph-toolbox -- bash\n</code></pre></li> <li> <p>Obtain Ceph Object Gateway default zonegroup configuration:      <pre><code>radosgw-admin zonegroup get --rgw-zonegroup=&lt;objectStorageName&gt; --rgw-zone=&lt;objectStorageName&gt; | tee zonegroup.json\n</code></pre></p> <p>Substitute <code>&lt;objectStorageName&gt;</code> with the Ceph Object Storage name from   <code>spec.objectStorage.rgw.name</code>.</p> </li> <li> <p>Inspect <code>zonegroup.json</code> and verify that the <code>hostnames</code> key is a      list that contains two endpoints: an internal endpoint and a custom      public endpoint:      <pre><code>\"hostnames\": [\"rook-ceph-rgw-&lt;objectStorageName&gt;.rook-ceph.svc\", &lt;customPublicEndpoint&gt;]\n</code></pre></p> <p>Substitute <code>&lt;objectStorageName&gt;</code> with the Ceph Object Storage name and  <code>&lt;customPublicEndpoint&gt;</code> with the public endpoint with a custom public  domain.</p> </li> <li> <p>If one or both endpoints are omitted in the list, add the missing      endpoints to the <code>hostnames</code> list in the <code>zonegroup.json</code> file and      update Ceph Object Gateway zonegroup configuration:      <pre><code>radosgw-admin zonegroup set --rgw-zonegroup=&lt;objectStorageName&gt; --rgw-zone=&lt;objectStorageName&gt; --infile zonegroup.json\nradosgw-admin period update --commit\n</code></pre></p> </li> <li> <p>Verify that the <code>hostnames</code> list contains both the internal and custom public endpoint:      <pre><code>radosgw-admin --rgw-zonegroup=&lt;objectStorageName&gt; --rgw-zone=&lt;objectStorageName&gt; zonegroup get | jq -r \".hostnames\"\n</code></pre></p> <p>Example of system response:    <pre><code>[\n  \"rook-ceph-rgw-obj-store.rook-ceph.svc\",\n  \"obj-store.mcc1.cluster1.example.com\"\n]\n</code></pre></p> </li> <li> <p>Exit the <code>pelagia-ceph-toolbox</code> pod:      <pre><code>exit\n</code></pre></p> </li> </ol> </li> </ol> <p>Once done, Ceph Object Gateway becomes available by the custom public endpoint with an S3 API client, OpenStack Swift CLI, and OpenStack Horizon Containers plugin.</p>"},{"location":"ops-guide/deployment/rgw/","title":"Enable Ceph RGW Object Storage","text":""},{"location":"ops-guide/deployment/rgw/#enable-ceph-rgw-object-storage","title":"Enable Ceph RGW Object Storage","text":"<p>Pelagia enables you to deploy Ceph RADOS Gateway (RGW) Object Storage instances and automatically manage its resources such as users and buckets.</p> <p>Pelagia has an integration for Ceph Object Storage with OpenStack Object Storage (<code>Swift</code>) provided by Rockoon.</p>"},{"location":"ops-guide/deployment/rgw/#ceph-rgw-object-storage-parameters","title":"Ceph RGW Object Storage parameters","text":"<ul> <li><code>name</code> - Required. Ceph Object Storage instance name.</li> <li> <p><code>dataPool</code> - Required if <code>zone.name</code> is not specified. Mutually exclusive with   <code>zone</code>. Must be used together with <code>metadataPool</code>.</p> <p>Object storage data pool spec that must only contain <code>replicated</code> or <code>erasureCoded</code>, <code>deviceClass</code> and <code>failureDomain</code> parameters. The <code>failureDomain</code> parameter may be set to <code>host</code>, <code>rack</code>, <code>room</code>, or <code>datacenter</code>, defining the failure domain across which the data will be spread. The <code>deviceClass</code> must be explicitly defined. For <code>dataPool</code>, We recommend using an <code>erasureCoded</code> pool.</p> <pre><code>spec:\n   objectStorage:\n     rgw:\n       dataPool:\n         deviceClass: hdd\n         failureDomain: host\n         erasureCoded:\n           codingChunks: 1\n           dataChunks: 2\n</code></pre> </li> <li> <p><code>metadataPool</code> - Required if <code>zone.name</code> is not specified. Mutually exclusive with   <code>zone</code>. Must be used together with <code>dataPool</code>.</p> <p>Object storage metadata pool spec that must only contain <code>replicated</code>, <code>deviceClass</code> and <code>failureDomain</code> parameters. The <code>failureDomain</code> parameter may be set to <code>host</code>, <code>rack</code>, <code>room</code>, or <code>datacenter</code>, defining the failure domain across which the data will be spread. The <code>deviceClass</code> must be explicitly defined. Can use only <code>replicated</code> settings. For example:</p> <pre><code>spec:\n   objectStorage:\n     rgw:\n       metadataPool:\n         deviceClass: hdd\n         failureDomain: host\n         replicated:\n           size: 3\n</code></pre> <p>where <code>replicated.size</code> is the number of full copies of data on multiple nodes.</p> <p>Warning</p> <p>When using the non-recommended Ceph pools <code>replicated.size</code> of less than <code>3</code>, Ceph OSD removal cannot be performed. The minimal replica size equals a rounded up half of the specified <code>replicated.size</code>.</p> <p>For example, if <code>replicated.size</code> is <code>2</code>, the minimal replica size is <code>1</code>, and if <code>replicated.size</code> is <code>3</code>, then the minimal replica size is <code>2</code>. The replica size of <code>1</code> allows Ceph having PGs with only one Ceph OSD in the <code>acting</code> state, which may cause a <code>PG_TOO_DEGRADED</code> health warning that blocks Ceph OSD removal. We recommend setting <code>replicated.size</code> to <code>3</code> for each Ceph pool.</p> </li> <li> <p><code>gateway</code> - Required. The gateway settings corresponding to the <code>rgw</code> daemon   settings. Includes the following parameters:</p> <ul> <li><code>port</code> - the port on which the Ceph RGW service will be listening on   HTTP.</li> <li><code>securePort</code> - the port on which the Ceph RGW service will be   listening on HTTPS.</li> <li> <p><code>instances</code> - the number of pods in the Ceph RGW ReplicaSet. If   <code>allNodes</code> is set to <code>true</code>, a DaemonSet is created instead.</p> <p>Note</p> <p>We recommend using 3 instances for Ceph Object Storage.</p> </li> <li> <p><code>allNodes</code> - defines whether to start the Ceph RGW pods as a   DaemonSet on all nodes. The <code>instances</code> parameter is ignored if   <code>allNodes</code> is set to <code>true</code>.</p> </li> <li><code>splitDaemonForMultisiteTrafficSync</code> - Optional. For multisite setup defines   whether to split RGW daemon on daemon responsible for sync between zones and daemon   for serving clients request.</li> <li><code>rgwSyncPort</code> - Optional. Port the rgw multisite traffic service will be listening on (http).   Has effect only for multisite configuration.</li> <li><code>resources</code> - Optional. Represents Kubernetes resource requirements for Ceph RGW pods. For details   see: Kubernetes docs: Resource Management for Pods and Containers.</li> <li> <p><code>externalRgwEndpoint</code> - Required for external Ceph cluster Setup. Represents external RGW Endpoint to use,   only when external Ceph cluster is used. Contains the following parameters:</p> <ul> <li><code>ip</code> - represents the IP address of RGW endpoint.</li> <li><code>hostname</code> - represents the DNS-addressable hostname of RGW endpoint.   This field will be preferred over IP if both are given.</li> </ul> <pre><code>spec:\n  objectStorage:\n    rgw:\n      gateway:\n        allNodes: false\n        instances: 3\n        port: 80\n        securePort: 8443\n</code></pre> </li> </ul> </li> <li> <p><code>preservePoolsOnDelete</code> - Optional. Defines whether to delete the data and metadata pools in   the <code>rgw</code> section if the Object Storage is deleted. Set this parameter   to <code>true</code> if you need to store data even if the object storage is   deleted. However, we recommend setting this parameter to <code>false</code>.</p> </li> <li> <p><code>objectUsers</code> and <code>buckets</code> - Optional. To create new Ceph RGW resources, such as buckets or users,   specify the following keys. Ceph Controller will automatically create   the specified object storage users and buckets in the Ceph cluster.</p> <ul> <li> <p><code>objectUsers</code> - a list of user specifications to create for object   storage. Contains the following fields:</p> <ul> <li><code>name</code> - a user name to create.</li> <li><code>displayName</code> - the Ceph user name to display.</li> <li> <p><code>capabilities</code> - user capabilities:</p> <ul> <li><code>user</code> - admin capabilities to read/write Ceph Object Store   users.</li> <li><code>bucket</code> - admin capabilities to read/write Ceph Object Store   buckets.</li> <li><code>metadata</code> - admin capabilities to read/write Ceph Object Store   metadata.</li> <li><code>usage</code> - admin capabilities to read/write Ceph Object Store   usage.</li> <li><code>zone</code> - admin capabilities to read/write Ceph Object Store   zones.</li> </ul> <p>The available options are <code>*</code>, <code>read</code>, <code>write</code>, <code>read, write</code>. For details, see Ceph documentation: Add/remove admin capabilities.</p> </li> <li> <p><code>quotas</code> - user quotas:</p> <ul> <li><code>maxBuckets</code> - the maximum bucket limit for the Ceph user.   Integer, for example, <code>10</code>.</li> <li><code>maxSize</code> - the maximum size limit of all objects across all the   buckets of a user. String size, for example, <code>10G</code>.</li> <li><code>maxObjects</code> - the maximum number of objects across all buckets   of a user. Integer, for example, <code>10</code>.</li> </ul> </li> </ul> <pre><code>spec:\n  objectStorage:\n    rgw:\n      objectUsers:\n      - name: test-user\n         displayName: test-user\n         capabilities:\n           bucket: '*'\n           metadata: read\n           user: read\n         quotas:\n           maxBuckets: 10\n           maxSize: 10G\n</code></pre> </li> <li> <p><code>buckets</code> - a list of strings that contain bucket names to create   for object storage.</p> </li> </ul> </li> <li> <p><code>zone</code> - Required if <code>dataPool</code> and <code>metadataPool</code> are not specified. Mutually     exclusive with these parameters.</p> <p>Defines the Ceph Multisite zone where the object storage must be placed.   Includes the <code>name</code> parameter that must be set to one of the <code>zones</code>   items. For details, see the Ops Guide: Enable Multisite for Ceph Object Storage.</p> <pre><code>spec:\n  objectStorage:\n    multisite:\n      zones:\n      - name: master-zone\n        ...\n    rgw:\n      zone:\n        name: master-zone\n</code></pre> </li> <li> <p><code>SSLCert</code> - Optional. Custom TLS certificate parameters used to access the Ceph RGW     endpoint. If not specified, a self-signed certificate will be generated.</p> <pre><code>spec:\n  objectStorage:\n    rgw:\n      SSLCert:\n        cacert: |\n          -----BEGIN CERTIFICATE-----\n          ca-certificate here\n          -----END CERTIFICATE-----\n        tlsCert: |\n          -----BEGIN CERTIFICATE-----\n          private TLS certificate here\n          -----END CERTIFICATE-----\n        tlsKey: |\n          -----BEGIN RSA PRIVATE KEY-----\n          private TLS key here\n          -----END RSA PRIVATE KEY-----\n</code></pre> </li> <li> <p><code>SSLCertInRef</code> - Optional. Flag to determine that a TLS     certificate for accessing the Ceph RGW endpoint is used but not exposed     in <code>spec</code>. For example:</p> <pre><code>spec:\n  objectStorage:\n    rgw:\n      SSLCertInRef: true\n</code></pre> <p>The operator must manually provide TLS configuration using the   <code>rgw-ssl-certificate</code> secret in the <code>rook-ceph</code> namespace of the   managed cluster. The secret object must have the following structure:</p> <pre><code>data:\n  cacert: &lt;base64encodedCaCertificate&gt;\n  cert: &lt;base64encodedCertificate&gt;\n</code></pre> <p>When removing an already existing <code>SSLCert</code> block, no additional actions   are required, because this block uses the same <code>rgw-ssl-certificate</code> secret   in the <code>rook-ceph</code> namespace.</p> <p>When adding a new secret directly without exposing it in <code>spec</code>, the following   rules apply:</p> <ul> <li><code>cert</code> - base64 representation of a file with the server TLS key,     server TLS cert, and CA certificate.</li> <li><code>cacert</code> - base64 representation of a CA certificate only.</li> </ul> </li> </ul>"},{"location":"ops-guide/deployment/rgw/#to-enable-the-rgw-object-storage","title":"To enable the RGW Object Storage:","text":"<ol> <li>Open the <code>CephDeployment</code> resource for editing:    <pre><code>kubectl -n pelagia edit cephdpl &lt;name&gt;\n</code></pre>    Substitute <code>&lt;name&gt;</code> with the name of your <code>CephDeployment</code>.</li> <li> <p>Using Ceph RGW Object Storage parameters, update the <code>objectStorage.rgw</code> section    specification.</p> <p>For example:  <pre><code>rgw:\n  name: rgw-store\n  dataPool:\n    deviceClass: hdd\n    erasureCoded:\n      codingChunks: 1\n      dataChunks: 2\n    failureDomain: host\n  metadataPool:\n    deviceClass: hdd\n    failureDomain: host\n    replicated:\n      size: 3\n  gateway:\n    allNodes: false\n    instances: 3\n    port: 80\n    securePort: 8443\n  preservePoolsOnDelete: false\n</code></pre> 3. Save the changes and exit the editor.</p> </li> </ol>"},{"location":"ops-guide/deployment/rook-daemon-place/","title":"Rook Daemons Placement","text":""},{"location":"ops-guide/deployment/rook-daemon-place/#specify-rook-daemons-placement","title":"Specify Rook daemons placement","text":"<p>If you need to configure the placement of Rook daemons on nodes, you can set extra values to Pelagia Helm chart release.</p> <p>The procedures in this section describe how to specify the placement of <code>rook-ceph-operator</code>, <code>rook-discover</code>, and Ceph CSI pods such as <code>csi-rbdplugin</code>, <code>csi-cephfsplugin</code>, <code>csi-rbdplugin-provisioner</code> and <code>csi-cephfsplugin-provisioner</code>.</p>"},{"location":"ops-guide/deployment/rook-daemon-place/#specify-rook-ceph-operator-placement","title":"Specify rook-ceph-operator placement","text":"<p>Use the following Pelagia Helm chart values to specify <code>rook-ceph-operator</code> placement:</p> <ul> <li><code>rookConfig.rookOperatorPlacement.affinity</code> is a key-value mapping that contains   a valid Kubernetes <code>affinity</code> specification.</li> <li><code>rookConfig.rookOperatorPlacement.nodeSelector</code> is a key-value mapping that contains   a valid Kubernetes <code>nodeSelector</code> specification.</li> <li><code>rookConfig.rookOperatorPlacement.tolerations</code> is a list that contains valid Kubernetes <code>toleration</code> items.</li> </ul> <p>Upgrade Pelagia Helm release with the desired placement values by setting them directly to release: <pre><code>helm upgrade --install pelagia-ceph oci://registry.mirantis.com/pelagia/pelagia-ceph --version 1.0.0 -n pelagia \\\n  --set rookConfig.rookOperatorPlacement.affinity=&lt;rookOperatorAffinity&gt;,\\\n        rookConfig.rookOperatorPlacement.nodeSelector=&lt;rookOperatorNodeSelector&gt;,\\\n        rookConfig.rookOperatorPlacement.tolerations=&lt;rookOperatorTolerations&gt;\n</code></pre></p> <p>After upgrading Pelagia Helm release, wait for some time and verify that the changes have applied: <pre><code>kubectl -n rook-ceph get deploy rook-ceph-operator -o yaml\n</code></pre></p>"},{"location":"ops-guide/deployment/rook-daemon-place/#specify-ceph-csi-pods-placement","title":"Specify Ceph CSI pods placement","text":"<p>Use the following Pelagia Helm chart values to specify Ceph CSI pods placement:</p> <ul> <li><code>rookConfig.csiPlacement.nodeAffinity.csiprovisioner</code> is a valid Kubernetes label selector expression.</li> <li><code>rookConfig.csiPlacement.nodeAffinity.csiplugin</code> is a valid Kubernetes label selector expression. Default is   <code>ceph-daemonset-available-node=true</code>.</li> <li> <p><code>rookConfig.csiPlacement.csiplugin.tolerations</code> is a string which contains a valid list of Kubernetes <code>toleration</code>   items. For example:   <pre><code>csiplugin: |\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/controlplane\n    operator: Exists\n</code></pre></p> </li> <li> <p><code>rookConfig.csiPlacement.csiprovisioner.tolerations</code> is a string which contains a valid list of Kubernetes   <code>toleration</code> items. For example:   <pre><code>csiprovisioner: |\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/controlplane\n    operator: Exists\n</code></pre></p> </li> </ul> <p>Upgrade Pelagia Helm release with the desired placement values by setting them directly to release: <pre><code>helm upgrade --install pelagia-ceph oci://registry.mirantis.com/pelagia/pelagia-ceph --version 1.0.0 -n pelagia \\\n  --set rookConfig.csiPlacement.nodeAffinity.csiprovisioner=\"&lt;nodeAffinityLabelSelector&gt;\"\n</code></pre></p> <p>After upgrading Pelagia Helm release, wait for some time and verify that the changes have applied: <pre><code>kubectl -n rook-ceph get ds csi-rbdplugin -o yaml\nkubectl -n rook-ceph get ds csi-cephfsplugin -o yaml\nkubectl -n rook-ceph get deploy csi-rbdplugin-provisioner -o yaml\nkubectl -n rook-ceph get deploy csi-cephfsplugin-provisioner -o yaml\n</code></pre></p>"},{"location":"ops-guide/deployment/rook-daemon-place/#specify-rook-discover-placement","title":"Specify rook-discover placement","text":"<p>Use the following Pelagia Helm chart values to specify <code>rook-discover</code> placement:</p> <ul> <li><code>rookConfig.rookDiscoverPlacement.nodeAffinity</code> is a valid Kubernetes label selector expression. Default is   <code>ceph-daemonset-available-node=true;ceph_role_osd=true</code>.</li> <li><code>rookConfig.rookDiscoverPlacement.tolerations</code> is a string which contains a valid list of Kubernetes <code>toleration</code>   items. For example:   <pre><code>tolerations: |\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/controlplane\n    operator: Exists\n</code></pre></li> </ul> <p>Upgrade Pelagia Helm release with the desired placement values by setting them directly to release: <pre><code>helm upgrade --install pelagia-ceph oci://registry.mirantis.com/pelagia/pelagia-ceph --version 1.0.0 -n pelagia \\\n  --set rookConfig.rookDiscoverPlacement.nodeAffinity=\"&lt;nodeAffinityLabelSelector&gt;\"\n</code></pre></p> <p>After upgrading Pelagia Helm release, wait for some time and verify that the changes have applied: <pre><code>kubectl -n rook-ceph get ds rook-discover -o yaml\n</code></pre></p>"},{"location":"ops-guide/deployment/ceph-resource-mgmt/enable-resource-mgmt/","title":"Enable Ceph resources Management","text":""},{"location":"ops-guide/deployment/ceph-resource-mgmt/enable-resource-mgmt/#enable-management-of-ceph-tolerations-and-resources","title":"Enable management of Ceph tolerations and resources","text":"<p>Warning</p> <p>This document does not provide any specific recommendations on requests and limits for Ceph resources. The document stands for a native Ceph resources configuration.</p> <p>You can configure Pelagia to manage Ceph resources by specifying their requirements and constraints. To configure the resource consumption for Ceph nodes, consider the following options that are based on different Helm release configuration values:</p> <ul> <li>Configuring tolerations for taint nodes for the Ceph Monitor, Ceph Manager,   and Ceph OSD daemons. For details, see   Taints and Tolerations.</li> <li>Configuring node resource requests or limits for the Ceph daemons and for   each Ceph OSD device class such as HDD, SSD, or NVMe. For details, see   Managing Resources for Containers.</li> </ul>"},{"location":"ops-guide/deployment/ceph-resource-mgmt/enable-resource-mgmt/#enable-management-of-ceph-tolerations-and-resources_1","title":"Enable management of Ceph tolerations and resources","text":"<ol> <li> <p>To avoid Ceph cluster health issues during daemon configuration changes,    set Ceph <code>noout</code>, <code>nobackfill</code>, <code>norebalance</code>, and <code>norecover</code>    flags through the <code>pelagia-ceph-toolbox</code> pod before editing Ceph tolerations    and resources:    <pre><code>kubectl -n rook-ceph exec deploy/pelagia-ceph-toolbox -- bash\nceph osd set noout\nceph osd set nobackfill\nceph osd set norebalance\nceph osd set norecover\nexit\n</code></pre></p> <p>Note</p> <p>Skip this step if you are only configuring the PG rebalance timeout and replicas count parameters.</p> </li> <li> <p>Open the <code>CephDeployment</code> custom resource (CR) for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>Specify the parameters in the <code>hyperconverge</code> section as required. The    <code>hyperconverge</code> section includes the following parameters:</p> <ul> <li> <p><code>tolerations</code> - Specifies tolerations for taint nodes for the defined daemon type.    Each daemon type key contains the following parameters:    <pre><code>spec:\n  hyperconverge:\n    tolerations:\n      &lt;daemonType&gt;:\n        rules:\n        - key: \"\"\n          operator: \"\"\n          value: \"\"\n          effect: \"\"\n          tolerationSeconds: 0\n</code></pre></p> <p>Possible values for <code>&lt;daemonType&gt;</code> are <code>osd</code>, <code>mon</code>, <code>mgr</code>, and <code>rgw</code>. The following values are also supported:</p> <ul> <li><code>all</code> - specifies general toleration rules for all daemons if no separate daemon rule is specified.</li> <li><code>mds</code> - specifies the CephFS Metadata Server daemons.</li> </ul> <p> Example configuration <pre><code>spec:\n  hyperconverge:\n    tolerations:\n      mon:\n        rules:\n        - effect: NoSchedule\n          key: node-role.kubernetes.io/controlplane\n          operator: Exists\n      mgr:\n        rules:\n        - effect: NoSchedule\n          key: node-role.kubernetes.io/controlplane\n          operator: Exists\n      osd:\n        rules:\n        - effect: NoSchedule\n          key: node-role.kubernetes.io/controlplane\n          operator: Exists\n      rgw:\n        rules:\n        - effect: NoSchedule\n          key: node-role.kubernetes.io/controlplane\n          operator: Exists\n</code></pre> </p> </li> <li> <p><code>resources</code> - Specifies resource requests or limits. The parameter is a map with the    daemon type as a key and the following structure as a value:    <pre><code>spec:\n  hyperconverge:\n    resources:\n      &lt;daemonType&gt;:\n        requests: &lt;kubernetes valid spec of daemon resource requests&gt;\n        limits: &lt;kubernetes valid spec of daemon resource limits&gt;\n</code></pre></p> <p>Possible values for <code>&lt;daemonType&gt;</code> are <code>mon</code>, <code>mgr</code>, <code>osd</code>, <code>osd-hdd</code>, <code>osd-ssd</code>, <code>osd-nvme</code>, <code>prepareosd</code>,  <code>rgw</code>, and <code>mds</code>. The <code>osd-hdd</code>, <code>osd-ssd</code>, and <code>osd-nvme</code> resource requirements handle only the Ceph OSDs  with a corresponding device class.</p> <p> Example configuration <pre><code>spec:\n  hyperconverge:\n    resources:\n      mon:\n        requests:\n          memory: 1Gi\n          cpu: 2\n        limits:\n          memory: 2Gi\n          cpu: 3\n      mgr:\n        requests:\n          memory: 1Gi\n          cpu: 2\n        limits:\n          memory: 2Gi\n          cpu: 3\n      osd:\n        requests:\n          memory: 1Gi\n          cpu: 2\n        limits:\n          memory: 2Gi\n          cpu: 3\n      osd-hdd:\n        requests:\n          memory: 1Gi\n          cpu: 2\n        limits:\n          memory: 2Gi\n          cpu: 3\n      osd-ssd:\n        requests:\n          memory: 1Gi\n          cpu: 2\n        limits:\n          memory: 2Gi\n          cpu: 3\n      osd-nvme:\n        requests:\n          memory: 1Gi\n          cpu: 2\n        limits:\n          memory: 2Gi\n          cpu: 3\n</code></pre> </p> </li> </ul> </li> <li> <p>For the Ceph node-specific resource settings, specify the <code>resources</code>    section in the corresponding <code>nodes</code> spec of <code>CephDeployment</code> CR:    <pre><code>spec:\n  nodes:\n  - name: &lt;nodeName&gt;\n    resources:\n      requests: &lt;kubernetes valid spec of daemon resource requests&gt;\n      limits: &lt;kubernetes valid spec of daemon resource limits&gt;\n</code></pre></p> <p>Substitute <code>&lt;nodeName&gt;</code> with the node requested for specific resources.  For example:  <pre><code>spec:\n  nodes:\n  - name: kaas-node-worker-1\n    resources:\n      requests:\n        memory: 1Gi\n        cpu: 2\n      limits:\n        memory: 2Gi\n        cpu: 3\n</code></pre></p> </li> <li> <p>For the RADOS Gateway instances specific resource settings, specify the    <code>resources</code> section in the <code>rgw</code> spec of <code>CephDeployment</code> CR:    <pre><code>spec:\n  objectStorage:\n    rgw:\n      gateway:\n        resources:\n          requests: &lt;kubernetes valid spec of daemon resource requests&gt;\n          limits: &lt;kubernetes valid spec of daemon resource limits&gt;\n</code></pre></p> <p>For example:  <pre><code>spec:\n  objectStorage:\n    rgw:\n      gateway:\n        resources:\n          requests:\n            memory: 1Gi\n            cpu: 2\n          limits:\n            memory: 2Gi\n            cpu: 3\n</code></pre></p> </li> <li> <p>Save the reconfigured <code>CephDeployment</code> CR and wait for Pelagia Deployment Controller to apply the updated    Ceph configuration to Rook. Rook will recreate Ceph Monitors, Ceph Managers, or Ceph OSDs according to the    specified <code>hyperconverge</code> configuration.</p> </li> <li>Specify tolerations for different Rook resources using Pelagia Helm chart values. For details, see    Rook Ceph daemons placement.</li> <li> <p>After a successful Ceph reconfiguration, unset the flags set in step 1    through the <code>pelagia-ceph-toolbox</code> pod:    <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- bash\nceph osd unset\nceph osd unset noout\nceph osd unset nobackfill\nceph osd unset norebalance\nceph osd unset norecover\nexit\n</code></pre></p> <p>Note</p> <p>Skip this step if you have only configured the PG rebalance timeout and replicas count parameters.</p> </li> </ol> <p>Once done, proceed to Verify Ceph tolerations and resources.</p>"},{"location":"ops-guide/deployment/ceph-resource-mgmt/verify-resource-mgmt/","title":"Verify Ceph resources Management","text":""},{"location":"ops-guide/deployment/ceph-resource-mgmt/verify-resource-mgmt/#verify-ceph-tolerations-and-resources","title":"Verify Ceph tolerations and resources","text":"<p>After you enable Ceph resources management as described in Enable management of Ceph tolerations and resources, perform the steps below to verify that the configured tolerations, requests, or limits have been successfully specified in the Ceph cluster.</p>"},{"location":"ops-guide/deployment/ceph-resource-mgmt/verify-resource-mgmt/#verify-ceph-tolerations-and-resources_1","title":"Verify Ceph tolerations and resources","text":"<ul> <li> <p>To verify that the required tolerations are specified in the Ceph cluster,   inspect the output of the following commands:   <pre><code>kubectl -n rook-ceph get $(kubectl -n rook-ceph get cephcluster -o name) -o jsonpath='{.spec.placement.mon.tolerations}'\nkubectl -n rook-ceph get $(kubectl -n rook-ceph get cephcluster -o name) -o jsonpath='{.spec.placement.mgr.tolerations}'\nkubectl -n rook-ceph get $(kubectl -n rook-ceph get cephcluster -o name) -o jsonpath='{.spec.placement.osd.tolerations}'\n</code></pre></p> </li> <li> <p>To verify RADOS Gateway tolerations:   <pre><code>kubectl -n rook-ceph get $(kubectl -n rook-ceph get cephobjectstore -o name) -o jsonpath='{.spec.gateway.placement.tolerations}'\n</code></pre></p> </li> <li> <p>To verify that the required resource requests or limits are specified for   the Ceph <code>mon</code>, <code>mgr</code>, or <code>osd</code> daemons, inspect the output of the   following command:   <pre><code>kubectl -n rook-ceph get $(kubectl -n rook-ceph get cephcluster -o name) -o jsonpath='{.spec.resources}'\n</code></pre></p> </li> <li> <p>To verify that the required resource requests and limits are specified for   the RADOS Gateway daemons, inspect the output of the following command:   <pre><code>kubectl -n rook-ceph get $(kubectl -n rook-ceph get cephobjectstore -o name) -o jsonpath='{.spec.gateway.resources}'\n</code></pre></p> </li> <li> <p>To verify that the required resource requests or limits are specified for   the Ceph OSDs <code>hdd</code>, <code>ssd</code>, or <code>nvme</code> device classes, perform the   following steps:</p> <ol> <li> <p>Identify which Ceph OSDs belong to the <code>&lt;deviceClass&gt;</code> device class in    question:    <pre><code>kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph osd crush class ls-osd &lt;deviceClass&gt;\n</code></pre></p> </li> <li> <p>For each <code>&lt;osdID&gt;</code> obtained in the previous step, run the following    command. Compare the output with the desired result.    <pre><code>kubectl -n rook-ceph get deploy rook-ceph-osd-&lt;osdID&gt; -o jsonpath='{.spec.template.spec.containers[].resources}'\n</code></pre></p> </li> </ol> </li> </ul>"},{"location":"ops-guide/deployment/ceph-user-creds/ceph-client/","title":"Enable Ceph RBD/CephFS Client","text":""},{"location":"ops-guide/deployment/ceph-user-creds/ceph-client/#manage-ceph-rbd-or-cephfs-clients","title":"Manage Ceph RBD or CephFS clients","text":"<p>The <code>CephDeployment</code> custom resource (CR) allows managing custom Ceph RADOS Block Device (RBD) or Ceph File System (CephFS) clients. This section describes how to create, access, and remove Ceph RBD or CephFS clients.</p> <p>For all supported parameters of Ceph clients, refer to CephDeployment: Clients parameters.</p>"},{"location":"ops-guide/deployment/ceph-user-creds/ceph-client/#create-an-rbd-or-cephfs-client","title":"Create an RBD or CephFS client","text":"<ol> <li> <p>Edit the <code>CephDeployment</code> CR by adding a new Ceph client to the <code>spec</code> section:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> <p>Example of adding an RBD client to the <code>kubernetes-ssd</code> pool:  <pre><code>spec:\n  clients:\n  - name: rbd-client\n    caps:\n      mon: allow r, allow command \"osd blacklist\"\n      osd: profile rbd pool=kubernetes-ssd\n</code></pre></p> <p>Example of adding a CephFS client to the <code>cephfs-1</code> Ceph File System:  <pre><code>spec:\n  clients:\n  - name: cephfs-1-client\n    caps:\n      mds: allow rw\n      mon: allow r, allow command \"osd blacklist\"\n      osd: allow rw tag cephfs data=cephfs-1 metadata=*\n</code></pre></p> <p>For details about <code>caps</code>, refer to  Ceph documentation: Authorization (capabilities).</p> </li> <li> <p>Wait for created clients to become ready in the <code>CephDeploymentHealth</code> CR status:    <pre><code>kubectl -n pelagia get cephdeploymenthealth -o yaml\n</code></pre></p> <p>Example output:  <pre><code>status:\n  healthReport:\n    rookCephObjects:\n      cephClients:\n        rbd-client:\n          info:\n            secretName: rook-ceph-client-rbd-client\n          observedGeneration: 1\n          phase: Ready\n        cephfs-1-client:\n          info:\n            secretName: rook-ceph-client-cephfs-1-client\n          observedGeneration: 1\n          phase: Ready\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/deployment/ceph-user-creds/ceph-client/#access-data-using-an-rbd-or-cephfs-client","title":"Access data using an RBD or CephFS client","text":"<ol> <li> <p>Using the <code>CephDeploymentSecret</code> status, obtain <code>secretInfo</code> with the Ceph    client credentials:    <pre><code>kubectl -n pelagia get cephdeploymentsecret -o yaml\n</code></pre></p> <p>Example output:  <pre><code>status:\n  secretInfo:\n    clientSecrets:\n    - name: client.rbd-client\n      secretName: rook-ceph-client-rbd-client\n      secretNamespace: rook-ceph\n    - name: client.cephfs-1-client\n      secretName: rook-ceph-client-cephfs-1-client\n      secretNamespace: rook-ceph\n</code></pre></p> </li> <li> <p>Use <code>secretName</code> and <code>secretNamespace</code> to access the Ceph client credentials:    <pre><code>kubectl -n &lt;secretNamespace&gt; get secret &lt;secretName&gt; -o jsonpath='{.data.&lt;clientName&gt;}' | base64 -d; echo\n</code></pre></p> <p>Substitute the following parameters:</p> <ul> <li><code>&lt;secretNamespace&gt;</code> with <code>secretNamespace</code> from the previous step;</li> <li><code>&lt;secretName&gt;</code> with <code>secretName</code> from the previous step;</li> <li><code>&lt;clientName&gt;</code> with the Ceph RBD or CephFS client name set in    <code>spec.clients</code> the <code>CephDeployment</code> resource, for example, <code>rbd-client</code>.</li> </ul> <p>Example output:  <pre><code>AQAGHDNjxWYXJhAAjafCn3EtC6KgzgI1x4XDlg==\n</code></pre></p> </li> <li> <p>Using the obtained credentials, create two configuration files on the    required workloads to connect them with Ceph pools or file systems:</p> <ul> <li> <p><code>/etc/ceph/ceph.conf</code>:    <pre><code>[default]\n   mon_host = &lt;mon1IP&gt;:6789,&lt;mon2IP&gt;:6789,...,&lt;monNIP&gt;:6789\n</code></pre></p> <p>where <code>mon_host</code> are the comma-separated IP addresses with <code>6789</code> ports of the current Ceph Monitors.  For example, <code>10.10.0.145:6789,10.10.0.153:6789,10.10.0.235:6789</code>.</p> </li> <li> <p><code>/etc/ceph/ceph.client.&lt;clientName&gt;.keyring</code>:    <pre><code>[client.&lt;clientName&gt;]\n    key = &lt;cephClientCredentials&gt;\n</code></pre></p> <ul> <li><code>&lt;clientName&gt;</code> is a client name set in <code>spec.clients</code> of the    <code>CephDeployment</code> resource. For example, <code>rbd-client</code>.</li> <li><code>&lt;cephClientCredentials&gt;</code> are the client credentials obtained in the    previous steps. For example, <code>AQAGHDNjxWYXJhAAjafCn3EtC6KgzgI1x4XDlg==</code>.</li> </ul> </li> </ul> </li> <li> <p>If the client <code>caps</code> parameters contain <code>mon: allow r</code>, verify the    client access using the following command:    <pre><code>ceph -n client.&lt;clientName&gt; -s\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/deployment/ceph-user-creds/ceph-client/#remove-an-rbd-or-cephfs-client","title":"Remove an RBD or CephFS client","text":"<ol> <li> <p>Edit the <code>CephDeployment</code> CR by removing the Ceph client from <code>spec.clients</code>:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>Wait for the client to be removed from the <code>CephDeployment</code>    status in <code>status.healthReport.rookCephObjects.cephClients</code>:    <pre><code>kubectl -n pelagia get cephdeploymenthealth -o yaml\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/deployment/ceph-user-creds/ceph-rgw-user/","title":"Enable Ceph Object Storage User","text":""},{"location":"ops-guide/deployment/ceph-user-creds/ceph-rgw-user/#manage-ceph-object-storage-users","title":"Manage Ceph Object Storage users","text":"<p>The <code>CephDeployment</code> custom resource (CR) allows managing custom Ceph Object Storage users. This section describes how to create, access, and remove Ceph Object Storage users.</p> <p>For all supported parameters of Ceph Object Storage users, refer to CephDeployment: Ceph Object Storage parameters.</p>"},{"location":"ops-guide/deployment/ceph-user-creds/ceph-rgw-user/#create-a-ceph-object-storage-user","title":"Create a Ceph Object Storage user","text":"<ol> <li> <p>Edit the <code>CephDeployment</code> CR by adding a new Ceph Object Storage user to    the <code>spec</code> section:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> <p>Example of adding the Ceph Object Storage user <code>user-a</code>:  <pre><code>spec:\n  objectStorage:\n    rgw:\n      name: rgw-store\n      objectUsers:\n      - capabilities:\n          bucket: '*'\n          metadata: read\n          user: read\n        displayName: user-a\n        name: userA\n        quotas:\n          maxBuckets: 10\n          maxSize: 10G\n</code></pre></p> </li> <li> <p>Wait for the created user to become ready in the <code>CephDeploymentHealth</code> status:    <pre><code>kubectl -n pelagia get cephdeploymenthealth -o yaml\n</code></pre></p> <p>Example output:  <pre><code>status:\n  healthReport:\n    rookCephObjects:\n      objectStorage:\n        cephObjectStoreUsers:\n          user-a:\n            info:\n              secretName: rook-ceph-object-user-rgw-store-user-a\n            observedGeneration: 1\n            phase: Ready\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/deployment/ceph-user-creds/ceph-rgw-user/#access-data-using-a-ceph-object-storage-user","title":"Access data using a Ceph Object Storage user","text":"<ol> <li> <p>Using the <code>CephDeploymentSecret</code> status, obtain <code>secretInfo</code> with the Ceph    user credentials:    <pre><code>kubectl -n pelagia get cephdeploymentsecret -o yaml\n</code></pre></p> <p>Example output:  <pre><code>status:\n  secretInfo:\n    rgwUserSecrets:\n    - name: user-a\n      secretName: rook-ceph-object-user-&lt;objstoreName&gt;-&lt;username&gt;\n      secretNamespace: rook-ceph\n</code></pre></p> <p>Substitute <code>&lt;objstoreName&gt;</code> with a Ceph Object Storage name and <code>&lt;username&gt;</code> with a Ceph Object Storage user name.</p> </li> <li> <p>Use <code>secretName</code> and <code>secretNamespace</code> to access the Ceph Object    Storage user credentials. The secret contains Amazon S3 access and secret    keys.</p> <ul> <li> <p>To obtain the user S3 access key:    <pre><code>kubectl -n &lt;secretNamespace&gt; get secret &lt;secretName&gt; -o jsonpath='{.data.AccessKey}' | base64 -d; echo\n</code></pre></p> <p>Substitute the following parameters in the commands above and below:</p> <ul> <li><code>&lt;secretNamespace&gt;</code> with <code>secretNamespace</code> from the previous step</li> <li><code>&lt;secretName&gt;</code> with <code>secretName</code> from the previous step</li> </ul> <p>Example output:  <pre><code>D49G060HQ86U5COBTJ13\n</code></pre></p> </li> <li> <p>To obtain the user S3 secret key:    <pre><code>kubectl -n &lt;secretNamespace&gt; get secret &lt;secretName&gt; -o jsonpath='{.data.SecretKey}' | base64 -d; echo\n</code></pre></p> <p>Example output:  <pre><code>bpuYqIieKvzxl6nzN0sd7L06H40kZGXNStD4UNda\n</code></pre></p> </li> </ul> </li> <li> <p>Configure the S3 client with the access and secret keys of the created user.    You can access the S3 client using various tools such as s3cmd or awscli.</p> </li> </ol>"},{"location":"ops-guide/deployment/ceph-user-creds/ceph-rgw-user/#remove-a-ceph-object-storage-user","title":"Remove a Ceph Object Storage user","text":"<ol> <li> <p>Edit the <code>CephDeployment</code> CR by removing the required Ceph    Object Storage user from <code>spec.objectStorage.rgw.objectUsers</code>:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>Wait for the removed user to be removed from the <code>CephDeploymentHealth</code>    status in <code>status.healthReport.rookCephObjects.objectStorageStatus.cephObjectStoreUsers</code>:    <pre><code>kubectl -n pelagia get cephdeploymenthealth -o yaml\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/deployment/s3-bucket-policy/bucket-policy-rockoon/","title":"Bucket Policy for Rockoon OpenStack User","text":""},{"location":"ops-guide/deployment/s3-bucket-policy/bucket-policy-rockoon/#set-a-bucket-policy-for-openstack-users","title":"Set a bucket policy for OpenStack users","text":"<p>The following procedure illustrates the process of setting a bucket policy for a bucket between two OpenStack users deployed by Rockoon.</p> <p>Due to specifics of the Ceph integration with OpenStack, you should configure the bucket policy for OpenStack users indirectly through setting permissions for corresponding OpenStack projects.</p> <p>For illustration purposes, we use the following names in the procedure:</p> <ul> <li><code>test01</code> for the bucket</li> <li><code>user-a</code>, <code>user-t</code> for the OpenStack users</li> <li><code>project-a</code>, <code>project-t</code> for the OpenStack projects</li> </ul>"},{"location":"ops-guide/deployment/s3-bucket-policy/bucket-policy-rockoon/#configure-an-amazon-s3-bucket-policy-for-openstack-users","title":"Configure an Amazon S3 bucket policy for OpenStack users","text":"<ol> <li> <p>Specify the <code>rookConfig</code> section of the <code>CephDeployment</code> custom resource:    <pre><code>spec:\n  rookConfig:\n    rgw keystone implicit tenants: \"swift\"\n</code></pre></p> </li> <li> <p>Prepare the Ceph Object Storage similarly to the procedure described in    Create Ceph Object Storage users.</p> </li> <li> <p>Create two OpenStack projects:    <pre><code>openstack project create project-a\nopenstack project create project-t\n</code></pre></p> <p>Example of system response:  <pre><code>+-------------+----------------------------------+\n| Field       | Value                            |\n+-------------+----------------------------------+\n| description |                                  |\n| domain_id   | default                          |\n| enabled     | True                             |\n| id          | faf957b776874a2e80384cb882ebf6ab |\n| is_domain   | False                            |\n| name        | project-a                         |\n| options     | {}                               |\n| parent_id   | default                          |\n| tags        | []                               |\n+-------------+----------------------------------+\n</code></pre></p> <p>You can also use existing projects. Save the ID of each project for the bucket policy specification.</p> <p>Note</p> <p>For details how to access OpenStack CLI, refer Access OpenStack.</p> </li> <li> <p>Create an OpenStack user for each project:    <pre><code>openstack user create user-a --project project-a\nopenstack user create user-t --project project-t\n</code></pre></p> <p>Example of system response:  <pre><code>+---------------------+----------------------------------+\n| Field               | Value                            |\n+---------------------+----------------------------------+\n| default_project_id  | faf957b776874a2e80384cb882ebf6ab |\n| domain_id           | default                          |\n| enabled             | True                             |\n| id                  | cc2607dc383e4494948d68eeb556f03b |\n| name                | user-a                            |\n| options             | {}                               |\n| password_expires_at | None                             |\n+---------------------+----------------------------------+\n</code></pre></p> <p>You can also use existing project users.</p> </li> <li> <p>Assign the <code>member</code> role to the OpenStack users:    <pre><code>openstack role add member --user user-a --project project-a\nopenstack role add member --user user-t --project project-t\n</code></pre></p> </li> <li> <p>Verify that the OpenStack users have obtained the <code>member</code> roles paying attention to the role IDs:    <pre><code>openstack role show member\n</code></pre></p> <p>Example of system response:  <pre><code>+-------------+----------------------------------+\n| Field       | Value                            |\n+-------------+----------------------------------+\n| description | None                             |\n| domain_id   | None                             |\n| id          | 8f0ce4f6cd61499c809d6169b2b5bd93 |\n| name        | member                           |\n| options     | {'immutable': True}              |\n+-------------+----------------------------------+\n</code></pre></p> </li> <li> <p>List the role assignments for the <code>user-a</code> and <code>user-t</code>:    <pre><code>openstack role assignment list --user user-a --project project-a\nopenstack role assignment list --user user-t --project project-t\n</code></pre></p> <p>Example of system response:  <pre><code>+----------------------------------+----------------------------------+-------+----------------------------------+--------+--------+-----------+\n| Role                             | User                             | Group | Project                          | Domain | System | Inherited |\n+----------------------------------+----------------------------------+-------+----------------------------------+--------+--------+-----------+\n| 8f0ce4f6cd61499c809d6169b2b5bd93 | cc2607dc383e4494948d68eeb556f03b |       | faf957b776874a2e80384cb882ebf6ab |        |        | False     |\n+----------------------------------+----------------------------------+-------+----------------------------------+--------+--------+-----------+\n</code></pre></p> </li> <li> <p>Create Amazon EC2 credentials for <code>user-a</code> and <code>user-t</code>:    <pre><code>openstack ec2 credentials create --user user-a --project project-a\nopenstack ec2 credentials create --user user-t --project project-t\n</code></pre></p> <p>Example of system response:  <pre><code>+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Field      | Value                                                                                                                                                          |\n+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| access     | d03971aedc2442dd9a79b3b409c32046                                                                                                                               |\n| links      | {'self': 'http://keystone-api.openstack.svc.cluster.local:5000/v3/users/cc2607dc383e4494948d68eeb556f03b/credentials/OS-EC2/d03971aedc2442dd9a79b3b409c32046'} |\n| project_id | faf957b776874a2e80384cb882ebf6ab                                                                                                                               |\n| secret     | 0a9fd8d9e0d24aecacd6e75951154d0f                                                                                                                               |\n| trust_id   | None                                                                                                                                                           |\n| user_id    | cc2607dc383e4494948d68eeb556f03b                                                                                                                               |\n+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre></p> <p>Obtain the values from the <code>access</code> and <code>secret</code> fields to connect with Ceph Object Storage  through the <code>s3cmd</code> tool.</p> <p>Note</p> <p>The <code>s3cmd</code> is a free command-line tool for uploading, retrieving, and managing data in Amazon S3 and other cloud storage service providers that use the S3 protocol. You can download the <code>s3cmd</code> tool from Amazon S3 tools: Download s3cmd.</p> </li> <li> <p>Create bucket users and configure a bucket policy for the <code>project-t</code>    OpenStack project similarly to the procedure described in    Set a bucket policy for a Ceph Object Storage user.    Ceph integration does not allow providing permissions for OpenStack users    directly. Therefore, you need to set permissions for the project that    corresponds to the user:    <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"S3Policy1\",\n  \"Statement\": [\n    {\n     \"Sid\": \"BucketAllow\",\n     \"Effect\": \"Allow\",\n     \"Principal\": {\n       \"AWS\": [\"arn:aws:iam::&lt;PROJECT-T_ID&gt;:root\"]\n     },\n     \"Action\": [\n       \"s3:ListBucket\",\n       \"s3:PutObject\",\n       \"s3:GetObject\"\n     ],\n     \"Resource\": [\n       \"arn:aws:s3:::test01\",\n       \"arn:aws:s3:::test01/*\"\n     ]\n    }\n  ]\n}\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/deployment/s3-bucket-policy/bucket-policy-rockoon/#ceph-object-storage-bucket-policy-examples","title":"Ceph Object Storage bucket policy examples","text":"<p>You can configure different bucket policies for various situations. See examples below.</p>"},{"location":"ops-guide/deployment/s3-bucket-policy/bucket-policy-rockoon/#provide-access-to-a-bucket-from-one-openstack-project-to-another","title":"Provide access to a bucket from one OpenStack project to another","text":"<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"S3Policy1\",\n  \"Statement\": [\n    {\n     \"Sid\": \"BucketAllow\",\n     \"Effect\": \"Allow\",\n     \"Principal\": {\n       \"AWS\": [\"arn:aws:iam::&lt;osProjectId&gt;:root\"]\n     },\n     \"Action\": [\n       \"s3:ListBucket\",\n       \"s3:PutObject\",\n       \"s3:GetObject\"\n     ],\n     \"Resource\": [\n       \"arn:aws:s3:::&lt;bucketName&gt;\",\n       \"arn:aws:s3:::&lt;bucketName&gt;/*\"\n     ]\n    }\n  ]\n}\n</code></pre> <p>Substitute the following parameters:</p> <ul> <li><code>&lt;osProjectId&gt;</code> - the target OpenStack project ID</li> <li><code>&lt;bucketName&gt;</code> - the target bucket name where the policy will be set</li> </ul>"},{"location":"ops-guide/deployment/s3-bucket-policy/bucket-policy-rockoon/#provide-access-to-a-bucket-from-a-ceph-object-storage-user-to-an-openstack-project","title":"Provide access to a bucket from a Ceph Object Storage user to an OpenStack project","text":"<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"S3Policy1\",\n  \"Statement\": [\n    {\n     \"Sid\": \"BucketAllow\",\n     \"Effect\": \"Allow\",\n     \"Principal\": {\n       \"AWS\": [\"arn:aws:iam::&lt;osProjectId&gt;:root\"]\n     },\n     \"Action\": [\n       \"s3:ListBucket\",\n       \"s3:PutObject\",\n       \"s3:GetObject\"\n     ],\n     \"Resource\": [\n       \"arn:aws:s3:::&lt;bucketName&gt;\",\n       \"arn:aws:s3:::&lt;bucketName&gt;/*\"\n     ]\n    }\n  ]\n}\n</code></pre> <p>Substitute the following parameters:</p> <ul> <li><code>&lt;osProjectId&gt;</code> - the target OpenStack project ID</li> <li><code>&lt;bucketName&gt;</code> - the target bucket name where policy will be set</li> </ul>"},{"location":"ops-guide/deployment/s3-bucket-policy/bucket-policy-rockoon/#provide-access-to-a-bucket-from-an-openstack-user-to-a-ceph-object-storage-user","title":"Provide access to a bucket from an OpenStack user to a Ceph Object Storage user","text":"<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"S3Policy1\",\n  \"Statement\": [\n    {\n     \"Sid\": \"BucketAllow\",\n     \"Effect\": \"Allow\",\n     \"Principal\": {\n       \"AWS\": [\"arn:aws:iam:::user/&lt;userName&gt;\"]\n     },\n     \"Action\": [\n       \"s3:ListBucket\",\n       \"s3:PutObject\",\n       \"s3:GetObject\"\n     ],\n     \"Resource\": [\n       \"arn:aws:s3:::&lt;bucketName&gt;\",\n       \"arn:aws:s3:::&lt;bucketName&gt;/*\"\n     ]\n    }\n  ]\n}\n</code></pre> <p>Substitute the following parameters:</p> <ul> <li><code>&lt;userName&gt;</code> - the target Ceph Object Storage User name</li> <li><code>&lt;bucketName&gt;</code> - the target bucket name where policy will be set</li> </ul>"},{"location":"ops-guide/deployment/s3-bucket-policy/bucket-policy-rockoon/#provide-access-to-a-bucket-from-one-ceph-object-storage-user-to-another","title":"Provide access to a bucket from one Ceph Object Storage user to another","text":"<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"S3Policy1\",\n  \"Statement\": [\n    {\n     \"Sid\": \"BucketAllow\",\n     \"Effect\": \"Allow\",\n     \"Principal\": {\n       \"AWS\": [\"arn:aws:iam:::user/&lt;userName&gt;\"]\n     },\n     \"Action\": [\n       \"s3:ListBucket\",\n       \"s3:PutObject\",\n       \"s3:GetObject\"\n     ],\n     \"Resource\": [\n       \"arn:aws:s3:::&lt;bucketName&gt;\",\n       \"arn:aws:s3:::&lt;bucketName&gt;/*\"\n     ]\n    }\n  ]\n}\n</code></pre> <p>Substitute the following parameters:</p> <ul> <li><code>&lt;userName&gt;</code> - the target Ceph Object Storage user name</li> <li><code>&lt;bucketName&gt;</code> - the target bucket name where policy will be set</li> </ul>"},{"location":"ops-guide/deployment/s3-bucket-policy/bucket-policy-rockoon/#see-also","title":"SEE ALSO","text":"<ul> <li>AWS S3: Bucket policy examples</li> <li>Ceph documentation: Bucket policies</li> </ul>"},{"location":"ops-guide/deployment/s3-bucket-policy/bucket-policy-s3/","title":"Set a bucket policy for a Ceph Object Storage user","text":"<p>Amazon S3 is an object storage service with different access policies. A bucket policy is a resource-based policy that grants permissions to a bucket and objects in it. For more details, see Amazon S3 documentation: Using bucket policies.</p> <p>The following procedure illustrates the process of setting a bucket policy for a bucket (<code>test01</code>) stored in a Ceph Object Storage. The bucket policy requires at least two users: a bucket owner (<code>user-a</code>) and a bucket user (<code>user-t</code>). The bucket owner creates the bucket and sets the policy that regulates access for the bucket user.</p> <p>The procedure uses <code>s3cmd</code> command-line tool. To configure <code>s3cmd</code> for using it with Ceph Object Storage, please refer to Configure s3cmd.</p>"},{"location":"ops-guide/deployment/s3-bucket-policy/bucket-policy-s3/#configure-s3cmd-command-line-tool","title":"Configure <code>s3cmd</code> command-line tool","text":"<p>Note</p> <p>The s3cmd is a free command-line tool and client for uploading, retrieving, and managing data in Amazon S3 and other cloud storage service providers that use the S3 protocol. You can download the s3cmd CLI tool from Amazon S3 tools: Download s3cmd.</p> <p>Configure the s3cmd client with some s3 credentials, you need to run: <pre><code>s3cmd --configure --ca-certs=ca.crt\n</code></pre></p> <p>where <code>ca.crt</code> is a CA certificate signs Ceph Object Storage public endpoint.</p> <p>The command will ask to specify the following bucket access parameters:</p> Parameter Description Comment <code>Access Key</code> Public part of access credentials. Specify a user access key. <code>Secret Key</code> Secret part of access credentials. Specify a user secret key. <code>Default Region</code> Region of AWS servers where requests are sent by default. Use the default value. <code>S3 Endpoint</code> Connection point to the Ceph Object Storage. Specify the Ceph Object Storage public endpoint. <code>DNS-style bucket+hostname:port template for accessing a bucket</code> Bucket location. Specify the Ceph Object Storage public endpoint. <code>Path to GPG program</code> Path to the GNU Privacy Guard encryption suite. Use the default value. <code>Use HTTPS protocol</code> HTTPS protocol switch. Specify <code>Yes</code>. <code>HTTP Proxy server name</code> HTTP Proxy server name. Skip this parameter. <p>When configured correctly, the <code>s3cmd</code> tool connects to the Ceph Object Storage. Save new settings when prompted by the system.</p>"},{"location":"ops-guide/deployment/s3-bucket-policy/bucket-policy-s3/#configure-an-amazon-s3-bucket-policy","title":"Configure an Amazon S3 bucket policy","text":"<ol> <li>Configure the <code>s3cmd</code> client with the <code>user-a</code> credentials <code>AccessKey</code> and <code>SecretKey</code>.</li> <li> <p>As <code>user-a</code>, create a new bucket <code>test01</code>:    <pre><code>s3cmd mb s3://test01\n</code></pre></p> <p>Example of a positive system response:  <pre><code>Bucket 's3://test01/' created\n</code></pre></p> </li> <li> <p>Upload an object to the bucket:    <pre><code>touch test.txt\ns3cmd put test.txt s3://test01\n</code></pre></p> <p>Example of a positive system response:  <pre><code>upload: 'test.txt' -&gt; 's3://test01/test.txt'  [1 of 1]\n0 of 0     0% in    0s     0.00 B/s  done\n</code></pre></p> </li> <li> <p>Verify that the object is in the <code>test01</code> bucket:    <pre><code>s3cmd ls s3://test01\n</code></pre></p> <p>Example of a positive system response:  <pre><code>2022-09-02 13:06            0  s3://test01/test.txt\n</code></pre></p> </li> <li> <p>Create the bucket policy file and add bucket CRUD permissions    for <code>user-t</code>:    <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"S3Policy1\",\n  \"Statement\": [\n    {\n     \"Sid\": \"BucketAllow\",\n     \"Effect\": \"Allow\",\n     \"Principal\": {\n       \"AWS\": [\"arn:aws:iam:::user/user-t\"]\n     },\n     \"Action\": [\n       \"s3:ListBucket\",\n       \"s3:PutObject\",\n       \"s3:GetObject\"\n     ],\n     \"Resource\": [\n       \"arn:aws:s3:::test01\",\n       \"arn:aws:s3:::test01/*\"\n     ]\n    }\n  ]\n}\n</code></pre></p> </li> <li> <p>Set the bucket policy for the <code>test01</code> bucket:    <pre><code>s3cmd setpolicy policy.json s3://test01\n</code></pre></p> <p>Example of a positive system response:  <pre><code>s3://test01/: Policy updated\n</code></pre></p> </li> <li> <p>Verify that the <code>user-t</code> has access for the <code>test01</code> bucket by    reconfiguring the <code>s3cmd</code> client with the <code>user-t</code> credentials <code>AccessKey</code> and <code>SecretKey</code>.    Verify that the <code>user-t</code> can read the bucket <code>test01</code> content:    <pre><code>s3cmd ls s3://test01\n</code></pre></p> <p>Example of a positive system response:  <pre><code>2022-09-02 13:06            0  s3://test01/test.txt\n</code></pre></p> </li> <li> <p>Download the object from the <code>test01</code> bucket:    <pre><code>s3cmd get s3://test01/test.txt check.txt\n</code></pre></p> <p>Example of a positive system response:  <pre><code>download: 's3://test01/test.txt' -&gt; 'check.txt'  [1 of 1]\n 0 of 0     0% in    0s     0.00 B/s  done\n</code></pre></p> </li> <li> <p>Upload a new object to the <code>test01</code> bucket:    <pre><code>s3cmd put check.txt s3://test01\n</code></pre></p> <p>Example of a positive system response:  <pre><code>upload: 'check.txt' -&gt; 's3://test01/check.txt'  [1 of 1]\n 0 of 0     0% in    0s     0.00 B/s  done\n</code></pre></p> </li> <li> <p>Verify that the object is in the <code>test01</code> bucket:     <pre><code>s3cmd ls s3://test01\n</code></pre></p> <p>Example of a positive system response:   <pre><code>2022-09-02 14:41            0  s3://test01/check.txt\n2022-09-02 13:06            0  s3://test01/test.txt\n</code></pre></p> </li> <li> <p>Verify the new object by reconfiguring the <code>s3cmd</code> client with the     <code>user-a</code> credentials:     <pre><code>s3cmd --configure --ca-certs=ca.crt\n</code></pre></p> </li> <li> <p>List <code>test01</code> bucket objects:     <pre><code>s3cmd ls s3://test01\n</code></pre></p> <p>Example of a positive system response:   <pre><code>2022-09-02 14:41            0  s3://test01/check.txt\n2022-09-02 13:06            0  s3://test01/test.txt\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/deployment/s3-bucket-policy/s3-create-user/","title":"Create Ceph Object Storage User","text":""},{"location":"ops-guide/deployment/s3-bucket-policy/s3-create-user/#create-ceph-object-storage-users","title":"Create Ceph Object Storage users","text":"<p>Ceph Object Storage users can create Amazon S3 buckets and bucket policies that grant access to other users.</p> <p>This section describes how to create two Ceph Object Storage users and configure their S3 credentials.</p>"},{"location":"ops-guide/deployment/s3-bucket-policy/s3-create-user/#create-and-configure-ceph-object-storage-users","title":"Create and configure Ceph Object Storage users","text":"<ol> <li> <p>Open the <code>CephDeployment</code> custom resource for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>In the <code>spec.objectStorage.rgw</code> section, add new Ceph Object Storage users.    For example:    <pre><code>spec:\n  objectStorage:\n    rgw:\n      objectUsers:\n      - name: user-b\n        displayName: user-a\n        capabilities:\n          bucket: \"*\"\n          user: read\n      - name: user-t\n        displayName: user-t\n        capabilities:\n          bucket: \"*\"\n          user: read\n</code></pre></p> </li> <li> <p>Verify that <code>rgwUserSecrets</code> are created for both users:    <pre><code>kubectl -n pelagia get cephdeploymentsecret -o yaml\n</code></pre></p> <p>Example of a positive system response:  <pre><code>status:\n  secretInfo:\n    rgwUserSecrets:\n    - name: user-a\n      secretName: &lt;user-aCredSecretName&gt;\n      secretNamespace: &lt;user-aCredSecretNamespace&gt;\n    - name: user-t\n      secretName: &lt;user-tCredSecretName&gt;\n      secretNamespace: &lt;user-tCredSecretNamespace&gt;\n</code></pre></p> </li> <li> <p>Obtain S3 user credentials from the cluster secrets. Specify an access key and a secret key for both users:    <pre><code>kubectl -n &lt;user-aCredSecretNamespace&gt; get secret &lt;user-aCredSecretName&gt; -o jsonpath='{.data.AccessKey}' | base64 -d\nkubectl -n &lt;user-aCredSecretNamespace&gt; get secret &lt;user-aCredSecretName&gt; -o jsonpath='{.data.SecretKey}' | base64 -d\nkubectl -n &lt;user-tCredSecretNamespace&gt; get secret &lt;user-tCredSecretName&gt; -o jsonpath='{.data.AccessKey}' | base64 -d\nkubectl -n &lt;user-tCredSecretNamespace&gt; get secret &lt;user-tCredSecretName&gt; -o jsonpath='{.data.SecretKey}' | base64 -d\n</code></pre></p> <p>Substitute the corresponding <code>secretNamespace</code> and <code>secretName</code> for both  users.</p> </li> <li> <p>Obtain Ceph Object Storage public endpoint from the  <code>CephDeploymentHealth</code> status:    <pre><code>kubectl -n pelagia get cephdeploymenthealth -o yaml | grep publicEndpoint\n</code></pre></p> <p>Example of a positive system response:  <pre><code>publicEndpoint: https://object-storage.just.example.com\n</code></pre></p> </li> <li> <p>Obtain the CA certificate to use an HTTPS endpoint:    <pre><code>kubectl -n rook-ceph get secret $(kubectl -n rook-ceph get ingress -o jsonpath='{.items[0].spec.tls[0].secretName}{\"\\n\"}') -o jsonpath='{.data.ca\\.crt}' | base64 -d; echo\n</code></pre></p> <p>Save the output to <code>ca.crt</code>.</p> </li> </ol>"},{"location":"ops-guide/lcm/add-rm-ceph-node/","title":"Ceph Node Lifecycle Management","text":""},{"location":"ops-guide/lcm/add-rm-ceph-node/#add-remove-or-reconfigure-ceph-nodes","title":"Add, remove, or reconfigure Ceph nodes","text":"<p>Pelagia Lifecycle Management (LCM) Controller simplifies Ceph cluster management by automating LCM operations. This section describes how to add, remove, or reconfigure Ceph nodes.</p> <p>Note</p> <p>When adding a Ceph node with the Ceph Monitor role, if any issues occur with the Ceph Monitor, <code>rook-ceph</code> removes it and adds a new Ceph Monitor instead, named using the next alphabetic character in order. Therefore, the Ceph Monitor names may not follow the alphabetical order. For example, <code>a</code>, <code>b</code>, <code>d</code>, instead of <code>a</code>, <code>b</code>, <code>c</code>.</p>"},{"location":"ops-guide/lcm/add-rm-ceph-node/#add-a-ceph-node","title":"Add a Ceph node","text":"<ol> <li>Prepare a new node for the cluster.</li> <li> <p>Open the <code>CephDeployment</code> custom resource (CR) for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>In the <code>nodes</code> section, specify the parameters for a Ceph node as    required. For the parameter description, see    CephDeployment: Nodes parameters.</p> <p>The example configuration of the <code>nodes</code> section with the new node:  <pre><code>nodes:\n- name: storage-worker-414\n  roles:\n  - mon\n  - mgr\n  devices:\n  - config:\n      deviceClass: hdd\n    fullPath: /dev/disk/by-id/scsi-SATA_HGST_HUS724040AL_PN1334PEHN18ZS\n</code></pre></p> <p>You can also add a new node with device filters. For example:  <pre><code>nodes:\n- name: storage-worker-414\n  roles:\n  - mon\n  - mgr\n  config:\n   deviceClass: hdd\n  devicePathFilter: \"^/dev/disk/by-id/scsi-SATA_HGST+*\"\n</code></pre></p> <p>Warning</p> <p>We highly recommend using the non-wwn <code>by-id</code> symlinks to specify storage devices in the <code>devices</code> list. For details, see Architecture: Addressing Ceph devices.</p> <p>Note</p> <ul> <li>To use a new Ceph node for a Ceph Monitor or Ceph Manager deployment,   also specify the <code>roles</code> parameter.</li> <li>Reducing the number of Ceph Monitors is not supported and causes the   Ceph Monitor daemons removal from random nodes.</li> <li>Removal of the <code>mgr</code> role in the <code>nodes</code> section of the   <code>CephDeployment</code> CR does not remove Ceph Managers. To remove a Ceph   Manager from a node, remove it from the <code>nodes</code> spec and manually   delete the <code>mgr</code> pod in the Rook namespace.</li> </ul> </li> <li> <p>Verify that all new Ceph daemons for the specified node have been    successfully deployed in the Ceph cluster. The <code>CephDeploymentHealth</code> CR    <code>status.healthReport.cephDaemons.cephDaemons</code> should not contain any issues.    <pre><code>kubectl -n pelagia get cephdeploymenthealth -o yaml\n</code></pre></p> <p>Example of system response:  <pre><code>status:\n  healthReport:\n    cephDaemons:\n      cephDaemons:\n        mgr:\n          info:\n          - 'a is active mgr, standbys: [b]'\n          status: ok\n        mon:\n          info:\n          - 3 mons, quorum [a b c]\n          status: ok\n        osd:\n          info:\n          - 3 osds, 3 up, 3 in\n          status: ok\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/lcm/add-rm-ceph-node/#remove-a-ceph-node","title":"Remove a Ceph node","text":"<p>Note</p> <p>Ceph node removal presupposes usage of a <code>CephOsdRemoveTask</code> CR. For workflow overview, see High-level workflow of Ceph OSD or node removal.</p> <p>Note</p> <p>To remove a Ceph node with a <code>mon</code> role, first move the Ceph Monitor to another node and remove the <code>mon</code> role from the Ceph node as described in Move a Ceph Monitor daemon to another node.</p> <ol> <li> <p>Open the <code>CephDeployment</code> CR for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>In the <code>nodes</code> section, remove the required Ceph node specification.</p> <p>For example:  <pre><code>spec:\n    nodes:\n    - name: storage-worker-5 # remove the entire entry for the required node\n      devices: {...}\n      roles: [...]\n</code></pre></p> </li> <li> <p>Create a YAML template for the <code>CephOsdRemoveTask</code> CR. For example:    <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephOsdRemoveTask\nmetadata:\n  name: remove-osd-worker-5\n  namespace: pelagia\nspec:\n  nodes:\n    storage-worker-5:\n      completeCleanUp: true\n</code></pre></p> </li> <li> <p>Apply the template on the Rockoon cluster:    <pre><code>kubectl apply -f remove-osd-worker-5.yaml\n</code></pre></p> </li> <li> <p>Verify that the corresponding request has been created:    <pre><code>kubectl -n pelagia get cephosdremovetask remove-osd-worker-5\n</code></pre></p> </li> <li> <p>Verify that the <code>removeInfo</code> section appeared in the <code>CephOsdRemoveTask</code> CR <code>status</code>:    <pre><code>kubectl -n pelagia get cephosdremovetask remove-osd-worker-5 -o yaml\n</code></pre></p> <p>Example of system response:</p> <pre><code>status:\n  removeInfo:\n    cleanupMap:\n      storage-worker-5:\n        osdMapping:\n          \"10\":\n            deviceMapping:\n              sdb:\n                path: \"/dev/disk/by-path/pci-0000:00:1t.9\"\n                partition: \"/dev/ceph-b-vg_sdb/osd-block-b-lv_sdb\"\n                type: \"block\"\n                class: \"hdd\"\n                zapDisk: true\n          \"16\":\n            deviceMapping:\n              sdc:\n                path: \"/dev/disk/by-path/pci-0000:00:1t.10\"\n                partition: \"/dev/ceph-b-vg_sdb/osd-block-b-lv_sdc\"\n                type: \"block\"\n                class: \"hdd\"\n                zapDisk: true\n</code></pre> </li> <li> <p>Verify that the <code>cleanupMap</code> section matches the required removal and wait    for the <code>ApproveWaiting</code> phase to appear in <code>status</code>:    <pre><code>kubectl -n pelagia get cephosdremovetask remove-osd-worker-5 -o yaml\n</code></pre></p> <p>Example of system response:  <pre><code>status:\n  phase: ApproveWaiting\n</code></pre></p> </li> <li> <p>Edit the <code>CephOsdRemoveTask</code> CR and set the <code>approve</code> flag to <code>true</code>:    <pre><code>kubectl -n pelagia edit cephosdremovetask remove-osd-worker-5\n</code></pre></p> <p>For example:  <pre><code>spec:\n  approve: true\n</code></pre></p> </li> <li> <p>Review the status of the <code>CephOsdRemoveTask</code> resource    processing. The valuable parameters are as follows:</p> <ul> <li><code>status.phase</code> - the current state of task processing</li> <li><code>status.messages</code> - the description of the current phase</li> <li><code>status.conditions</code> - full history of task processing before the    current phase</li> <li><code>status.removeInfo.issues</code> and <code>status.removeInfo.warnings</code> - contain    error and warning messages occurred during task processing</li> </ul> </li> <li> <p>Verify that the <code>CephOsdRemoveTask</code> has been completed. For example:     <pre><code>status:\n  phase: Completed # or CompletedWithWarnings if there are non-critical issues\n</code></pre></p> </li> <li> <p>Remove the device cleanup jobs:     <pre><code>kubectl delete jobs -n pelagia -l app=pelagia-lcm-cleanup-disks\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/lcm/add-rm-ceph-node/#reconfigure-a-ceph-node","title":"Reconfigure a Ceph node","text":"<p>There is no hot reconfiguration procedure for existing Ceph OSDs and Ceph Monitors. To reconfigure an existing Ceph node, follow the steps below:</p> <ol> <li>Remove the Ceph node from the Ceph cluster as described in    Remove a Ceph node.</li> <li>Add the same Ceph node but with a modified configuration as described in    Add a Ceph node.</li> </ol>"},{"location":"ops-guide/lcm/add-rm-ceph-osd-with-meta/","title":"Ceph OSD with metadata Lifecycle Management","text":""},{"location":"ops-guide/lcm/add-rm-ceph-osd-with-meta/#add-remove-or-reconfigure-ceph-osds-with-metadata-devices","title":"Add, remove, or reconfigure Ceph OSDs with metadata devices","text":"<p>Pelagia Lifecycle Management (LCM) Controller simplifies Ceph cluster management by automating LCM operations. This section describes how to add, remove, or reconfigure Ceph OSDs with a separate metadata device.</p>"},{"location":"ops-guide/lcm/add-rm-ceph-osd-with-meta/#add-a-ceph-osd-with-a-metadata-device","title":"Add a Ceph OSD with a metadata device","text":"<ol> <li> <p>Configure one disk for data and one logical volume for metadata of a Ceph OSD to be added to the Ceph cluster.</p> <p>Note</p> <p>If you add a new disk after node provisioning, manually prepare the required node devices using  Logical Volume Manager (LVM) 2 on the existing node.</p> </li> <li> <p>Optional. If you want to add a Ceph OSD on top of a raw device that already exists    on a node or is hot-plugged, add the required device using the following    guidelines:</p> <ul> <li>You can add a raw device to a node during node deployment.</li> <li>If a node supports adding devices without a node reboot, you can hot plug   a raw device to a node.</li> <li>If a node does not support adding devices without a node reboot, you can   hot plug a raw device during node shutdown.</li> </ul> </li> <li> <p>Open the <code>CephDeployment</code> custom resource (CR) for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>In the <code>nodes.&lt;nodeName&gt;.devices</code> section, specify the    parameters for a Ceph OSD as required. For the parameters description, see    CephDeployment: Nodes parameters.</p> <p>The example configuration of the <code>nodes</code> section with the new node:  <pre><code>nodes:\n- name: storage-worker-505\n  roles:\n  - mon\n  - mgr\n  devices:\n  - config: # existing item\n      deviceClass: hdd\n    fullPath: /dev/disk/by-id/scsi-SATA_HGST_HUS724040AL_PN1334PEHN18ZS\n  - config: # new item\n      deviceClass: hdd\n      metadataDevice: /dev/bluedb/meta_1\n    fullPath: /dev/disk/by-id/scsi-0ATA_HGST_HUS724040AL_PN1334PEHN1VBC\n</code></pre></p> <p>Warning</p> <p>We highly recommend using the non-wwn <code>by-id</code> symlinks to specify storage devices in the <code>devices</code> list. For details, see Architecture: Addressing Ceph devices.</p> </li> <li> <p>Verify that the Ceph OSD is successfully deployed on the specified node. The <code>CephDeploymentHealth</code> CR    <code>status.healthReport.cephDaemons.cephDaemons</code> section should not contain any issues:    <pre><code>kubectl -n pelagia get cephdeploymenthealth -o yaml\n</code></pre></p> <p>For example:  <pre><code>status:\n  healthReport:\n    cephDaemons:\n      cephDaemons:\n        osd:\n          info:\n          - 4 osds, 4 up, 4 in\n          status: ok\n</code></pre></p> </li> <li> <p>Verify the Ceph OSD status:    <pre><code>kubectl -n rook-ceph get pod -l app=rook-ceph-osd -o wide | grep &lt;nodeName&gt;\n</code></pre></p> <p>Substitute <code>&lt;nodeName&gt;</code> with the corresponding node name.</p> <p>Example of system response:  <pre><code>rook-ceph-osd-0-7b8d4d58db-f6czn   1/1     Running   0          42h   10.100.91.6   kaas-node-6c5e76f9-c2d2-4b1a-b047-3c299913a4bf   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-1-78fbc47dc5-px9n2   1/1     Running   0          21h   10.100.91.6   kaas-node-6c5e76f9-c2d2-4b1a-b047-3c299913a4bf   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-3-647f8d6c69-87gxt   1/1     Running   0          21h   10.100.91.6   kaas-node-6c5e76f9-c2d2-4b1a-b047-3c299913a4bf   &lt;none&gt;           &lt;none&gt;\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/lcm/add-rm-ceph-osd-with-meta/#remove-a-ceph-osd-with-a-metadata-device","title":"Remove a Ceph OSD with a metadata device","text":"<p>Note</p> <p>Ceph OSD removal presupposes usage of a <code>CephOsdRemoveTask</code> CR. For workflow overview, see High-level workflow of Ceph OSD or node removal.</p> <p>Warning</p> <p>When using the non-recommended Ceph pools <code>replicated.size</code> of less than <code>3</code>, Ceph OSD removal cannot be performed. The minimal replica size equals a rounded up half of the specified <code>replicated.size</code>.</p> <p>For example, if <code>replicated.size</code> is <code>2</code>, the minimal replica size is <code>1</code>, and if <code>replicated.size</code> is <code>3</code>, then the minimal replica size is <code>2</code>. The replica size of <code>1</code> allows Ceph having PGs with only one Ceph OSD in the <code>acting</code> state, which may cause a <code>PG_TOO_DEGRADED</code> health warning that blocks Ceph OSD removal. We recommend setting <code>replicated.size</code> to <code>3</code> for each Ceph pool.</p> <ol> <li> <p>Open the <code>CephDeployment</code> object for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>Remove the required Ceph OSD specification from the <code>spec.nodes.&lt;nodeName&gt;.devices</code> list:</p> <p>The example configuration of the <code>nodes</code> section with the new node:  <pre><code>nodes:\n- name: storage-worker-505\n  roles:\n  - mon\n  - mgr\n  storageDevices:\n  - config:\n      deviceClass: hdd\n    fullPath: /dev/disk/by-id/scsi-SATA_HGST_HUS724040AL_PN1334PEHN18ZS\n  - config: # remove the entire item entry from devices list\n      deviceClass: hdd\n      metadataDevice: /dev/bluedb/meta_1\n    fullPath: /dev/disk/by-id/scsi-0ATA_HGST_HUS724040AL_PN1334PEHN1VBC\n</code></pre></p> </li> <li> <p>Create a YAML template for the <code>CephOsdRemoveTask</code> CR. Select from the following options:</p> <ul> <li> <p>Remove Ceph OSD by device name, <code>by-path</code> symlink, or <code>by-id</code> symlink:   <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephOsdRemoveTask\nmetadata:\n  name: remove-osd-&lt;nodeName&gt;-task\n  namespace: pelagia\nspec:\n  nodes:\n    &lt;nodeName&gt;:\n      cleanupByDevice:\n      - device: sdb\n      - device: sdc\n</code></pre></p> <p>Warning</p> <p>We do not recommend setting device name or device <code>by-path</code> symlink in the <code>cleanupByDevice</code> field as these identifiers are not persistent and can change at node boot. Remove Ceph OSDs with <code>by-id</code> symlinks or use <code>cleanupByOsdId</code> instead. For details, see Architecture: Addressing Ceph devices.</p> <p>Note</p> <p>If a device was physically removed from a node, <code>cleanupByDevice</code> is not supported. Therefore, use <code>cleanupByOsdId</code> instead.</p> </li> <li> <p>Remove Ceph OSD by OSD ID:   <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephOsdRemoveTask\nmetadata:\n  name: remove-osd-&lt;nodeName&gt;-task\n  namespace: pelagia\nspec:\n  nodes:\n    &lt;nodeName&gt;:\n      cleanupByOsdId:\n      - id: 5\n      - id: 10\n</code></pre></p> </li> </ul> </li> <li> <p>Apply the template:    <pre><code>kubectl apply -f remove-osd-&lt;nodeName&gt;-task.yaml\n</code></pre></p> </li> <li> <p>Verify that the corresponding task has been created:    <pre><code>kubectl -n pelagia get cephosdremovetask remove-osd-&lt;nodeName&gt;-task\n</code></pre></p> </li> <li> <p>Verify that the <code>removeInfo</code> section appeared in the <code>CephOsdRemoveTask</code> CR <code>status</code>:    <pre><code>kubectl -n pelagia get cephosdremovetask remove-osd-&lt;nodeName&gt;-task -o yaml\n</code></pre></p> <p>Example of system response:  <pre><code>status:\n  removeInfo:\n    cleanupMap:\n      storage-worker-505:\n        osdMapping:\n          \"10\":\n            deviceMapping:\n              sdb:\n                path: \"/dev/disk/by-path/pci-0000:00:1t.9\"\n                partition: \"/dev/ceph-b-vg_sdb/osd-block-b-lv_sdb\"\n                type: \"block\"\n                class: \"hdd\"\n                zapDisk: true\n          \"5\":\n            deviceMapping:\n              /dev/sdc:\n                deviceClass: hdd\n                devicePath: /dev/disk/by-path/pci-0000:00:0f.0\n                devicePurpose: block\n                usedPartition: /dev/ceph-2d11bf90-e5be-4655-820c-fb4bdf7dda63/osd-block-e41ce9a8-4925-4d52-aae4-e45167cfcf5c\n                zapDisk: true\n              /dev/sdf:\n                deviceClass: hdd\n                devicePath: /dev/disk/by-path/pci-0000:00:12.0\n                devicePurpose: db\n                usedPartition: /dev/bluedb/meta_1\n</code></pre></p> </li> <li> <p>Verify that the <code>cleanupMap</code> section matches the required removal and    wait for the <code>ApproveWaiting</code> phase to appear in <code>status</code>:    <pre><code>kubectl -n pelagia get cephosdremovetask remove-osd-&lt;nodeName&gt;-task -o yaml\n</code></pre></p> <p>Example of system response:  <pre><code>status:\n  phase: ApproveWaiting\n</code></pre></p> </li> <li> <p>In the <code>CephOsdRemoveTask</code> CR, set the <code>approve</code> flag to <code>true</code>:    <pre><code>kubectl -n pelagia edit cephosdremovetask remove-osd-&lt;nodeName&gt;-task\n</code></pre></p> <p>Configuration snippet:  <pre><code>spec:\n  approve: true\n</code></pre></p> </li> <li> <p>Review the following <code>status</code> fields of the Ceph LCM CR processing:</p> <ul> <li><code>status.phase</code> - current state of task processing;</li> <li><code>status.messages</code> - description of the current phase;</li> <li><code>status.conditions</code> - full history of task processing before the    current phase;</li> <li><code>status.removeInfo.issues</code> and <code>status.removeInfo.warnings</code> - error    and warning messages occurred during task processing, if any.</li> </ul> </li> <li> <p>Verify that the <code>CephOsdRemoveTask</code> has been completed.</p> <p>Example of the positive <code>status.phase</code> field:   <pre><code>status:\n  phase: Completed # or CompletedWithWarnings if there are non-critical issues\n</code></pre></p> </li> <li> <p>Remove the device cleanup jobs:     <pre><code>kubectl delete jobs -n pelagia -l app=pelagia-lcm-cleanup-disks\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/lcm/add-rm-ceph-osd-with-meta/#reconfigure-a-partition-of-a-ceph-osd-metadata-device","title":"Reconfigure a partition of a Ceph OSD metadata device","text":"<p>There is no hot reconfiguration procedure for existing Ceph OSDs. To reconfigure an existing Ceph node, remove and re-add a Ceph OSD with a metadata device. However, the automated LCM will clean up the logical volume without a removal, and it can be reused. For this reason, to reconfigure a partition of a Ceph OSD metadata device:</p> <ol> <li>Remove a Ceph OSD from the Ceph cluster as described in    Remove a Ceph OSD with a metadata device.</li> <li>Add the same Ceph OSD but with a modified configuration as described in    Add a Ceph OSD with a metadata device.</li> </ol>"},{"location":"ops-guide/lcm/add-rm-ceph-osd/","title":"Ceph OSD Lifecycle Management","text":""},{"location":"ops-guide/lcm/add-rm-ceph-osd/#add-remove-or-reconfigure-ceph-osds","title":"Add, remove, or reconfigure Ceph OSDs","text":"<p>Pelagia Lifecycle Management (LCM) Controller simplifies Ceph cluster management by automating LCM operations. This section describes how to add, remove, or reconfigure Ceph OSDs.</p>"},{"location":"ops-guide/lcm/add-rm-ceph-osd/#add-a-ceph-osd","title":"Add a Ceph OSD","text":"<ol> <li> <p>Manually prepare the required devices on the existing node.</p> </li> <li> <p>Optional. If you want to add a Ceph OSD on top of a raw device that already exists    on a node or is hot-plugged, add the required device using the following    guidelines:</p> <ul> <li>You can add a raw device to a node during node deployment.</li> <li>If a node supports adding devices without a node reboot, you can hot plug    a raw device to a node.</li> <li>If a node does not support adding devices without a node reboot, you can    hot plug a raw device during node shutdown.</li> </ul> </li> <li> <p>Open the <code>CephDeployment</code> custom resource (CR) for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>In one of the following sections, specify parameters for Ceph OSD:</p> <ul> <li><code>nodes.&lt;nodeName&gt;.devices</code></li> <li><code>nodes.&lt;nodeName&gt;.deviceFilter</code></li> <li><code>nodes.&lt;nodeName&gt;.devicePathFilter</code></li> </ul> <p>For description of parameters, see  CephDeployment: Nodes parameters.</p> <p>The example configuration of the <code>nodes</code> section with the new node:  <pre><code>nodes:\n- name: storage-worker-52\n  roles:\n  - mon\n  - mgr\n  devices:\n  - config: # existing item\n      deviceClass: hdd\n    fullPath: /dev/disk/by-id/scsi-SATA_HGST_HUS724040AL_PN1334PEHN18ZS\n  - config: # new item\n      deviceClass: hdd\n    fullPath: /dev/disk/by-id/scsi-0ATA_HGST_HUS724040AL_PN1334PEHN1VBC\n</code></pre></p> <p>Warning</p> <p>We highly recommend using the non-wwn <code>by-id</code> symlinks to specify storage devices in the <code>devices</code> list. For details, see Architecture: Addressing Ceph devices.</p> </li> <li> <p>Verify that the Ceph OSD on the specified node is successfully deployed. The    <code>CephDeploymentHealth</code> CR <code>status.healthReport.cephDaemons.cephDaemons</code> section should not contain any issues.    <pre><code>kubectl -n pelagia get cephdeploymenthealth -o yaml\n</code></pre></p> <p>For example:  <pre><code>status:\n  healthReport:\n    cephDaemons:\n      cephDaemons:\n        osd:\n          info:\n          - 3 osds, 3 up, 3 in\n          status: ok\n</code></pre></p> </li> <li> <p>Verify the desired Ceph OSD pod is <code>Running</code>:    <pre><code>kubectl -n rook-ceph get pod -l app=rook-ceph-osd -o wide | grep &lt;nodeName&gt;\n</code></pre></p> </li> </ol> <p></p>"},{"location":"ops-guide/lcm/add-rm-ceph-osd/#remove-a-ceph-osd","title":"Remove a Ceph OSD","text":"<p>Note</p> <p>Ceph OSD removal presupposes usage of a <code>CephOsdRemoveTask</code> CR. For workflow overview, see High-level workflow of Ceph OSD or node removal.</p> <p>Warning</p> <p>When using the non-recommended Ceph pools <code>replicated.size</code> of less than <code>3</code>, Ceph OSD removal cannot be performed. The minimal replica size equals a rounded up half of the specified <code>replicated.size</code>.</p> <p>For example, if <code>replicated.size</code> is <code>2</code>, the minimal replica size is <code>1</code>, and if <code>replicated.size</code> is <code>3</code>, then the minimal replica size is <code>2</code>. The replica size of <code>1</code> allows Ceph having PGs with only one Ceph OSD in the <code>acting</code> state, which may cause a <code>PG_TOO_DEGRADED</code> health warning that blocks Ceph OSD removal. We recommend setting <code>replicated.size</code> to <code>3</code> for each Ceph pool.</p> <ol> <li> <p>Open the <code>CephDeployment</code> CR on for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>Remove the required Ceph OSD specification from the    <code>spec.nodes.&lt;nodeName&gt;.devices</code> list:</p> <p>The example configuration of the <code>nodes</code> section with removing device:  <pre><code>nodes:\n- name: storage-worker-52\n  roles:\n  - mon\n  - mgr\n  devices:\n  - config:\n      deviceClass: hdd\n    fullPath: /dev/disk/by-id/scsi-SATA_HGST_HUS724040AL_PN1334PEHN18ZS\n  - config: # remove the entire item entry from devices list\n      deviceClass: hdd\n    fullPath: /dev/disk/by-id/scsi-0ATA_HGST_HUS724040AL_PN1334PEHN1VBC\n</code></pre></p> </li> <li> <p>Create a YAML template for the <code>CephOsdRemoveTask</code> CR. Select from the following options:</p> <ul> <li> <p>Remove Ceph OSD by device name, <code>by-path</code> symlink, or <code>by-id</code> symlink:    <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephOsdRemoveTask\nmetadata:\n  name: remove-osd-&lt;nodeName&gt;-task\n  namespace: pelagia\nspec:\n  nodes:\n    &lt;nodeName&gt;:\n      cleanupByDevice:\n      - device: sdb\n</code></pre></p> <p>Warning</p> <p>We do not recommend setting device name or device <code>by-path</code> symlink in the <code>cleanupByDevice</code> field  as these identifiers are not persistent and can change at node boot. Remove Ceph OSDs with <code>by-id</code>  symlinks or use <code>cleanupByOsdId</code> instead. For details, see  Architecture: Addressing Ceph devices.</p> <p>Note</p> <p>If a device was physically removed from a node, <code>cleanupByDevice</code> is not supported. Therefore, use <code>cleanupByOsdId</code> instead.</p> </li> <li> <p>Remove Ceph OSD by OSD ID:    <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephOsdRemoveTask\nmetadata:\n  name: remove-osd-&lt;nodeName&gt;-task\n  namespace: pelagia\nspec:\n  nodes:\n    &lt;nodeName&gt;:\n      cleanupByOsdId:\n      - id: 2\n</code></pre></p> </li> </ul> </li> <li> <p>Apply the template:    <pre><code>kubectl apply -f remove-osd-&lt;nodeName&gt;-task.yaml\n</code></pre></p> </li> <li> <p>Verify that the corresponding request has been created:    <pre><code>kubectl -n pelagia get cephosdremovetask remove-osd-&lt;nodeName&gt;-task\n</code></pre></p> </li> <li> <p>Verify that the <code>removeInfo</code> section appeared in the <code>CephOsdRemoveTask</code> CR <code>status</code>:    <pre><code>kubectl -n pelagia get cephosdremovetask remove-osd-&lt;nodeName&gt;-task -o yaml\n</code></pre></p> <p>Example of system response:  <pre><code>status:\n  removeInfo:\n    cleanupMap:\n      storage-worker-52:\n        osdMapping:\n          \"2\":\n            deviceMapping:\n              sdb:\n                path: \"/dev/disk/by-path/pci-0000:00:1t.9\"\n                partition: \"/dev/ceph-b-vg_sdb/osd-block-b-lv_sdb\"\n                type: \"block\"\n                class: \"hdd\"\n                zapDisk: true\n</code></pre></p> </li> <li> <p>Verify that the <code>cleanupMap</code> section matches the required removal and    wait for the <code>ApproveWaiting</code> phase to appear in <code>status</code>:    <pre><code>kubectl -n pelagia get cephosdremovetask remove-osd-&lt;nodeName&gt;-task -o yaml\n</code></pre></p> <p>Example of system response:  <pre><code>status:\n  phase: ApproveWaiting\n</code></pre></p> </li> <li> <p>Edit the <code>CephOsdRemoveTask</code> CR and set the <code>approve</code> flag to <code>true</code>:    <pre><code>kubectl -n pelagia edit cephosdremovetask remove-osd-&lt;nodeName&gt;-task\n</code></pre></p> <p>For example:  <pre><code>spec:\n  approve: true\n</code></pre></p> </li> <li> <p>Review the following <code>status</code> fields of the Ceph LCM CR processing:</p> <ul> <li><code>status.phase</code> - current state of request processing;</li> <li><code>status.messages</code> - description of the current phase;</li> <li><code>status.conditions</code> - full history of request processing before the current phase;</li> <li><code>status.removeInfo.issues</code> and <code>status.removeInfo.warnings</code> - error    and warning messages occurred during request processing, if any.</li> </ul> </li> <li> <p>Verify that the <code>CephOsdRemoveTask</code> has been completed. For example:     <pre><code>status:\n  phase: Completed # or CompletedWithWarnings if there are non-critical issues\n</code></pre></p> </li> <li> <p>Remove the device cleanup jobs:     <pre><code>kubectl delete jobs -n pelagia -l app=pelagia-lcm-cleanup-disks\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/lcm/add-rm-ceph-osd/#reconfigure-a-ceph-osd","title":"Reconfigure a Ceph OSD","text":"<p>There is no hot reconfiguration procedure for existing Ceph OSDs. To reconfigure an existing Ceph OSD, follow the steps below:</p> <ol> <li>Remove a Ceph OSD from the Ceph cluster as described in    Remove a Ceph OSD.</li> <li>Add the same Ceph OSD but with a modified configuration as described in    Add a Ceph OSD.</li> </ol>"},{"location":"ops-guide/lcm/create-task-workflow/","title":"Create Ceph OSD remove Task","text":""},{"location":"ops-guide/lcm/create-task-workflow/#creating-a-ceph-osd-remove-task","title":"Creating a Ceph OSD remove task","text":"<p>The workflow of creating a Ceph OSD removal task includes the following steps:</p> <ol> <li> <p>Removing obsolete nodes or disks from the <code>spec.nodes</code> section of the    <code>CephDeployment</code> custom resource (CR) as described in    Architecture: CephDeployment nodes parameters.</p> <p>Note</p> <p>Note the names of the removed nodes, devices or their paths exactly as they were specified   in <code>CephDeployment</code> for further usage.</p> </li> <li> <p>Creating a YAML template for the <code>CephOsdRemoveTask</code> CR. For    details, see    Architecture: CephOsdRemoveTask.</p> <ul> <li>If <code>CephOsdRemoveTask</code> contains information about Ceph OSDs to remove in a proper format,    the information will be validated to eliminate human error and avoid a wrong Ceph OSD removal.</li> <li>If the <code>nodes</code> section of <code>CephOsdRemoveTask</code> is empty, the Pelagia LCM Controller will automatically    detect Ceph OSDs for removal, if any. Auto-detection is based not only on the information    provided in the Rook <code>CephCluster</code> CR but also on the information from the Ceph cluster itself.</li> </ul> <p>Once the validation or auto-detection completes, the entire information about the Ceph OSDs to remove appears  in the <code>CephOsdRemoveTask</code> object: hosts they belong to, OSD IDs, disks, partitions, and so on. The  request then moves to the <code>ApproveWaiting</code> phase until the cloud operator manually specifies the <code>approve</code>  flag in the spec.</p> </li> <li> <p>Manually adding an affirmative <code>approve</code> flag in the <code>CephOsdRemoveTask</code> spec. Once done, Pelagia Controllers and    Rook Ceph Operator reconciliation pause until the task is handled and execute the following:</p> <ul> <li>Stops regular Rook Ceph Operator orchestration. Also, Pelagia Deployment Controller pauses its reconcile.</li> <li>Removes Ceph OSDs.</li> <li>Runs batch jobs to clean up the device, if possible.</li> <li>Removes host information from the Ceph cluster if the entire Ceph node is removed.</li> <li>Marks the task with an appropriate result with a description of occurred issues.</li> </ul> <p>Note</p> <p>If the task completes successfully, Rook Ceph Operator and Pelagia Deployment Controller reconciliation  resumes. Otherwise, it remains paused until the issue is resolved.</p> </li> <li> <p>Reviewing the Ceph OSD removal status. For details, see    Architecture: CephOsdRemoveTask status.</p> </li> <li> <p>Manual removal of device cleanup jobs.</p> <p>Note</p> <p>Device cleanup jobs are not removed automatically and are kept in Pelagia namespace along with pods containing  information about the executed actions. The jobs have the following labels:  <pre><code>labels:\n  app: pelagia-lcm-cleanup-disks\n  host: &lt;HOST-NAME&gt;\n  osd: &lt;OSD-ID&gt;\n  rook-cluster: &lt;ROOK-CLUSTER-NAME&gt;\n</code></pre></p> <p>Additionally, jobs are labeled with disk names that will be cleaned up, such as <code>sdb=true</code>.  You can remove a single job or a group of jobs using any label described above, such as host, disk, and so on.</p> </li> </ol>"},{"location":"ops-guide/lcm/create-task-workflow/#example-of-cephosdremovetask-resource","title":"Example of <code>CephOsdRemoveTask</code> resource","text":"<pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephOsdRemoveTask\nmetadata:\n  name: remove-osd-3-4-task\n  namespace: pelagia\nspec:\n  nodes:\n    worker-3:\n      cleanupByDevice:\n      - device: sdb\n      - device: /dev/disk/by-path/pci-0000:00:1t.9\n</code></pre>"},{"location":"ops-guide/lcm/create-task-workflow/#cephosdremovetask-to-find-all-ready-to-remove-ceph-osds","title":"<code>CephOsdRemoveTask</code> to find all ready to remove Ceph OSDs","text":"<pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephOsdRemoveTask\nmetadata:\n  generateName: remove-osds\n  namespace: pelagia\nspec:\n  nodes: {}\n</code></pre>"},{"location":"ops-guide/lcm/create-task-workflow/#see-also","title":"SEE ALSO","text":"<p>CephOsdRemoveRequest failure with a timeout during rebalance</p>"},{"location":"ops-guide/lcm/increase-storage-size/","title":"Increase Ceph Cluster storage size","text":""},{"location":"ops-guide/lcm/increase-storage-size/#increase-ceph-cluster-storage-size","title":"Increase Ceph cluster storage size","text":"<p>This section describes how to increase the overall storage size for all Ceph pools of the same device class: <code>hdd</code>, <code>ssd</code>, or <code>nvme</code>. The procedure presupposes adding a new Ceph OSD. The overall storage size for the required device class automatically increases once the Ceph OSD becomes available in the Ceph cluster.</p>"},{"location":"ops-guide/lcm/increase-storage-size/#increase-the-overall-storage-size-for-a-device-class","title":"Increase the overall storage size for a device class","text":"<ol> <li> <p>Identify the current storage size for the required device class:    <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- ceph df\n</code></pre></p> <p>Example of system response:  <pre><code>--- RAW STORAGE ---\nCLASS  SIZE     AVAIL    USED    RAW USED  %RAW USED\nhdd    128 GiB  101 GiB  23 GiB    27 GiB      21.40\nTOTAL  128 GiB  101 GiB  23 GiB    27 GiB      21.40\n\n--- POOLS ---\nPOOL                   ID  PGS  STORED  OBJECTS  USED    %USED  MAX AVAIL\ndevice_health_metrics   1    1     0 B        0     0 B      0     30 GiB\nkubernetes-hdd          2   32  12 GiB    3.13k  23 GiB  20.57     45 GiB\n</code></pre></p> </li> <li> <p>Identify the number of Ceph OSDs with the required device class:    <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- ceph osd df &lt;deviceClass&gt;\n</code></pre></p> <p>Substitute <code>&lt;deviceClass&gt;</code> with the required device class: <code>hdd</code>, <code>ssd</code>, or <code>nvme</code>.</p> <p>Example of system response for the <code>hdd</code> device class:  <pre><code>ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP      META      AVAIL    %USE   VAR   PGS  STATUS\n 1    hdd  0.03119   1.00000   32 GiB  5.8 GiB  4.8 GiB   1.5 MiB  1023 MiB   26 GiB  18.22  0.85   14      up\n 3    hdd  0.03119   1.00000   32 GiB  6.9 GiB  5.9 GiB   1.1 MiB  1023 MiB   25 GiB  21.64  1.01   17      up\n 0    hdd  0.03119   0.84999   32 GiB  6.8 GiB  5.8 GiB  1013 KiB  1023 MiB   25 GiB  21.24  0.99   16      up\n 2    hdd  0.03119   1.00000   32 GiB  7.9 GiB  6.9 GiB   1.2 MiB  1023 MiB   24 GiB  24.55  1.15   20      up\n                       TOTAL  128 GiB   27 GiB   23 GiB   4.8 MiB   4.0 GiB  101 GiB  21.41\nMIN/MAX VAR: 0.85/1.15  STDDEV: 2.29\n</code></pre></p> </li> <li> <p>Follow Lifecycle management: Add a Ceph OSD    to add a new device with a supported device class: <code>hdd</code>, <code>ssd</code>, or <code>nvme</code>.</p> </li> <li> <p>Wait for the new Ceph OSD pod to start <code>Running</code>:    <pre><code>kubectl -n rook-ceph get pod -l app=rook-ceph-osd\n</code></pre></p> </li> <li> <p>Verify that the new Ceph OSD has rebalanced and Ceph health is <code>HEALTH_OK</code>:    <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- ceph -s\n</code></pre></p> </li> <li> <p>Verify that the new Ceph has been OSD added to the list of device class OSDs:    <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- ceph osd df &lt;deviceClass&gt;\n</code></pre></p> <p>Substitute <code>&lt;deviceClass&gt;</code> with the required device class: <code>hdd</code>, <code>ssd</code>, or <code>nvme</code></p> <p>Example of system response for the <code>hdd</code> device class after adding a new Ceph OSD:  <pre><code>ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP      META      AVAIL    %USE   VAR   PGS  STATUS\n 1    hdd  0.03119   1.00000   32 GiB  4.5 GiB  3.5 GiB   1.5 MiB  1023 MiB   28 GiB  13.93  0.78   10      up\n 3    hdd  0.03119   1.00000   32 GiB  5.5 GiB  4.5 GiB   1.1 MiB  1023 MiB   26 GiB  17.22  0.96   13      up\n 0    hdd  0.03119   0.84999   32 GiB  6.5 GiB  5.5 GiB  1013 KiB  1023 MiB   25 GiB  20.32  1.14   15      up\n 2    hdd  0.03119   1.00000   32 GiB  7.5 GiB  6.5 GiB   1.2 MiB  1023 MiB   24 GiB  23.43  1.31   19      up\n 4    hdd  0.03119   1.00000   32 GiB  4.6 GiB  3.6 GiB       0 B     1 GiB   27 GiB  14.45  0.81   10      up\n                       TOTAL  160 GiB   29 GiB   24 GiB   4.8 MiB   5.0 GiB  131 GiB  17.87\nMIN/MAX VAR: 0.78/1.31  STDDEV: 3.62\n</code></pre></p> </li> <li> <p>Verify the total storage capacity increased for the entire device class:    <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- ceph df\n</code></pre></p> <p>Example of system response:  <pre><code>--- RAW STORAGE ---\nCLASS  SIZE     AVAIL    USED    RAW USED  %RAW USED\nhdd    160 GiB  131 GiB  24 GiB    29 GiB      17.97\nTOTAL  160 GiB  131 GiB  24 GiB    29 GiB      17.97\n\n--- POOLS ---\nPOOL                   ID  PGS  STORED  OBJECTS  USED    %USED  MAX AVAIL\ndevice_health_metrics   1    1     0 B        0     0 B      0     38 GiB\nkubernetes-hdd          2   32  12 GiB    3.18k  24 GiB  17.17     57 GiB\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/lcm/manual-osd-remove/","title":"Remove Ceph OSD manually","text":""},{"location":"ops-guide/lcm/manual-osd-remove/#remove-ceph-osd-manually","title":"Remove Ceph OSD manually","text":"<p>You may need to manually remove a Ceph OSD, for example, in the following cases:</p> <ul> <li>If you accidentally have removed an OSD with ceph CLI or <code>rook-ceph-osd</code>   deployment.</li> <li>If you do not want to rely on Pelagia LCM operations and want to manage the Ceph   OSDs lifecycle manually.</li> </ul> <p>To safely remove one or multiple Ceph OSDs from a Ceph cluster, perform the following procedure for each Ceph OSD one by one.</p> <p>Warning</p> <p>The procedure presupposes the Ceph OSD disk or logical volumes partition cleanup.</p>"},{"location":"ops-guide/lcm/manual-osd-remove/#remove-a-ceph-osd-manually","title":"Remove a Ceph OSD manually","text":"<ol> <li> <p>Open the <code>CephDeployment</code> custom resource (CR) for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>In the <code>spec.nodes</code> section, remove the required <code>devices</code> item of the corresponding node spec.    When using <code>deviceFilter</code> or <code>devicePathFilter</code>, update regexp accordingly.</p> <p>If after removal <code>devices</code>, <code>deviceFilter</code>, or <code>devicePathFilter</code>  become empty and the node spec has no roles specified, also remove the node  spec.</p> </li> <li> <p>Verify that all Ceph OSDs are <code>up</code> and <code>in</code>, the Ceph cluster is    healthy, and no rebalance or recovery is in progress:    <pre><code>kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph -s\n</code></pre></p> <p>Example of system response:  <pre><code>cluster:\n  id:     8cff5307-e15e-4f3d-96d5-39d3b90423e4\n  health: HEALTH_OK\n  ...\n  osd: 4 osds: 4 up (since 10h), 4 in (since 10h)\n</code></pre></p> </li> <li> <p>Stop all deployments in Pelagia namespace to prevent autoscaling <code>rook-ceph-operator</code> deployment to 1 replica:    <pre><code>kubectl -n pelagia scale deploy --all --replicas 0\n</code></pre></p> </li> <li> <p>Stop the Rook Ceph Operator deployment to avoid premature re-orchestration of the Ceph cluster:    <pre><code>kubectl -n rook-ceph scale deploy rook-ceph-operator --replicas 0\n</code></pre></p> </li> <li> <p>Enter the <code>pelagia-ceph-toolbox</code> pod:    <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- bash\n</code></pre></p> </li> <li> <p>Mark the required Ceph OSD as <code>out</code>:    <pre><code>ceph osd out osd.&lt;ID&gt;\n</code></pre></p> <p>Note</p> <p>In the command above and in the steps below, substitute <code>&lt;ID&gt;</code>  with the number of the Ceph OSD to remove.</p> </li> <li> <p>Wait until data backfilling to other OSDs is complete:    <pre><code>ceph -s\n</code></pre></p> <p>Once all the PGs are <code>active+clean</code>, backfilling is complete, and it is  safe to remove the disk.</p> <p>Note</p> <p>For additional information on PGs backfilling, run <code>ceph pg dump_stuck</code>.</p> </li> <li> <p>Exit from the <code>pelagia-ceph-toolbox</code> pod:    <pre><code>exit\n</code></pre></p> </li> <li> <p>Scale the <code>rook-ceph/rook-ceph-osd-&lt;ID&gt;</code> deployment to <code>0</code> replicas:     <pre><code>kubectl -n rook-ceph scale deploy rook-ceph-osd-&lt;ID&gt; --replicas 0\n</code></pre></p> </li> <li> <p>Enter the <code>pelagia-ceph-toolbox</code> pod:     <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- bash\n</code></pre></p> </li> <li> <p>Verify that the number of Ceph OSDs that are <code>up</code> and <code>in</code> has decreased     by one daemon:     <pre><code>ceph -s\n</code></pre></p> <p>Example of system response:   <pre><code>osd: 4 osds: 3 up (since 1h), 3 in (since 5s)\n</code></pre></p> </li> <li> <p>Remove the Ceph OSD from the Ceph cluster:     <pre><code>ceph osd purge &lt;ID&gt; --yes-i-really-mean-it\n</code></pre></p> </li> <li> <p>Delete the Ceph OSD <code>auth</code> entry, if present. Otherwise, skip this step.     <pre><code>ceph auth del osd.&lt;ID&gt;\n</code></pre></p> </li> <li> <p>If you have removed the last Ceph OSD on the node and want to remove this     node from the Ceph cluster, remove the CRUSH map entry:     <pre><code>ceph osd crush remove &lt;nodeName&gt;\n</code></pre></p> <p>Substitute <code>&lt;nodeName&gt;</code> with the name of the node where the removed Ceph   OSD was placed.</p> </li> <li> <p>Verify that the failure domain within Ceph OSDs has been removed from the     CRUSH map:     <pre><code>ceph osd tree\n</code></pre></p> <p>If you have removed the node, it will be removed from the CRUSH map.</p> </li> <li> <p>Exit from the <code>pelagia-ceph-toolbox</code> pod:     <pre><code>exit\n</code></pre></p> </li> <li> <p>Clean up the disk used by the removed Ceph OSD. For details, see official     Rook: Zapping Devices.</p> <p>Warning</p> <p>If you are using multiple Ceph OSDs per device or metadata  device, make sure that you can clean up the entire disk. Otherwise,  instead clean up only the logical volume partitions for the volume group  by running <code>lvremove &lt;lvpartion_uuid&gt;</code> any Ceph OSD pod that  belongs to the same host as the removed Ceph OSD.</p> </li> <li> <p>Delete the <code>rook-ceph/rook-ceph-osd-&lt;ID&gt;</code> deployment previously scaled to     <code>0</code> replicas:     <pre><code>kubectl -n rook-ceph delete deploy rook-ceph-osd-&lt;ID&gt;\n</code></pre></p> <p>Substitute <code>&lt;ID&gt;</code> with the number of the removed Ceph OSD.</p> </li> <li> <p>Scale the <code>rook-ceph/rook-ceph-operator</code> deployment to <code>1</code> replica and     wait for the orchestration to complete:     <pre><code>kubectl -n rook-ceph scale deploy rook-ceph-operator --replicas 1\nkubectl -n rook-ceph get pod -w\n</code></pre></p> </li> <li> <p>Scale all deployments in Pelagia namespace to continue spec reconcile and regular work:     <pre><code>kubectl -n pelagia scale deploy --all --replicas 3\n</code></pre></p> <p>Once done, Ceph OSD removal is complete.</p> </li> </ol>"},{"location":"ops-guide/lcm/replace-ceph-node/","title":"Replace failed Ceph Node","text":""},{"location":"ops-guide/lcm/replace-ceph-node/#replace-a-failed-ceph-node","title":"Replace a failed Ceph node","text":"<p>After a physical node replacement, you can use the Pelagia LCM API to redeploy failed Ceph nodes. The common flow of replacing a failed Ceph node is as follows:</p> <ol> <li>Remove the obsolete Ceph node from the Ceph cluster.</li> <li>Add a new Ceph node with the same configuration to the Ceph cluster.</li> </ol> <p>Note</p> <p>Ceph node removal presupposes usage of a <code>CephOsdRemoveTask</code> CR. For workflow overview, see High-level workflow of Ceph OSD or node removal.</p>"},{"location":"ops-guide/lcm/replace-ceph-node/#remove-a-failed-ceph-node","title":"Remove a failed Ceph node","text":"<ol> <li> <p>Open the <code>CephDeployment</code> CR for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>In the <code>nodes</code> section, remove the required device. When using device    filters, update regexp accordingly.</p> <p>For example:  <pre><code>spec:\n  nodes:\n  - name: &lt;nodeName&gt; # remove the entire entry for the node to replace\n    devices: {...}\n    role: [...]\n</code></pre></p> <p>Substitute <code>&lt;nodeName&gt;</code> with the node name to replace.</p> </li> <li> <p>Save <code>CephDeployment</code> changes and close the editor.</p> </li> <li> <p>Create a <code>CephOsdRemoveTask</code> CR template and save it as <code>replace-failed-&lt;nodeName&gt;-task.yaml</code>:    <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephOsdRemoveTask\nmetadata:\n  name: replace-failed-&lt;nodeName&gt;-task\n  namespace: pelagia\nspec:\n  nodes:\n    &lt;nodeName&gt;:\n      completeCleanUp: true\n</code></pre></p> </li> <li> <p>Apply the template to the cluster:    <pre><code>kubectl apply -f replace-failed-&lt;nodeName&gt;-task.yaml\n</code></pre></p> </li> <li> <p>Verify that the corresponding task has been created:    <pre><code>kubectl -n pelagia get cephosdremovetask\n</code></pre></p> </li> <li> <p>Verify that the <code>removeInfo</code> section appeared in the <code>CephOsdRemoveTask</code> CR <code>status</code>:    <pre><code>kubectl -n pelagia get cephosdremovetask replace-failed-&lt;nodeName&gt;-task -o yaml\n</code></pre></p> <p>Example of system response:  <pre><code>removeInfo:\n  cleanupMap:\n    &lt;nodeName&gt;:\n      osdMapping:\n        ...\n        &lt;osdId&gt;:\n          deviceMapping:\n            ...\n            &lt;deviceName&gt;:\n              path: &lt;deviceByPath&gt;\n              partition: \"/dev/ceph-b-vg_sdb/osd-block-b-lv_sdb\"\n              type: \"block\"\n              class: \"hdd\"\n              zapDisk: true\n</code></pre></p> <p>Definition of values in angle brackets:</p> <ul> <li><code>&lt;nodeName&gt;</code> - underlying machine node name, for example,    <code>storage-worker-5</code>.</li> <li><code>&lt;osdId&gt;</code> - actual Ceph OSD ID for the device being replaced, for    example, <code>1</code>.</li> <li><code>&lt;deviceName&gt;</code> - actual device name placed on the node, for    example, <code>sdb</code>.</li> <li><code>&lt;deviceByPath&gt;</code> - actual device <code>by-path</code> placed on the node, for    example, <code>/dev/disk/by-path/pci-0000:00:1t.9</code>.</li> </ul> </li> <li> <p>Verify that the <code>cleanupMap</code> section matches the required removal and wait    for the <code>ApproveWaiting</code> phase to appear in <code>status</code>:    <pre><code>kubectl -n pelagia get cephosdremovetask replace-failed-&lt;nodeName&gt;-task -o yaml\n</code></pre></p> <p>Example of system response:  <pre><code>status:\n  phase: ApproveWaiting\n</code></pre></p> </li> <li> <p>Edit the <code>CephOsdRemoveTask</code> CR and set the <code>approve</code> flag to <code>true</code>:    <pre><code>kubectl -n pelagia edit cephosdremovetask replace-failed-&lt;nodeName&gt;-task\n</code></pre></p> <p>For example:  <pre><code>spec:\n  approve: true\n</code></pre></p> </li> <li> <p>Review the following <code>status</code> fields of the Ceph LCM CR processing:</p> <ul> <li><code>status.phase</code> - current state of task processing;</li> <li><code>status.messages</code> - description of the current phase;</li> <li><code>status.conditions</code> - full history of task processing before the     current phase;</li> <li><code>status.removeInfo.issues</code> and <code>status.removeInfo.warnings</code> - error     and warning messages occurred during task processing, if any.</li> </ul> </li> <li> <p>Verify that the <code>CephOsdRemoveTask</code> has been completed.     For example:     <pre><code>status:\n  phase: Completed # or CompletedWithWarnings if there are non-critical issues\n</code></pre></p> </li> <li> <p>Remove the device cleanup jobs:     <pre><code>kubectl delete jobs -n pelagia -l app=pelagia-lcm-cleanup-disks\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/lcm/replace-ceph-node/#deploy-a-new-ceph-node-after-removal-of-a-failed-one","title":"Deploy a new Ceph node after removal of a failed one","text":"<p>Note</p> <p>You can spawn Ceph OSD on a raw device, but it must be clean and without any data or partitions. If you want to add a device that was in use, also ensure it is raw and clean. To clean up all data and partitions from a device, refer to official Rook documentation.</p> <ol> <li> <p>Open the <code>CephDeployment</code> CR for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>In the <code>nodes</code> section, add a new device:    <pre><code>spec:\n  nodes:\n  - name: &lt;nodeName&gt; # add new configuration for replaced Ceph node\n    devices:\n    - fullPath: &lt;deviceByID&gt; # Recommended. Non-wwn by-id symlink.\n      # name: &lt;deviceByID&gt; # Not recommended. If a device is supposed to be added with by-id.\n      # fullPath: &lt;deviceByPath&gt; # if device is supposed to be added with by-path.\n      config:\n        deviceClass: hdd\n      ...\n</code></pre></p> <p>Substitute <code>&lt;nodeName&gt;</code> with the replaced node name and configure it as required.</p> <p>Warning</p> <p>We highly recommend using the non-wwn <code>by-id</code> symlinks to specify storage devices in the <code>devices</code> list. For details, see Architecture: Addressing Ceph devices.</p> </li> <li> <p>Verify that all Ceph daemons from the replaced node have appeared on the    Ceph cluster and are <code>in</code> and <code>up</code>. The <code>healthReport</code> section of <code>CephDeploymentHealth</code> CR    should not contain any issues.    <pre><code>kubectl -n pelagia get cephdeploymenthealth -o yaml\n</code></pre></p> <p>Example of system response:  <pre><code>status:\n  healthReport:\n    rookCephObjects:\n      cephCluster:\n        ceph:\n          health: HEALTH_OK\n          ...\n    cephDaemons:\n      cephDaemons:\n        mgr:\n          info:\n          - 'a is active mgr, standbys: [b]'\n          status: ok\n        mon:\n          info:\n          - 3 mons, quorum [a b c]\n          status: ok\n        osd:\n          info:\n          - 3 osds, 3 up, 3 in\n          status: ok\n</code></pre></p> </li> <li> <p>Verify the Ceph node in the Rook namespace:    <pre><code>kubectl -n rook-ceph get pod -o wide | grep &lt;nodeName&gt;\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/lcm/replace-ceph-osd/","title":"Replace failed Ceph OSD","text":""},{"location":"ops-guide/lcm/replace-ceph-osd/#replace-a-failed-ceph-osd","title":"Replace a failed Ceph OSD","text":"<p>After a physical disk replacement, you can use Pelagia Lifecycle Management (LCM) API to redeploy a failed Ceph OSD. The common flow of replacing a failed Ceph OSD is as follows:</p> <ol> <li>Remove the obsolete Ceph OSD from the Ceph cluster by device symlink, by device name, or by Ceph    OSD ID.</li> <li>Add a new Ceph OSD on the new disk to the Ceph cluster.</li> </ol> <p>Note</p> <p>Ceph OSD removal presupposes usage of a <code>CephOsdRemoveTask</code> CR. For workflow overview, see High-level workflow of Ceph OSD or node removal.</p>"},{"location":"ops-guide/lcm/replace-ceph-osd/#remove-a-failed-ceph-osd-by-device-name-path-or-id","title":"Remove a failed Ceph OSD by device name, path, or ID","text":"<p>Warning</p> <p>The procedure below presupposes that the cloud operator knows the exact device name, <code>by-path</code>, or <code>by-id</code> of the replaced device, as well as on which node the replacement occurred.</p> <p>Warning</p> <p>A Ceph OSD removal using <code>by-path</code>, <code>by-id</code>, or device name is not supported if a device was physically removed from a node. Therefore, use <code>cleanupByOsdId</code> instead. For details, see Remove a failed Ceph OSD by Ceph OSD ID.</p> <p>Warning</p> <p>We do not recommend setting device name or device <code>by-path</code> symlink in the <code>cleanupByDevice</code> field  as these identifiers are not persistent and can change at node boot. Remove Ceph OSDs with <code>by-id</code>  symlinks or use <code>cleanupByOsdId</code> instead. For details, see  Architecture: Addressing Ceph devices.</p> <ol> <li> <p>Open the <code>CephDeployment</code> CR for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>In the <code>nodes</code> section, remove the required device from <code>devices</code>. When    using device filters, update the <code>deviceFilter</code> or <code>devicePathFilter</code>    regexp accordingly.</p> <p>For example:  <pre><code>spec:\n  nodes:\n  - name: &lt;nodeName&gt;\n    devices:\n    - name: &lt;deviceName&gt;  # remove the entire item from devices list\n      # fullPath: &lt;deviceByPath&gt; if device is specified with symlink instead of name\n      config:\n        deviceClass: hdd\n</code></pre></p> <p>Substitute <code>&lt;nodeName&gt;</code> with the node name where the device  <code>&lt;deviceName&gt;</code> or <code>&lt;deviceByPath&gt;</code> is going to be replaced.</p> </li> <li> <p>Save <code>CephDeployment</code> changes and close the editor.</p> </li> <li> <p>Create a <code>CephOsdRemoveTask</code> CR template and save it as <code>replace-failed-osd-&lt;nodeName&gt;-&lt;deviceName&gt;-task.yaml</code>:    <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephOsdRemoveTask\nmetadata:\n  name: replace-failed-osd-&lt;nodeName&gt;-&lt;deviceName&gt;\n  namespace: pelagia\nspec:\n  nodes:\n    &lt;nodeName&gt;:\n      cleanupByDevice:\n      - name: &lt;deviceName&gt;\n        # If a device is specified with by-path or by-id instead of\n        # name, path: &lt;deviceByPath&gt; or &lt;deviceById&gt;.\n</code></pre></p> </li> <li> <p>Apply the template to the cluster:    <pre><code>kubectl apply -f replace-failed-osd-&lt;nodeName&gt;-&lt;deviceName&gt;-task.yaml\n</code></pre></p> </li> <li> <p>Verify that the corresponding request has been created:    <pre><code>kubectl -n pelagia get cephosdremovetask\n</code></pre></p> </li> <li> <p>Verify that the <code>removeInfo</code> section appeared in the <code>CephOsdRemoveTask</code> CR <code>status</code>:    <pre><code>kubectl -n pelagia get cephosdremovetask replace-failed-osd-&lt;nodeName&gt;-&lt;deviceName&gt; -o yaml\n</code></pre></p> <p>Example of system response:  <pre><code>status:\n  osdRemoveStatus:\n    removeInfo:\n      cleanupMap:\n        &lt;nodeName&gt;:\n          osdMapping:\n            &lt;osdId&gt;:\n              deviceMapping:\n                &lt;dataDevice&gt;:\n                  deviceClass: hdd\n                  devicePath: &lt;dataDeviceByPath&gt;\n                  devicePurpose: block\n                  usedPartition: /dev/ceph-d2d3a759-2c22-4304-b890-a2d87e056bd4/osd-block-ef516477-d2da-492f-8169-a3ebfc3417e2\n                  zapDisk: true\n</code></pre></p> <p>Definition of values in angle brackets:</p> <ul> <li><code>&lt;nodeName&gt;</code> - underlying node name of the machine, for example,    <code>storage-worker-52</code>;</li> <li><code>&lt;osdId&gt;</code> - Ceph OSD ID for the device being replaced, for example,    <code>1</code>;</li> <li><code>&lt;dataDeviceByPath&gt;</code> - <code>by-path</code> of the device placed on the node,    for example, <code>/dev/disk/by-path/pci-0000:00:1t.9</code>;</li> <li><code>&lt;dataDevice&gt;</code> - name of the device placed on the node, for example,    <code>/dev/sdb</code>.</li> </ul> </li> <li> <p>Verify that the <code>cleanupMap</code> section matches the required removal and wait    for the <code>ApproveWaiting</code> phase to appear in <code>status</code>:    <pre><code>kubectl -n pelagia get cephosdremovetask replace-failed-osd-&lt;nodeName&gt;-&lt;deviceName&gt; -o yaml\n</code></pre></p> <p>Example of system response:  <pre><code>status:\n  phase: ApproveWaiting\n</code></pre></p> </li> <li> <p>Edit the <code>CephOsdRemoveTask</code> CR and set the <code>approve</code> flag to <code>true</code>:    <pre><code>kubectl -n pelagia edit cephosdremovetask replace-failed-osd-&lt;nodeName&gt;-&lt;deviceName&gt;\n</code></pre></p> <p>For example:  <pre><code>spec:\n  approve: true\n</code></pre></p> </li> <li> <p>Review the following <code>status</code> fields of the Ceph LCM CR processing:</p> <ul> <li><code>status.phase</code> - current state of request processing;</li> <li><code>status.messages</code> - description of the current phase;</li> <li><code>status.conditions</code> - full history of request processing before the   current phase;</li> <li><code>status.removeInfo.issues</code> and <code>status.removeInfo.warnings</code> - error   and warning messages occurred during request processing, if any.</li> </ul> </li> <li> <p>Verify that the <code>CephOsdRemoveTask</code> has been completed. For example:     <pre><code>status:\n  phase: Completed # or CompletedWithWarnings if there are non-critical issues\n</code></pre></p> </li> <li> <p>Remove the device cleanup jobs:     <pre><code>kubectl delete jobs -n pelagia -l app=pelagia-lcm-cleanup-disks\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/lcm/replace-ceph-osd/#remove-a-failed-ceph-osd-by-ceph-osd-id","title":"Remove a failed Ceph OSD by Ceph OSD ID","text":"<ol> <li> <p>Identify the node and device names used by the affected Ceph OSD. Using the    Ceph CLI in the <code>pelagia-ceph-toolbox</code> Pod, run:    <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- ceph osd metadata &lt;osdId&gt;\n</code></pre></p> <p>Substitute <code>&lt;osdId&gt;</code> with the affected OSD ID.</p> <p>Example output:  <pre><code>{\n  \"id\": 1,\n  ...\n  \"bluefs_db_devices\": \"vdc\",\n  ...\n  \"bluestore_bdev_devices\": \"vde\",\n  ...\n  \"devices\": \"vdc,vde\",\n  ...\n  \"hostname\": \"kaas-node-6c5e76f9-c2d2-4b1a-b047-3c299913a4bf\",\n  ...\n},\n</code></pre></p> <p>In the example above, <code>hostname</code> is the node name and <code>devices</code> are  all devices used by the affected Ceph OSD.</p> </li> <li> <p>Open the <code>CephDeployment</code> CR for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>In the <code>nodes</code> section, remove the required device:    <pre><code>spec:\n  nodes:\n  - name: &lt;nodeName&gt;\n    devices:\n    - name: &lt;deviceName&gt;  # remove the entire item from devices list\n      config:\n        deviceClass: hdd\n</code></pre></p> <p>Substitute <code>&lt;nodeName&gt;</code> with the node name where the device <code>&lt;deviceName&gt;</code> is going to be replaced.</p> </li> <li> <p>Save <code>CephDeployment</code> changes and close the editor.</p> </li> <li> <p>Create a <code>CephOsdRemoveTask</code> CR template and save it as <code>replace-failed-&lt;nodeName&gt;-osd-&lt;osdId&gt;-task.yaml</code>:    <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephOsdRemoveTask\nmetadata:\n  name: replace-failed-&lt;nodeName&gt;-osd-&lt;osdId&gt;\n  namespace: pelagia\nspec:\n  nodes:\n    &lt;nodeName&gt;:\n      cleanupByOsdId:\n      - id: &lt;osdId&gt;\n</code></pre></p> </li> <li> <p>Apply the template to the cluster:    <pre><code>kubectl apply -f replace-failed-&lt;nodeName&gt;-osd-&lt;osdId&gt;-task.yaml\n</code></pre></p> </li> <li> <p>Verify that the corresponding request has been created:    <pre><code>kubectl -n pelagia get cephosdremovetask\n</code></pre></p> </li> <li> <p>Verify that the <code>removeInfo</code> section appeared in the <code>CephOsdRemoveTask</code> CR <code>status</code>:    <pre><code>kubectl -n pelagia get cephosdremovetask replace-failed-&lt;nodeName&gt;-osd-&lt;osdId&gt;-task -o yaml\n</code></pre></p> <p>Example of system response  <pre><code>status:\n  osdRemoveStatus:\n    removeInfo:\n      cleanupMap:\n        &lt;nodeName&gt;:\n          osdMapping:\n            &lt;osdId&gt;:\n              deviceMapping:\n                &lt;dataDevice&gt;:\n                  deviceClass: hdd\n                  devicePath: &lt;dataDeviceByPath&gt;\n                  devicePurpose: block\n                  usedPartition: /dev/ceph-d2d3a759-2c22-4304-b890-a2d87e056bd4/osd-block-ef516477-d2da-492f-8169-a3ebfc3417e2\n                  zapDisk: true\n</code></pre></p> <p>Definition of values in angle brackets:</p> <ul> <li><code>&lt;nodeName&gt;</code> - underlying node name of the machine, for example,    <code>storage-worker-52</code>;</li> <li><code>&lt;osdId&gt;</code> - Ceph OSD ID for the device being replaced, for example,    <code>1</code>;</li> <li><code>&lt;dataDeviceByPath&gt;</code> - <code>by-path</code> of the device placed on the node,    for example, <code>/dev/disk/by-path/pci-0000:00:1t.9</code>;</li> <li><code>&lt;dataDevice&gt;</code> - name of the device placed on the node, for example,    <code>/dev/sdb</code>.</li> </ul> </li> <li> <p>Verify that the <code>cleanupMap</code> section matches the required removal and wait    for the <code>ApproveWaiting</code> phase to appear in <code>status</code>:    <pre><code>kubectl -n pelagia get cephosdremovetask replace-failed-&lt;nodeName&gt;-osd-&lt;osdId&gt;-task -o yaml\n</code></pre></p> <p>Example of system response:  <pre><code>status:\n  phase: ApproveWaiting\n</code></pre></p> </li> <li> <p>Edit the <code>CephOsdRemoveTask</code> CR and set the <code>approve</code> flag to <code>true</code>:     <pre><code>kubectl -n pelagia edit cephosdremovetask replace-failed-&lt;nodeName&gt;-osd-&lt;osdId&gt;-request\n</code></pre></p> <p>For example:   <pre><code>spec:\n  approve: true\n</code></pre></p> </li> <li> <p>Review the following <code>status</code> fields of the Ceph LCM CR processing:</p> <ul> <li><code>status.phase</code> - current state of request processing;</li> <li><code>status.messages</code> - description of the current phase;</li> <li><code>status.conditions</code> - full history of request processing before the   current phase;</li> <li><code>status.removeInfo.issues</code> and <code>status.removeInfo.warnings</code> - error   and warning messages occurred during request processing, if any.</li> </ul> </li> <li> <p>Verify that the <code>CephOsdRemoveTask</code> has been completed. For example:     <pre><code>status:\n  phase: Completed # or CompletedWithWarnings if there are non-critical issues\n</code></pre></p> </li> <li> <p>Remove the device cleanup jobs:     <pre><code>kubectl delete jobs -n pelagia -l app=pelagia-lcm-cleanup-disks\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/lcm/replace-ceph-osd/#deploy-a-new-device-after-removal-of-a-failed-one","title":"Deploy a new device after removal of a failed one","text":"<p>Note</p> <p>You can spawn Ceph OSD on a raw device, but it must be clean and without any data or partitions. If you want to add a device that was in use, also ensure it is raw and clean. To clean up all data and partitions from a device, refer to official Rook documentation.</p> <ol> <li> <p>Manually prepare the replacement device on the existing node.</p> </li> <li> <p>Optional. If you want to add a Ceph OSD on top of a raw device that already exists    on a node or is hot-plugged, add the required device using the following    guidelines:</p> <ul> <li>You can add a raw device to a node during node deployment.</li> <li>If a node supports adding devices without a node reboot, you can hot plug   a raw device to a node.</li> <li>If a node does not support adding devices without a node reboot, you can   hot plug a raw device during node shutdown.</li> </ul> </li> <li> <p>Open the <code>CephDeployment</code> CR for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>In the <code>nodes</code> section, add a new device:    <pre><code>spec:\n  nodes:\n  - name: &lt;nodeName&gt;\n    devices:\n    - fullPath: &lt;deviceByID&gt; # Recommended. Non-wwn by-id symlink.\n      # name: &lt;deviceByID&gt; # Not recommended. If a device is supposed to be added with by-id.\n      # fullPath: &lt;deviceByPath&gt; # Not recommended. If a device is supposed to be added with by-path.\n      config:\n        deviceClass: hdd\n</code></pre></p> <p>Substitute <code>&lt;nodeName&gt;</code> with the node name where device <code>&lt;deviceName&gt;</code>  or <code>&lt;deviceByPath&gt;</code> is going to be added as a Ceph OSD.</p> </li> <li> <p>Verify that the Ceph OSD on the specified node is successfully deployed. The    <code>CephDeploymentHealth</code> CR <code>status.healthReport.cephDaemons.cephDaemons</code> section should not contain any issues.    <pre><code>kubectl -n pelagia get cephdeploymenthealth -o yaml\n</code></pre></p> <p>For example:  <pre><code>status:\n  healthReport:\n    cephDaemons:\n      cephDaemons:\n        osd:\n          info:\n          - 3 osds, 3 up, 3 in\n          status: ok\n</code></pre></p> </li> <li> <p>Verify the desired Ceph OSD pod is <code>Running</code>:    <pre><code>kubectl -n rook-ceph get pod -l app=rook-ceph-osd -o wide | grep &lt;nodeName&gt;\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/lcm/replace-meta/","title":"Replace failed metadata device","text":""},{"location":"ops-guide/lcm/replace-meta/#replace-a-failed-metadata-device","title":"Replace a failed metadata device","text":"<p>This section describes the scenario when an underlying metadata device fails with all related Ceph OSDs. In this case, the only solution is to remove all Ceph OSDs related to the failed metadata device, then attach a device that will be used as a new metadata device, and re-create all affected Ceph OSDs.</p>"},{"location":"ops-guide/lcm/replace-meta/#remove-failed-ceph-osds-with-affected-metadata-device","title":"Remove failed Ceph OSDs with affected metadata device","text":"<ol> <li>Save the <code>CephDeployment</code> specification of all Ceph OSDs affected by the    failed metadata device to re-use this specification during re-creation of    Ceph OSDs after disk replacement.</li> <li> <p>Identify Ceph OSD IDs related to the failed metadata device, for example,    using Ceph CLI in the <code>pelagia-ceph-toolbox</code> Pod:    <pre><code>ceph osd metadata\n</code></pre></p> <p>Example of system response:  <pre><code>{\n    \"id\": 11,\n    ...\n    \"bluefs_db_devices\": \"vdc\",\n    ...\n    \"bluestore_bdev_devices\": \"vde\",\n    ...\n    \"devices\": \"vdc,vde\",\n    ...\n    \"hostname\": \"kaas-node-6c5e76f9-c2d2-4b1a-b047-3c299913a4bf\",\n    ...\n},\n{\n    \"id\": 12,\n    ...\n    \"bluefs_db_devices\": \"vdd\",\n    ...\n    \"bluestore_bdev_devices\": \"vde\",\n    ...\n    \"devices\": \"vdd,vde\",\n    ...\n    \"hostname\": \"kaas-node-6c5e76f9-c2d2-4b1a-b047-3c299913a4bf\",\n    ...\n},\n{\n    \"id\": 13,\n    ...\n    \"bluefs_db_devices\": \"vdf\",\n    ...\n    \"bluestore_bdev_devices\": \"vde\",\n    ...\n    \"devices\": \"vde,vdf\",\n    ...\n    \"hostname\": \"kaas-node-6c5e76f9-c2d2-4b1a-b047-3c299913a4bf\",\n    ...\n},\n...\n</code></pre></p> </li> <li> <p>Open the <code>CephDeployment</code> custom resource (CR) for editing:    <pre><code>kubectl -n pelagia edit miraceph\n</code></pre></p> </li> <li> <p>In the <code>nodes</code> section, remove all <code>devices</code> items that relate    to the failed metadata device. When using a metadata device with device filters, remove the whole node    section, including the node <code>name: &lt;nodeName&gt;</code> field, and perform a complete node cleanup as described in    Remove a Ceph node.</p> <p>For example:  <pre><code>spec:\n  nodes:\n  - name: &lt;nodeName&gt;\n    devices:\n    - name: &lt;deviceName1&gt;  # remove the entire item from the devices list\n      # fullPath: &lt;deviceByPath&gt; if device is specified using symlink instead of name\n      config:\n        deviceClass: hdd\n        metadataDevice: &lt;metadataDevice&gt;\n    - name: &lt;deviceName2&gt;  # remove the entire item from the devices list\n      config:\n        deviceClass: hdd\n        metadataDevice: &lt;metadataDevice&gt;\n    - name: &lt;deviceName3&gt;  # remove the entire item from the devices list\n      config:\n        deviceClass: hdd\n        metadataDevice: &lt;metadataDevice&gt;\n    ...\n</code></pre></p> <p>In the example above, <code>&lt;nodeName&gt;</code> is the node name where the metadata device <code>&lt;metadataDevice&gt;</code> must be replaced.</p> </li> <li> <p>Create a <code>CephOsdRemoveTask</code> CR template and save it as <code>replace-failed-meta-&lt;nodeName&gt;-&lt;metadataDevice&gt;-task.yaml</code>:    <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephOsdRemoveTask\nmetadata:\n  name: replace-failed-meta-&lt;nodeName&gt;-&lt;metadataDevice&gt;\n  namespace: pelagia\nspec:\n  nodes:\n    &lt;nodeName&gt;:\n      cleanupByOsdId:\n      - id: &lt;osdID-1&gt;\n      - id: &lt;osdID-2&gt;\n      ...\n</code></pre></p> <p>Substitute the following parameters:</p> <ul> <li><code>&lt;nodeName&gt;</code> and <code>&lt;metadataDevice&gt;</code> with the node and device    names from the previous step</li> <li><code>&lt;osdID-*&gt;</code> with IDs of the affected Ceph OSDs</li> </ul> </li> <li> <p>Apply the template to the cluster:    <pre><code>kubectl apply -f replace-failed-meta-&lt;nodeName&gt;-&lt;metadataDevice&gt;-task.yaml\n</code></pre></p> </li> <li> <p>Verify that the corresponding request has been created:    <pre><code>kubectl -n pelagia get cephosdremovetask\n</code></pre></p> </li> <li> <p>Verify that the <code>removeInfo</code> section is present in the <code>CephOsdRemoveTask</code> CR <code>status</code> and that the <code>cleanupMap</code>    section matches the required removal:    <pre><code>kubectl -n pelagia get cephosdremovetask replace-failed-meta-&lt;nodeName&gt;-&lt;metadataDevice&gt; -o yaml\n</code></pre></p> <p>Example of system response:  <pre><code>removeInfo:\n  cleanupMap:\n    &lt;nodeName&gt;:\n      osdMapping:\n        \"&lt;osdID-1&gt;\":\n          deviceMapping:\n            &lt;dataDevice-1&gt;:\n              deviceClass: hdd\n              devicePath: &lt;dataDeviceByPath-1&gt;\n              devicePurpose: block\n              usedPartition: &lt;dataLvPartition-1&gt;\n              zapDisk: true\n            &lt;metadataDevice&gt;:\n              deviceClass: hdd\n              devicePath: &lt;metadataDeviceByPath&gt;\n              devicePurpose: db\n              usedPartition: /dev/ceph-b0c70c72-8570-4c9d-93e9-51c3ab4dd9f9/osd-db-ecf64b20-1e07-42ac-a8ee-32ba3c0b7e2f\n          uuid: ef516477-d2da-492f-8169-a3ebfc3417e2\n        \"&lt;osdID-2&gt;\":\n          deviceMapping:\n            &lt;dataDevice-2&gt;:\n              deviceClass: hdd\n              devicePath: &lt;dataDeviceByPath-2&gt;\n              devicePurpose: block\n              usedPartition: &lt;dataLvPartition-2&gt;\n              zapDisk: true\n            &lt;metadataDevice&gt;:\n              deviceClass: hdd\n              devicePath: &lt;metadataDeviceByPath&gt;\n              devicePurpose: db\n              usedPartition: /dev/ceph-b0c70c72-8570-4c9d-93e9-51c3ab4dd9f9/osd-db-ecf64b20-1e07-42ac-a8ee-32ba3c0b7e2f\n          uuid: ef516477-d2da-492f-8169-a3ebfc3417e2\n        ...\n</code></pre></p> <p>Definition of values in angle brackets:</p> <ul> <li><code>&lt;nodeName&gt;</code> - underlying node name of the machine, for example,    <code>storage-worker-55</code></li> <li><code>&lt;osdId&gt;</code> - Ceph OSD ID for the device being replaced, for example,    <code>1</code></li> <li><code>&lt;dataDeviceByPath&gt;</code> - <code>by-path</code> of the device placed on the node,    for example, <code>/dev/disk/by-path/pci-0000:00:1t.9</code></li> <li><code>&lt;dataDevice&gt;</code> - name of the device placed on the node, for example,    <code>/dev/vdc</code></li> <li><code>&lt;metadataDevice&gt;</code> - metadata name of the device placed on the node,    for example, <code>/dev/vde</code></li> <li><code>&lt;metadataDeviceByPath&gt;</code> - metadata <code>by-path</code> of the device placed    on the node, for example, <code>/dev/disk/by-path/pci-0000:00:12.0</code></li> <li><code>&lt;dataLvPartition&gt;</code> - logical volume partition of the data device</li> </ul> </li> <li> <p>Wait for the <code>ApproveWaiting</code> phase to appear in <code>status</code>:    <pre><code>kubectl -n pelagia get cephosdremovetask replace-failed-meta-&lt;nodeName&gt;-&lt;metadataDevice&gt; -o yaml\n</code></pre></p> <p>Example of system response:  <pre><code>status:\n  phase: ApproveWaiting\n</code></pre></p> </li> <li> <p>In the <code>CephOsdRemoveTask</code> CR, set the <code>approve</code> flag to <code>true</code>:     <pre><code>kubectl -n pelagia edit cephosdremovetask replace-failed-meta-&lt;nodeName&gt;-&lt;metadataDevice&gt;\n</code></pre></p> <p>Configuration snippet:   <pre><code>spec:\n  approve: true\n</code></pre></p> </li> <li> <p>Review the following <code>status</code> fields of the Ceph LCM CR processing:</p> <ul> <li><code>status.phase</code> - current state of task processing;</li> <li><code>status.messages</code> - description of the current phase;</li> <li><code>status.conditions</code> - full history of task processing before the     current phase;</li> <li><code>status.removeInfo.issues</code> and <code>status.removeInfo.warnings</code> - error     and warning messages occurred during task processing, if any.</li> </ul> </li> <li> <p>Verify that the <code>CephOsdRemoveTask</code> has been completed. For example:     <pre><code>status:\n  phase: Completed # or CompletedWithWarnings if there are non-critical issues\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/lcm/replace-meta/#prepare-the-replaced-metadata-device-for-ceph-osd-re-creation","title":"Prepare the replaced metadata device for Ceph OSD re-creation","text":"<p>Note</p> <p>This section describes how to create a metadata disk partition on N logical volumes. To create one partition on a metadata disk, refer to Re-create the partition on the existing metadata disk.</p> <ol> <li> <p>Partition the replaced metadata device by N logical volumes (LVs), where N    is the number of Ceph OSDs previously located on a failed metadata device.</p> <p>Calculate the new metadata LV percentage of used volume group capacity using the <code>100 / N</code> formula.</p> </li> <li> <p>Log in to the node with the replaced metadata disk.</p> </li> <li> <p>Create an LVM physical volume atop the replaced metadata device:    <pre><code>pvcreate &lt;metadataDisk&gt;\n</code></pre></p> <p>Substitute <code>&lt;metadataDisk&gt;</code> with the replaced metadata device.</p> </li> <li> <p>Create an LVM volume group atop of the physical volume:    <pre><code>vgcreate bluedb &lt;metadataDisk&gt;\n</code></pre></p> <p>Substitute <code>&lt;metadataDisk&gt;</code> with the replaced metadata device.</p> </li> <li> <p>Create N LVM logical volumes with the calculated capacity per each volume:    <pre><code>lvcreate -l &lt;X&gt;%VG -n meta_&lt;i&gt; bluedb\n</code></pre></p> <p>Substitute <code>&lt;X&gt;</code> with the result of the <code>100 / N</code> formula and <code>&lt;i&gt;</code>  with the current number of metadata partitions.</p> </li> </ol> <p>As a result, the replaced metadata device will have N LVM paths, for example, <code>/dev/bluedb/meta_1</code>.</p>"},{"location":"ops-guide/lcm/replace-meta/#re-create-a-ceph-osd-on-the-replaced-metadata-device","title":"Re-create a Ceph OSD on the replaced metadata device","text":"<p>Note</p> <p>You can spawn Ceph OSD on a raw device, but it must be clean and without any data or partitions. If you want to add a device that was in use, also ensure it is raw and clean. To clean up all data and partitions from a device, refer to official Rook documentation.</p> <ol> <li> <p>Open the <code>CephDeployment</code> CR for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>In the <code>nodes</code> section, add the cleaned Ceph OSD device with the replaced    LVM paths of the metadata device from previous steps. For example:    <pre><code>spec:\n  nodes:\n  - name: &lt;nodeName&gt;\n    devices:\n    - fullPath: &lt;deviceByID-1&gt; # Recommended. Add the new device by ID /dev/disk/by-id/...\n      #name: &lt;deviceByID-1&gt; # Not recommended. Add a new device by ID, for example, /dev/disk/by-id/...\n      #fullPath: &lt;deviceByPath-1&gt; # Not recommended. Add a new device by path /dev/disk/by-path/...\n      config:\n        deviceClass: hdd\n        metadataDevice: /dev/&lt;vgName&gt;/&lt;lvName-1&gt;\n    - fullPath: &lt;deviceByID-2&gt; # Recommended. Add the new device by ID /dev/disk/by-id/...\n      #name: &lt;deviceByID-2&gt; # Not recommended. Add a new device by ID, for example, /dev/disk/by-id/...\n      #fullPath: &lt;deviceByPath-2&gt; # Not recommended. Add a new device by path /dev/disk/by-path/...\n      config:\n        deviceClass: hdd\n        metadataDevice: /dev/&lt;vgName&gt;/&lt;lvName-2&gt;\n    - fullPath: &lt;deviceByID-3&gt; # Recommended. Add the new device by ID /dev/disk/by-id/...\n      #name: &lt;deviceByID-3&gt; # Not recommended. Add a new device by ID, for example, /dev/disk/by-id/...\n      #fullPath: &lt;deviceByPath-3&gt; # Not recommended. Add a new device by path /dev/disk/by-path/...\n      config:\n        deviceClass: hdd\n        metadataDevice: /dev/&lt;vgName&gt;/&lt;lvName-3&gt;\n</code></pre></p> <ul> <li>Substitute <code>&lt;nodeName&gt;</code> with the node name where the    metadata device has been replaced.</li> <li>Add all data devices for re-created Ceph OSDs and specify    <code>metadataDevice</code> that is the path to the previously created logical    volume. Substitute <code>&lt;vgName&gt;</code> with a volume group name that contains N    logical volumes <code>&lt;lvName-i&gt;</code>.</li> </ul> </li> <li> <p>Wait for the re-created Ceph OSDs to apply to the Ceph cluster. You can monitor the application state using    either the <code>status</code> section of the <code>CephDeploymentHealth</code> CR or in the <code>pelagia-ceph-toolbox</code> Pod:    <pre><code>kubectl -n pelagia get cephdeploymenthealth -o yaml\nkubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- ceph -s\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/lcm/replace-osd-meta-device/","title":"Replace a failed Ceph OSD disk with a metadata device as a device name","text":"<p>You can apply the below procedure if a Ceph OSD failed with data disk outage and Ceph OSD metadata device specified as a disk. This scenario implies that the Ceph cluster automatically creates a required metadata logical volume on a desired device.</p>"},{"location":"ops-guide/lcm/replace-osd-meta-device/#remove-a-ceph-osd-with-a-metadata-device-as-a-disk-name","title":"Remove a Ceph OSD with a metadata device as a disk name","text":"<p>To remove the affected Ceph OSD with a metadata device as a device name, follow the Remove a failed Ceph OSD by ID with a defined metadata device procedure and capture the following details:</p> <ul> <li> <p>While editing <code>CephDeployment</code> custom resource (CR) in the <code>nodes</code> section, capture the   <code>metadataDevice</code> path to reuse it during re-creation of the Ceph OSD.</p> <p>Example of the <code>spec.nodes</code> section: <pre><code>spec:\n  nodes:\n  - name: &lt;nodeName&gt;\n    devices:\n    - name: &lt;deviceName&gt;  # remove the entire item from the devices list\n      # fullPath: &lt;deviceByPath&gt; if device is specified using by-path instead of name\n      config:\n        deviceClass: hdd\n        metadataDevice: /dev/nvme0n1\n</code></pre></p> <p>In the example above, save the <code>metadataDevice</code> device name <code>/dev/nvme0n1</code>.</p> </li> <li> <p>During <code>CephOsdRemoveTask</code> CR verification of <code>removeInfo</code>, capture the <code>usedPartition</code> value   of the metadata device located in the <code>deviceMapping.&lt;metadataDevice&gt;</code> section.</p> <p>Example of the <code>removeInfo</code> section: <pre><code>removeInfo:\n  cleanUpMap:\n    &lt;nodeName&gt;:\n      osdMapping:\n        \"&lt;osdID&gt;\":\n          deviceMapping:\n            &lt;dataDevice&gt;:\n              deviceClass: hdd\n              devicePath: &lt;dataDeviceByPath&gt;\n              devicePurpose: block\n              usedPartition: /dev/ceph-d2d3a759-2c22-4304-b890-a2d87e056bd4/osd-block-ef516477-d2da-492f-8169-a3ebfc3417e2\n              zapDisk: true\n            &lt;metadataDevice&gt;:\n              deviceClass: hdd\n              devicePath: &lt;metadataDeviceByPath&gt;\n              devicePurpose: db\n              usedPartition: /dev/ceph-b0c70c72-8570-4c9d-93e9-51c3ab4dd9f9/osd-db-ecf64b20-1e07-42ac-a8ee-32ba3c0b7e2f\n          uuid: ef516477-d2da-492f-8169-a3ebfc3417e2\n</code></pre></p> <p>In the example above, capture the following values from the <code>&lt;metadataDevice&gt;</code> section:</p> <ul> <li><code>ceph-b0c70c72-8570-4c9d-93e9-51c3ab4dd9f9</code> - name of the volume group     that contains all metadata partitions on the <code>&lt;metadataDevice&gt;</code> disk;</li> <li><code>osd-db-ecf64b20-1e07-42ac-a8ee-32ba3c0b7e2f</code> - name of the logical     volume that relates to a failed Ceph OSD.</li> </ul> </li> </ul>"},{"location":"ops-guide/lcm/replace-osd-meta-device/#re-create-the-partition-on-the-existing-metadata-disk","title":"Re-create the partition on the existing metadata disk","text":"<p>After you remove the Ceph OSD disk, manually create a separate logical volume for the metadata partition in an existing volume group on the metadata device:</p> <pre><code>lvcreate -l 100%FREE -n meta_1 &lt;vgName&gt;\n</code></pre> <p>Substitute <code>&lt;vgName&gt;</code> with the name of a volume group captured in the <code>usedPartiton</code> parameter.</p> <p>Note</p> <p>If you removed more than one OSD, replace <code>100%FREE</code> with the corresponding partition size. For example: <pre><code>lvcreate -l &lt;partitionSize&gt; -n meta_1 &lt;vgName&gt;\n</code></pre></p> <p>Substitute <code>&lt;partitionSize&gt;</code> with the corresponding value that matches the size of other partitions placed on the affected metadata drive. To obtain <code>&lt;partitionSize&gt;</code>, use the output of the lvs command. For example: <code>16G</code>.</p> <p>During execution of the <code>lvcreate</code> command, the system asks you to wipe the found bluestore label on a metadata device. For example: <pre><code>WARNING: ceph_bluestore signature detected on /dev/ceph-b0c70c72-8570-4c9d-93e9-51c3ab4dd9f9/meta_1 at offset 0. Wipe it? [y/n]:\n</code></pre></p> <p>Using the interactive shell, answer <code>n</code> to keep all metadata partitions alive. After answering <code>n</code>, the system outputs the following:</p> <pre><code>Aborted wiping of ceph_bluestore.\n1 existing signature left on the device.\nLogical volume \"meta_1\" created.\n</code></pre>"},{"location":"ops-guide/lcm/replace-osd-meta-device/#re-create-the-ceph-osd-with-the-re-created-metadata-partition","title":"Re-create the Ceph OSD with the re-created metadata partition","text":"<p>Note</p> <p>You can spawn Ceph OSD on a raw device, but it must be clean and without any data or partitions. If you want to add a device that was in use, also ensure it is raw and clean. To clean up all data and partitions from a device, refer to official Rook documentation.</p> <ol> <li> <p>Optional. If you want to add a Ceph OSD on top of a raw device that already exists    on a node or is hot-plugged, add the required device using the following    guidelines:</p> <ul> <li>You can add a raw device to a node during node deployment.</li> <li>If a node supports adding devices without a node reboot, you can hot plug   a raw device to a node.</li> <li>If a node does not support adding devices without a node reboot, you can   hot plug a raw device during node shutdown.</li> </ul> </li> <li> <p>Open the <code>CephDeployment</code> CR for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>In the <code>nodes</code> section, add the replaced device with the same <code>metadataDevice</code> path as in the previous Ceph OSD:    <pre><code>spec:\n  nodes:\n  - name: &lt;nodeName&gt;\n    devices:\n    - fullPath: &lt;deviceByID&gt; # Recommended. Add a new device by-id symlink, for example, /dev/disk/by-id/...\n      #name: &lt;deviceByID&gt; # Not recommended. Add a new device by ID, for example, /dev/disk/by-id/...\n      #fullPath: &lt;deviceByPath&gt; # Not recommended. Add a new device by path, for example, /dev/disk/by-path/...\n      config:\n        deviceClass: hdd\n        metadataDevice: /dev/&lt;vgName&gt;/meta_1\n</code></pre></p> <p>Substitute <code>&lt;nodeName&gt;</code> with the node name where the new device <code>&lt;deviceByID&gt;</code> or <code>&lt;deviceByPath&gt;</code> must be added.  Also specify <code>metadataDevice</code> with the path to the logical volume created  during the Re-create the partition on the existing metadata disk procedure.</p> </li> <li> <p>Wait for the replaced disk to apply to the Ceph cluster as a new Ceph OSD.    You can monitor the application state using either the <code>status</code> section    of the <code>CephDeploymentHealth</code> CR or in the <code>pelagia-ceph-toolbox</code> pod:    <pre><code>kubectl -n pelagia get cephdeploymenthealth -o yaml\nkubectl -n rook-ceph exec -it deploy/pelagia-cephtoolbox -- ceph -s\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/lcm/replace-osd-meta-lvm/","title":"Replace failed Ceph OSD with metadata LVM","text":""},{"location":"ops-guide/lcm/replace-osd-meta-lvm/#replace-a-failed-ceph-osd-with-a-metadata-device-as-a-logical-volume-path","title":"Replace a failed Ceph OSD with a metadata device as a logical volume path","text":"<p>You can apply the below procedure in the following cases:</p> <ul> <li>A Ceph OSD failed without a data or metadata device outage. In this case,   first remove a failed Ceph OSD and clean up all corresponding disks and   partitions. Then add a new Ceph OSD to the same data and metadata paths.</li> <li>A Ceph OSD failed with data or metadata device outage. In this case, you   also first remove a failed Ceph OSD and clean up all corresponding disks and   partitions. Then add a new Ceph OSD to a newly replaced data device with the   same metadata path.</li> </ul> <p>Note</p> <p>The below procedure also applies to manually created metadata partitions.</p>"},{"location":"ops-guide/lcm/replace-osd-meta-lvm/#remove-a-failed-ceph-osd-by-id-with-a-defined-metadata-device","title":"Remove a failed Ceph OSD by ID with a defined metadata device","text":"<ol> <li> <p>Identify the ID of Ceph OSD related to a failed device. For example, use    the Ceph CLI in the <code>pelagia-ceph-toolbox</code> Pod:    <pre><code>ceph osd metadata\n</code></pre></p> <p>Example of system response:  <pre><code>{\n    \"id\": 0,\n    ...\n    \"bluestore_bdev_devices\": \"vdc\",\n    ...\n    \"devices\": \"vdc\",\n    ...\n    \"hostname\": \"kaas-node-6c5e76f9-c2d2-4b1a-b047-3c299913a4bf\",\n    ...\n    \"pod_name\": \"rook-ceph-osd-0-7b8d4d58db-f6czn\",\n    ...\n},\n{\n    \"id\": 1,\n    ...\n    \"bluefs_db_devices\": \"vdf\",\n    ...\n    \"bluestore_bdev_devices\": \"vde\",\n    ...\n    \"devices\": \"vde,vdf\",\n    ...\n    \"hostname\": \"kaas-node-6c5e76f9-c2d2-4b1a-b047-3c299913a4bf\",\n    ...\n    \"pod_name\": \"rook-ceph-osd-1-78fbc47dc5-px9n2\",\n    ...\n},\n...\n</code></pre></p> </li> <li> <p>Open the <code>CephDeployment</code> custom resource (CR) for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>In the <code>nodes</code> section:</p> <ol> <li>Find and capture the <code>metadataDevice</code> path to reuse it during re-creation of the Ceph OSD.</li> <li> <p>Remove the required device. Example configuration snippet:     <pre><code>spec:\n  nodes:\n  - name: &lt;nodeName&gt;\n    devices:\n    - name: &lt;deviceName&gt;  # remove the entire item from the devices list\n      # fullPath: &lt;deviceByPath&gt; if device is specified using by-path instead of name\n      config:\n        deviceClass: hdd\n        metadataDevice: /dev/bluedb/meta_1\n</code></pre></p> <p>In the example above, <code>&lt;nodeName&gt;</code> is the name of node on which   the device <code>&lt;deviceName&gt;</code> or <code>&lt;deviceByPath&gt;</code> must be replaced.</p> </li> </ol> </li> <li> <p>Create a <code>CephOsdRemoveTask</code> CR template and save it as <code>replace-failed-osd-&lt;nodeName&gt;-&lt;osdID&gt;-task.yaml</code>:    <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephOsdRemoveTask\nmetadata:\n  name: replace-failed-osd-&lt;nodeName&gt;-&lt;deviceName&gt;\n  namespace: pelagia\nspec:\n  nodes:\n    &lt;nodeName&gt;:\n      cleanupByOsdId:\n      - id: &lt;osdID&gt;\n</code></pre></p> <p>Substitute the following parameters:  - <code>&lt;nodeName&gt;</code> and <code>&lt;deviceName&gt;</code> with the node and device names    from the previous step;  - <code>&lt;osdID&gt;</code> with the ID of the affected Ceph OSD.</p> </li> <li> <p>Apply the template to the cluster:    <pre><code>kubectl apply -f replace-failed-osd-&lt;nodeName&gt;-&lt;osdID&gt;-task.yaml\n</code></pre></p> </li> <li> <p>Verify that the corresponding task has been created:    <pre><code>kubectl -n pelagia get cephosdremovetask\n</code></pre></p> </li> <li> <p>Verify that the <code>status</code> section of <code>CephOsdRemoveTask</code> contains    the <code>removeInfo</code> section:    <pre><code>kubectl -n pelagia get cephosdremovetask replace-failed-osd-&lt;nodeName&gt;-&lt;osdID&gt; -o yaml\n</code></pre></p> <p>Example of system response:  <pre><code>removeInfo:\n  cleanupMap:\n    &lt;nodeName&gt;:\n      osdMapping:\n        \"&lt;osdID&gt;\":\n          deviceMapping:\n            &lt;dataDevice&gt;:\n              deviceClass: hdd\n              devicePath: &lt;dataDeviceByPath&gt;\n              devicePurpose: block\n              usedPartition: /dev/ceph-d2d3a759-2c22-4304-b890-a2d87e056bd4/osd-block-ef516477-d2da-492f-8169-a3ebfc3417e2\n              zapDisk: true\n            &lt;metadataDevice&gt;:\n              deviceClass: hdd\n              devicePath: &lt;metadataDeviceByPath&gt;\n              devicePurpose: db\n              usedPartition: /dev/bluedb/meta_1\n          uuid: ef516477-d2da-492f-8169-a3ebfc3417e2\n</code></pre></p> <p>Definition of values in angle brackets:</p> <ul> <li><code>&lt;nodeName&gt;</code> - underlying node name of the machine, for example,    <code>storage-worker-3</code></li> <li><code>&lt;osdId&gt;</code> - Ceph OSD ID for the device being replaced, for example,    <code>1</code></li> <li><code>&lt;dataDeviceByPath&gt;</code> - <code>by-path</code> of the device placed on the node,    for example, <code>/dev/disk/by-path/pci-0000:00:1t.9</code></li> <li><code>&lt;dataDevice&gt;</code> - name of the device placed on the node, for example,    <code>/dev/vde</code></li> <li><code>&lt;metadataDevice&gt;</code> - metadata name of the device placed on the node,    for example, <code>/dev/vdf</code></li> <li><code>&lt;metadataDeviceByPath&gt;</code> - metadata <code>by-path</code> of the device placed    on the node, for example, <code>/dev/disk/by-path/pci-0000:00:12.0</code></li> </ul> </li> <li> <p>Verify that the <code>cleanupMap</code> section matches the required removal and    wait for the <code>ApproveWaiting</code> phase to appear in <code>status</code>:    <pre><code>kubectl -n pelagia get cephosdremovetask replace-failed-osd-&lt;nodeName&gt;-&lt;osdID&gt; -o yaml\n</code></pre></p> <p>Example of system response:  <pre><code>status:\n  phase: ApproveWaiting\n</code></pre></p> </li> <li> <p>In the <code>CephOsdRemoveTask</code> CR, set the <code>approve</code> flag to <code>true</code>:    <pre><code>kubectl -n pelagia edit cephosdremovetask replace-failed-osd-&lt;nodeName&gt;-&lt;osdID&gt;\n</code></pre></p> <p>Configuration snippet:  <pre><code>spec:\n  approve: true\n</code></pre></p> </li> <li> <p>Review the following <code>status</code> fields of the Ceph LCM CR processing:</p> <ul> <li><code>status.phase</code> - current state of task processing;</li> <li><code>status.messages</code> - description of the current phase;</li> <li><code>status.conditions</code> - full history of task processing before the     current phase;</li> <li><code>status.removeInfo.issues</code> and <code>status.removeInfo.warnings</code> - error     and warning messages occurred during task processing, if any.</li> </ul> </li> <li> <p>Verify that the <code>CephOsdRemoveTask</code> has been completed. For example:     <pre><code>status:\n  phase: Completed # or CompletedWithWarnings if there are non-critical issues\n</code></pre></p> </li> </ol> <p></p>"},{"location":"ops-guide/lcm/replace-osd-meta-lvm/#re-create-a-ceph-osd-with-the-same-metadata-partition","title":"Re-create a Ceph OSD with the same metadata partition","text":"<p>Note</p> <p>You can spawn Ceph OSD on a raw device, but it must be clean and without any data or partitions. If you want to add a device that was in use, also ensure it is raw and clean. To clean up all data and partitions from a device, refer to official Rook documentation.</p> <ol> <li> <p>Optional. If you want to add a Ceph OSD on top of a raw device that already exists    on a node or is hot-plugged, add the required device using the following    guidelines:</p> <ul> <li>You can add a raw device to a node during node deployment.</li> <li>If a node supports adding devices without a node reboot, you can hot plug   a raw device to a node.</li> <li>If a node does not support adding devices without a node reboot, you can   hot plug a raw device during node shutdown.</li> </ul> </li> <li> <p>Open the <code>CephDeployment</code> CR for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>In the <code>nodes</code> section, add the replaced device with the same    <code>metadataDevice</code> path as on the removed Ceph OSD. For example:    <pre><code>spec:\n  nodes:\n  - name: &lt;nodeName&gt;\n    devices:\n    - fullPath: &lt;deviceByID&gt; # Recommended. Add a new device by-id symlink, for example, /dev/disk/by-id/...\n      #name: &lt;deviceByID&gt; # Not recommended. Add a new device by ID, for example, /dev/disk/by-id/...\n      #fullPath: &lt;deviceByPath&gt; # Not recommended. Add a new device by path, for example, /dev/disk/by-path/...\n      config:\n        deviceClass: hdd\n        metadataDevice: /dev/bluedb/meta_1 # Must match the value of the previously removed OSD\n</code></pre></p> <p>Substitute <code>&lt;nodeName&gt;</code> with the node name where the new device <code>&lt;deviceByID&gt;</code> or <code>&lt;deviceByPath&gt;</code> must be added.</p> </li> <li> <p>Wait for the replaced disk to apply to the Ceph cluster as a new Ceph OSD. You can monitor the application    state using either the <code>status</code> section of the <code>CephDeploymentHealth</code> CR or in the <code>pelagia-ceph-toolbox</code> Pod:    <pre><code>kubectl -n pelagia get cephdeploymenthealth -o yaml\nkubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- ceph -s\n</code></pre></p> </li> </ol>"},{"location":"ops-guide/rockoon/calc-target-ratio/","title":"Calculate target ratios for Rockoon","text":""},{"location":"ops-guide/rockoon/calc-target-ratio/#calculate-target-ratios-for-ceph-pools-for-rockoon","title":"Calculate target ratios for Ceph pools for Rockoon","text":"<p>Ceph pool target ratio defines for the Placement Group (PG) autoscaler the amount of data the pools are expected to acquire over time in relation to each other. You can set initial PG values for each Ceph pool. Otherwise, the autoscaler starts with the minimum value and scales up, causing a lot of data to move in the background.</p> <p>You can allocate several pools to use the same device class, which is a solid block of available capacity in Ceph. For example, if three pools (<code>kubernetes-hdd</code>, <code>images-hdd</code>, and <code>volumes-hdd</code>) are set to use the same device class <code>hdd</code>, you can set the target ratio for Ceph pools to provide 80% of capacity to the <code>volumes-hdd</code> pool and distribute the remaining capacity evenly between the two other pools. This way, a Ceph pool target ratio instructs Ceph on when to warn that a pool is running out of free space. At the same time, it instructs Ceph on how many placement groups Ceph should allocate/autoscale for a pool for better data distribution.</p> <p>Ceph pool target ratio is not a constant value, and you can change it according to new capacity plans. Once you specify a target ratio, if the PG number of a pool scales, other pools with a specified target ratio will automatically scale accordingly.</p> <p>For details, see Ceph Documentation: Autoscaling Placement Groups.</p>"},{"location":"ops-guide/rockoon/calc-target-ratio/#calculate-a-target-ratio-for-each-ceph-pool","title":"Calculate a target ratio for each Ceph pool","text":"<ol> <li> <p>Define the raw capacity of the entire storage by device class:    <pre><code>kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o name) -- ceph df\n</code></pre></p> <p>For illustration purposes, the procedure below uses raw capacity of 185 TB  or 189440 GB.</p> </li> <li> <p>Design Ceph pools with the considered device class upper bounds of the    possible capacity. For example, consider the <code>hdd</code> device class that    contains the following pools:</p> <ul> <li>The <code>kubernetes-hdd</code> pool will contain not more than 2048 GB.</li> <li>The <code>images-hdd</code> pool will contain not more than 2048 GB.</li> <li>The <code>volumes-hdd</code> pool will contain 50 GB per VM. The upper bound of    used VMs on the cloud is 204, the pool's replicated size is <code>3</code>.    Therefore, calculate the upper bounds for <code>volumes-hdd</code>:    <pre><code>50 GB per VM * 204 VMs * 3 replicas = 30600 GB\n</code></pre></li> <li>The <code>backup-hdd</code> pool can be calculated as a relative of    <code>volumes-hdd</code>. For example, 1 <code>volumes-hdd</code> storage unit per 5    <code>backup-hdd</code> units.</li> <li>The <code>vms-hdd</code> is a pool for ephemeral storage Copy on Write (CoW). We    recommend designing the amount of ephemeral data it should store. For    example, we use 500 GB. However, in reality, despite the CoW data    reduction, this value is very optimistic.</li> </ul> <p>Note</p> <p>If <code>dataPool</code> is replicated and Ceph Object Store is planned for intensive use, also calculate upper bounds for <code>dataPool</code>.</p> </li> <li> <p>Calculate a target ratio for each considered pool. For example:</p> Pools upper bounds Pools capacity <ul><li><code>kubernetes-hdd</code> = 2048 GB</li><li><code>images-hdd</code> = 2048 GB</li><li><code>volumes-hdd</code> = 30600 GB</li><li><code>backup-hdd</code> = 30600 GB * 5 = 153000 GB</li><li><code>vms-hdd</code> = 500 GB</li></ul> <ul><li>Summary capacity = 188196 GB</li><li>Total raw capacity = 189440 GB</li></ul> <ol> <li> <p>Calculate pools' fit factor using the     (total raw capacity) / (pools' summary capacity) formula. For example:     <pre><code>pools fit factor = 189440 / 188196 = 1.0066\n</code></pre></p> </li> <li> <p>Calculate pools' upper bounds size using the     (pool upper bounds) * (pools fit factor) formula. For example:     <pre><code>kubernetes-hdd = 2048 GB * 1.0066   = 2061.5168 GB\nimages-hdd     = 2048 GB * 1.0066   = 2061.5168 GB\nvolumes-hdd    = 30600 GB * 1.0066  = 30801.96 GB\nbackup-hdd     = 153000 GB * 1.0066 = 154009.8 GB\nvms-hdd        = 500 GB * 1.0066    = 503.3 GB\n</code></pre></p> </li> <li> <p>Calculate pools' target ratio using the     (pool upper bounds) * 100 / (total raw capacity) formula. For     example:     <pre><code>kubernetes-hdd = 2061.5168 GB * 100 / 189440 GB = 1.088\nimages-hdd     = 2061.5168 GB * 100 / 189440 GB = 1.088\nvolumes-hdd    = 30801.96 GB * 100 / 189440 GB  = 16.259\nbackup-hdd     = 154009.8 GB * 100 / 189440 GB  = 81.297\nvms-hdd        = 503.3 GB * 100 / 189440 GB     = 0.266\n</code></pre></p> </li> </ol> </li> <li> <p>If required, calculate the target ratio for erasure-coded pools.</p> <p>Due to erasure-coded pools splitting each object into <code>K</code> data parts  and <code>M</code> coding parts, the total used storage for each object is less  than that in replicated pools. Indeed, <code>M</code> is equal to the number of  OSDs that can be missing from the cluster without the cluster experiencing  data loss. This means that planned data is stored with an efficiency  of <code>(K+M)/2</code> on the Ceph cluster. For example, if an erasure-coded data  pool with <code>K=2, M=2</code> planned capacity is 200 GB, then the total used  capacity is <code>200*(2+2)/2</code>, which is 400 GB.</p> </li> <li> <p>Open the <code>CephDeployment</code> CR for editing:    <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>In the <code>pools</code> section, specify the calculated relatives as <code>parameters.target_size_ratio</code> for each considered    replicated pool. For example:    <pre><code>spec:\n  pools:\n  - name: kubernetes\n    deviceClass: hdd\n    ...\n    replicated:\n      size: 3\n    parameters:\n      target_size_ratio: \"1.088\"\n  - name: images\n    deviceClass: hdd\n    ...\n    replicated:\n      size: 3\n    parameters:\n      target_size_ratio: \"1.088\"\n  - name: volumes\n    deviceClass: hdd\n    ...\n    replicated:\n      size: 3\n    parameters:\n      target_size_ratio: \"16.259\"\n  - name: backup\n    deviceClass: hdd\n    ...\n    replicated:\n      size: 3\n    parameters:\n      target_size_ratio: \"81.297\"\n  - name: vms\n    deviceClass: hdd\n    ...\n    replicated:\n      size: 3\n    parameters:\n      target_size_ratio: \"0.266\"\n</code></pre></p> <p>If Ceph Object Store <code>dataPool</code> is <code>replicated</code> and a proper value is  calculated, also specify it:  <pre><code>spec:\n  objectStorage:\n    rgw:\n      name: rgw-store\n      ...\n      dataPool:\n        deviceClass: hdd\n        ...\n        replicated:\n          size: 3\n        parameters:\n          target_size_ratio: \"&lt;relative&gt;\"\n</code></pre></p> </li> <li> <p>In the <code>pools</code> section, specify the calculated relatives as    <code>parameters.target_size_ratio</code> for each considered erasure-coded pool. For    example:</p> <p>Note</p> <p>The <code>parameters</code> section is a key-value mapping where the value is of the string type and should be quoted.</p> <pre><code>spec:\n  pools:\n  - name: ec-pool\n    deviceClass: hdd\n    ...\n    parameters:\n      target_size_ratio: \"&lt;relative&gt;\"\n</code></pre> <p>If Ceph Object Store <code>dataPool</code> is <code>erasure-coded</code> and a proper value  is calculated, also specify it:  <pre><code>spec:\n  objectStorage:\n    rgw:\n      name: rgw-store\n      ...\n      dataPool:\n        deviceClass: hdd\n        ...\n        parameters:\n          target_size_ratio: \"&lt;relative&gt;\"\n</code></pre></p> </li> <li> <p>Verify that all target ratios have been successfully applied to the Ceph cluster:    <pre><code>kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph osd pool autoscale-status\n</code></pre></p> <p>Example of system response:  <pre><code>POOL                                SIZE  TARGET SIZE  RATE  RAW CAPACITY  RATIO   TARGET RATIO  EFFECTIVE RATIO  BIAS  PG_NUM  NEW PG_NUM  AUTOSCALE\ndevice_health_metrics               0                  2.0   149.9G        0.0000                                 1.0   1                   on\nkubernetes-hdd                      2068               2.0   149.9G        0.0000  1.088         1.0885           1.0   32                  on\nvolumes-hdd                         19                 2.0   149.9G        0.0000  16.259        16.2591          1.0   256                 on\nvms-hdd                             19                 2.0   149.9G        0.0000  0.266         0.2661           1.0   128                 on\nbackup-hdd                          19                 2.0   149.9G        0.0000  81.297        81.2972          1.0   256                 on\nimages-hdd                          888.8M             2.0   149.9G        0.0116  1.088         1.0881           1.0   32                  on\n</code></pre></p> </li> <li> <p>Optional. Repeat the steps above for other device classes.</p> </li> </ol>"},{"location":"ops-guide/rockoon/cinder-multi-backend/","title":"Configure Ceph pools for Cinder multi-backend","text":""},{"location":"ops-guide/rockoon/cinder-multi-backend/#ceph-pools-for-cinder-multi-backend","title":"Ceph pools for Cinder multi-backend","text":"<p>The <code>CephDeployment</code> custom resource (CR) supports multiple Ceph pools with the <code>volumes</code> role to configure Cinder multiple backends in Rockoon OpenStack.</p>"},{"location":"ops-guide/rockoon/cinder-multi-backend/#configure-ceph-pools-for-cinder-multiple-backends","title":"Configure Ceph pools for Cinder multiple backends","text":"<ol> <li> <p>In the <code>CephDeployment</code> CR, add the desired number of Ceph pools to the <code>pools</code> section with the <code>volumes</code> role:    <pre><code>kubectl -n ceph-lcm-mirantis edit miraceph\n</code></pre></p> <p>Example configuration:  <pre><code>spec:\n  pools:\n  - default: false\n    deviceClass: hdd\n    name: volumes\n    replicated:\n      size: 3\n    role: volumes\n  - default: false\n    deviceClass: hdd\n    name: volumes-backend-1\n    replicated:\n      size: 3\n    role: volumes\n  - default: false\n    deviceClass: hdd\n    name: volumes-backend-2\n    replicated:\n      size: 3\n    role: volumes\n</code></pre></p> </li> <li> <p>Verify that Cinder backend pools are created and ready:    <pre><code>kubectl -n pelagia get cephdeploymenthealth -o yaml\n</code></pre></p> <p>Example output:  <pre><code>status:\n  healthReport:\n    rookCephObjects:\n      blockStorage:\n        cephBlockPools:\n          volumes-hdd:\n            info:\n              failureDomain: host\n              type: Replicated\n            observedGeneration: 1\n            phase: Ready\n            poolID: 12\n          volumes-backend-1-hdd:\n            info:\n              failureDomain: host\n              type: Replicated\n            observedGeneration: 1\n            phase: Ready\n            poolID: 13\n          volumes-backend-2-hdd:\n            info:\n              failureDomain: host\n              type: Replicated\n            observedGeneration: 1\n            phase: Ready\n            poolID: 14\n</code></pre></p> </li> <li> <p>Verify that the added Ceph pools are accessible from the Cinder service.    For example:    <pre><code>kubectl -n openstack exec -it cinder-volume-0 -- rbd ls -p volumes-backend-1-hdd -n client.cinder\nkubectl -n openstack exec -it cinder-volume-0 -- rbd ls -p volumes-backend-2-hdd -n client.cinder\n</code></pre></p> </li> </ol> <p>After Ceph pools become available, Rockoon will automatically specify them as an additional Cinder backend and register as a new volume type, which you can use to create Cinder volumes.</p>"},{"location":"ops-guide/rockoon/rockoon-integration/","title":"Integrate Pelagia with Rockoon","text":"<p>This document describes how to integrate Pelagia with Rockoon OpenStack Controllers. As Ceph supports integration with OpenStack, Pelagia provides a way to integrate Rook Ceph cluster with Rockoon OpenStack inside the Kubernetes cluster.</p>"},{"location":"ops-guide/rockoon/rockoon-integration/#how-to-integrate-pelagia-with-rockoon","title":"How to integrate Pelagia with Rockoon","text":"<p>In the <code>CephDeployment</code> custom resource, create the following Ceph pools required for Rockoon OpenStack and ensure that the <code>role</code> parameter is explicitly set:</p> <ul> <li><code>volumes</code> for the OpenStack Block Storage service (<code>cinder</code>)</li> <li><code>backup</code> for the OpenStack Block Storage service (<code>cinder</code>)</li> <li><code>vms</code> for the OpenStack Compute service (<code>nova</code>)</li> <li><code>images</code> for the OpenStack Image service (<code>glance</code>)</li> </ul> <p>For example:</p> <pre><code>spec:\n  pools:\n  ...\n  - default: false\n    deviceClass: hdd\n    name: volumes\n    replicated:\n      size: 3\n    role: volumes\n  - default: false\n    deviceClass: hdd\n    name: backup\n    replicated:\n      size: 3\n    role: backup\n  - default: false\n    deviceClass: hdd\n    name: vms\n    replicated:\n      size: 3\n    role: vms\n  - default: false\n    deviceClass: hdd\n    name: images\n    replicated:\n      size: 3\n    role: images\n</code></pre> <p>As a result, Pelagia creates the following Ceph pools: <code>volumes-hdd</code>, <code>backup-hdd</code>, <code>vms-hdd</code>, and <code>images-hdd</code>. Target ratios will be automatically configured for these pools to match the default OpenStack requirements:</p> <ul> <li>Volumes pool: 0.4</li> <li>Backup pool:  0.1</li> <li>VMs pool:     0.2</li> <li>Images pool:  0.1</li> </ul> <p>Mirantis recommends adjusting these ratios according to your OpenStack deployment requirements using the <code>parameters.target_size_ratio</code> parameter located in the <code>pools</code> section.</p> <p>After Ceph pools are created, Pelagia Deployment Controller creates a secret in the <code>openstack-ceph-shared</code> namespace with all necessary information for Rockoon OpenStack services to be configured with the Ceph cluster. Rockoon Controller watches this namespace and transforms the secret into the data structures expected by OpenStack Helm charts. After that, OpenStack services will be connected to the desired Ceph cluster.</p> <p>If <code>CephDeployment</code> contains the <code>objectStorage</code> section and Ceph Object Storage is deployed, then Pelagia and Rockoon enable Ceph RADOS Gateway integration with OpenStack Object Storage service (<code>swift</code>).</p>"},{"location":"ops-guide/verify-ceph/verify-ceph-core/","title":"Verify Ceph Cluster services","text":""},{"location":"ops-guide/verify-ceph/verify-ceph-core/#verify-the-ceph-core-services","title":"Verify the Ceph core services","text":"<p>To confirm that all Ceph components including <code>mon</code>, <code>mgr</code>, <code>osd</code>, and <code>rgw</code> have joined your cluster properly, analyze the logs for each pod and verify the Ceph status:</p> <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- ceph -s\n</code></pre> <p>Example of a positive system response:</p> <pre><code>cluster:\n    id:     4336ab3b-2025-4c7b-b9a9-3999944853c8\n    health: HEALTH_OK\n\nservices:\n    mon: 3 daemons, quorum a,b,c (age 20m)\n    mgr: a(active, since 19m)\n    osd: 6 osds: 6 up (since 16m), 6 in (since 16m)\n    rgw: 1 daemon active (miraobjstore.a)\n\ndata:\n    pools:   12 pools, 216 pgs\n    objects: 201 objects, 3.9 KiB\n    usage:   6.1 GiB used, 174 GiB / 180 GiB avail\n    pgs:     216 active+clean\n</code></pre> <p>To verify Ceph cluster health, run: <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- ceph health detail\n</code></pre></p> <p>A healthy cluster returns the following output: <pre><code>HEALTH_OK\n</code></pre></p> <p>If the output contains <code>HEALTH_WARN</code> or <code>HEALTH_ERR</code>, please see Ceph Health Checks.</p>"},{"location":"ops-guide/verify-ceph/verify-cephdeploymenthealth/","title":"Verify CephDeployment Health","text":""},{"location":"ops-guide/verify-ceph/verify-cephdeploymenthealth/#verify-ceph-cluster-state","title":"Verify Ceph cluster state","text":"<p>To verify the state of a Ceph cluster, Pelagia provides statuses to <code>CephDeployment</code> and <code>CephDeploymentHealth</code> custom resources (CR). These resources contain information about the state of the Ceph cluster components, their health, and potentially problematic components.</p> <p>To verify the Pelagia API health:</p> <ol> <li> <p>Obtain the <code>CephDeployment</code> CR:    <pre><code>kubectl -n pelagia get cephdpl -o yaml\n</code></pre></p> <p>Information from <code>CephDeployment.status</code> reflects the spec handling state and  validation result. For the description of status fields, see  CephDeployment: Status fields.</p> </li> <li> <p>Obtain the <code>CephDeploymentHealth</code> CR:    <pre><code>kubectl -n pelagia get cephdeploymenthealth -o yaml\n</code></pre></p> <p>Information from <code>CephDeploymentHealth.status</code> contains extensive details about  Ceph cluster and a shortened version with status summary. For the description of  status fields, see CephDeploymentHealth.</p> </li> </ol>"},{"location":"ops-guide/verify-ceph/verify-pelagia-and-rook/","title":"Verify Pelagia and Rook","text":""},{"location":"ops-guide/verify-ceph/verify-pelagia-and-rook/#verify-pelagia-controllers-and-rook-ceph-operator","title":"Verify Pelagia Controllers and Rook Ceph Operator","text":"<p>The starting point for Pelagia, Rook and Ceph troubleshooting is the Pelagia Controllers and Rook Ceph Operator logs. Once you locate the component that causes issues, verify the logs of the related pod. This section describes how to verify Pelagia Controllers and Rook objects of a Ceph cluster.</p>"},{"location":"ops-guide/verify-ceph/verify-pelagia-and-rook/#verify-pelagia-and-rook","title":"Verify Pelagia and Rook","text":"<ol> <li> <p>Verify that the status of each pod in the Pelagia and Rook namespaces is <code>Running</code>:</p> <ul> <li>For <code>pelagia</code>:    <pre><code>kubectl -n pelagia get pod\n</code></pre></li> <li>For <code>rook-ceph</code>:    <pre><code>kubectl -n rook-ceph get pod\n</code></pre></li> </ul> </li> <li> <p>Verify Pelagia Deployment Controller that prepares the configuration for Rook to deploy    the Ceph cluster, which is managed using the <code>CephDeployment</code> custom resource (CR).</p> <ol> <li> <p>List the pods:     <pre><code>kubectl -n pelagia get pods\n</code></pre></p> </li> <li> <p>Verify the logs of the required pod:     <pre><code>kubectl -n pelagia logs &lt;pelagia-deployment-controller-pod-name&gt;\n</code></pre></p> </li> <li> <p>Verify the configuration:     <pre><code>kubectl -n pelagia get cephdpl -o yaml\n</code></pre></p> </li> </ol> <p>If Rook cannot finish the deployment, verify the Rook Operator logs as  described in the following step.</p> </li> <li> <p>Verify the Rook Ceph Operator logs. Rook deploys a Ceph cluster based on custom    resources created by the Pelagia Deployment Controller, such as <code>cephblockpools</code>, <code>cephclients</code>,    <code>cephcluster</code>, and so on. Rook Ceph Operator logs contain details about component    orchestration.</p> <ol> <li> <p>Verify the Rook Ceph Operator logs:     <pre><code>kubectl -n rook-ceph logs -l app=rook-ceph-operator\n</code></pre></p> </li> <li> <p>Verify the <code>CephCluster</code> configuration:</p> <p>Note</p> <p>In Pelagia, <code>CephDeployment</code> manages the <code>CephCluster</code> CR. Use the <code>CephCluster</code> CR only for verification and do not modify it manually.</p> <pre><code>kubectl get cephcluster -n rook-ceph -o yaml\n</code></pre> </li> </ol> <p>For details about the Ceph cluster status and to get access to CLI tools,  connect to the <code>pelagia-ceph-toolbox</code> pod as described in the following step.</p> </li> <li> <p>Verify the <code>pelagia-ceph-toolbox</code> pod:</p> <ol> <li>Execute the <code>pelagia-ceph-toolbox</code> pod:     <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- bash\n</code></pre></li> <li>Verify that CLI commands can run on the <code>pelagia-ceph-toolbox</code> pod:     <pre><code>ceph -s\n</code></pre></li> </ol> </li> <li> <p>Verify hardware:</p> <ol> <li>Through the <code>pelagia-ceph-toolbox</code> pod, obtain the required device in your     cluster:     <pre><code>ceph osd tree\n</code></pre></li> <li>Enter all Ceph OSD pods in the <code>rook-ceph</code> namespace one by one:     <pre><code>kubectl exec -it -n rook-ceph &lt;osd-pod-name&gt; bash\n</code></pre></li> <li>Verify that the <code>ceph-volume</code> tool is available on all pods running on     the target node:     <pre><code>ceph-volume lvm list\n</code></pre></li> </ol> </li> <li> <p>Verify data access. Ceph volumes can be consumed directly by Kubernetes    workloads and internally, for example, by OpenStack services. To verify the    Kubernetes storage:</p> <ol> <li> <p>Verify the available storage classes. The storage classes that are     automatically managed by Ceph Controller use the     <code>rook-ceph.rbd.csi.ceph.com</code> provisioner.     <pre><code>kubectl get storageclass\n</code></pre></p> <p>Example of system response:   <pre><code>NAME                            PROVISIONER                    RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nkubernetes-ssd (default)        rook-ceph.rbd.csi.ceph.com     Delete          Immediate              false                  55m\n</code></pre></p> </li> <li> <p>Verify that volumes are properly connected to the Pod:</p> <ol> <li> <p>Obtain the list of volumes in all namespaces or use a particular one:      <pre><code>kubectl get persistentvolumeclaims -A\n</code></pre></p> <p>Example of system response:    <pre><code>NAMESPACE   NAME       STATUS   VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS     AGE\nrook-ceph   app-test   Bound    pv-test   1Gi        RWO            kubernetes-ssd   11m\n</code></pre></p> </li> <li> <p>For each volume, verify the connection. For example:      <pre><code>kubectl describe pvc app-test -n rook-ceph\n</code></pre></p> <p>Example of a positive system response:    <pre><code>Name:          app-test\nNamespace:     kaas\nStorageClass:  rook-ceph\nStatus:        Bound\nVolume:        pv-test\nLabels:        &lt;none&gt;\nAnnotations:   pv.kubernetes.io/bind-completed: yes\n               pv.kubernetes.io/bound-by-controller: yes\n               volume.beta.kubernetes.io/storage-provisioner: rook-ceph.rbd.csi.ceph.com\nFinalizers:    [kubernetes.io/pvc-protection]\nCapacity:      1Gi\nAccess Modes:  RWO\nVolumeMode:    Filesystem\nEvents:        &lt;none&gt;\n</code></pre></p> <p>In case of connection issues, inspect the Pod description for the    volume information:    <pre><code>kubectl describe pod &lt;crashloopbackoff-pod-name&gt;\n</code></pre></p> <p>Example of system response:    <pre><code>...\nEvents:\n  FirstSeen LastSeen Count From    SubObjectPath Type     Reason           Message\n  --------- -------- ----- ----    ------------- -------- ------           -------\n  1h        1h       3     default-scheduler     Warning  FailedScheduling PersistentVolumeClaim is not bound: \"app-test\" (repeated 2 times)\n  1h        35s      36    kubelet, 172.17.8.101 Warning  FailedMount      Unable to mount volumes for pod \"wordpress-mysql-918363043-50pjr_default(08d14e75-bd99-11e7-bc4c-001c428b9fc8)\": timeout expired waiting for volumes to attach/mount for pod \"default\"/\"wordpress-mysql-918363043-50pjr\". list of unattached/unmounted volumes=[mysql-persistent-storage]\n  1h        35s      36    kubelet, 172.17.8.101 Warning  FailedSync       Error syncing pod\n</code></pre></p> </li> </ol> </li> <li> <p>Verify that the CSI provisioner plugins started properly and are in     the <code>Running</code> status:</p> <ol> <li>Obtain the list of CSI provisioner plugins:    <pre><code>kubectl -n rook-ceph get pod -l app=csi-rbdplugin-provisioner\n</code></pre></li> <li>Verify the logs of the required CSI provisioner:    <pre><code>kubectl logs -n rook-ceph &lt;csi-provisioner-plugin-name&gt; csi-provisioner\n</code></pre></li> </ol> </li> </ol> </li> </ol>"},{"location":"ops-guide/verify-ceph/verify-rook-discover/","title":"Verify Rook Discover","text":""},{"location":"ops-guide/verify-ceph/verify-rook-discover/#verify-rook-discover","title":"Verify rook-discover","text":"<p>To ensure that <code>rook-discover</code> is running properly, verify if the <code>local-device</code> configmap has been created for each Ceph node specified in the cluster configuration:</p> <ol> <li> <p>Obtain the list of local devices:    <pre><code>kubectl get configmap -n rook-ceph | grep local-device\n</code></pre></p> <p>Example of a system response:  <pre><code>local-device-01      1      30m\nlocal-device-02      1      29m\nlocal-device-03      1      30m\n</code></pre></p> </li> <li> <p>Verify that each device from the list contains information about    available devices for the Ceph node deployment:    <pre><code>kubectl describe configmap local-device-01 -n rook-ceph\n</code></pre></p> <p>Example of a positive system response:  <pre><code>Name:         local-device-01\nNamespace:    rook-ceph\nLabels:       app=rook-discover\n              rook.io/node=01\nAnnotations:  &lt;none&gt;\n\nData\n====\ndevices:\n----\n[{\"name\":\"vdd\",\"parent\":\"\",\"hasChildren\":false,\"devLinks\":\"/dev/disk/by-id/virtio-41d72dac-c0ff-4f24-b /dev/disk/by-path/virtio-pci-0000:00:09.0\",\"size\":32212254720,\"uuid\":\"27e9cf64-85f4-48e7-8862-faa7270202ed\",\"serial\":\"41d72dac-c0ff-4f24-b\",\"type\":\"disk\",\"rotational\":true,\"readOnly\":false,\"Partitions\":null,\"filesystem\":\"\",\"vendor\":\"\",\"model\":\"\",\"wwn\":\"\",\"wwnVendorExtension\":\"\",\"empty\":true,\"cephVolumeData\":\"{\\\"path\\\":\\\"/dev/vdd\\\",\\\"available\\\":true,\\\"rejected_reasons\\\":[],\\\"sys_api\\\":{\\\"size\\\":32212254720.0,\\\"scheduler_mode\\\":\\\"none\\\",\\\"rotational\\\":\\\"1\\\",\\\"vendor\\\":\\\"0x1af4\\\",\\\"human_readable_size\\\":\\\"30.00 GB\\\",\\\"sectors\\\":0,\\\"sas_device_handle\\\":\\\"\\\",\\\"rev\\\":\\\"\\\",\\\"sas_address\\\":\\\"\\\",\\\"locked\\\":0,\\\"sectorsize\\\":\\\"512\\\",\\\"removable\\\":\\\"0\\\",\\\"path\\\":\\\"/dev/vdd\\\",\\\"support_discard\\\":\\\"0\\\",\\\"model\\\":\\\"\\\",\\\"ro\\\":\\\"0\\\",\\\"nr_requests\\\":\\\"128\\\",\\\"partitions\\\":{}},\\\"lvs\\\":[]}\",\"label\":\"\"},{\"name\":\"vdb\",\"parent\":\"\",\"hasChildren\":false,\"devLinks\":\"/dev/disk/by-path/virtio-pci-0000:00:07.0\",\"size\":67108864,\"uuid\":\"988692e5-94ac-4c9a-bc48-7b057dd94fa4\",\"serial\":\"\",\"type\":\"disk\",\"rotational\":true,\"readOnly\":false,\"Partitions\":null,\"filesystem\":\"\",\"vendor\":\"\",\"model\":\"\",\"wwn\":\"\",\"wwnVendorExtension\":\"\",\"empty\":true,\"cephVolumeData\":\"{\\\"path\\\":\\\"/dev/vdb\\\",\\\"available\\\":false,\\\"rejected_reasons\\\":[\\\"Insufficient space (\\\\u003c5GB)\\\"],\\\"sys_api\\\":{\\\"size\\\":67108864.0,\\\"scheduler_mode\\\":\\\"none\\\",\\\"rotational\\\":\\\"1\\\",\\\"vendor\\\":\\\"0x1af4\\\",\\\"human_readable_size\\\":\\\"64.00 MB\\\",\\\"sectors\\\":0,\\\"sas_device_handle\\\":\\\"\\\",\\\"rev\\\":\\\"\\\",\\\"sas_address\\\":\\\"\\\",\\\"locked\\\":0,\\\"sectorsize\\\":\\\"512\\\",\\\"removable\\\":\\\"0\\\",\\\"path\\\":\\\"/dev/vdb\\\",\\\"support_discard\\\":\\\"0\\\",\\\"model\\\":\\\"\\\",\\\"ro\\\":\\\"0\\\",\\\"nr_requests\\\":\\\"128\\\",\\\"partitions\\\":{}},\\\"lvs\\\":[]}\",\"label\":\"\"},{\"name\":\"vdc\",\"parent\":\"\",\"hasChildren\":false,\"devLinks\":\"/dev/disk/by-id/virtio-e8fdba13-e24b-41f0-9 /dev/disk/by-path/virtio-pci-0000:00:08.0\",\"size\":32212254720,\"uuid\":\"190a50e7-bc79-43a9-a6e6-81b173cd2e0c\",\"serial\":\"e8fdba13-e24b-41f0-9\",\"type\":\"disk\",\"rotational\":true,\"readOnly\":false,\"Partitions\":null,\"filesystem\":\"\",\"vendor\":\"\",\"model\":\"\",\"wwn\":\"\",\"wwnVendorExtension\":\"\",\"empty\":true,\"cephVolumeData\":\"{\\\"path\\\":\\\"/dev/vdc\\\",\\\"available\\\":true,\\\"rejected_reasons\\\":[],\\\"sys_api\\\":{\\\"size\\\":32212254720.0,\\\"scheduler_mode\\\":\\\"none\\\",\\\"rotational\\\":\\\"1\\\",\\\"vendor\\\":\\\"0x1af4\\\",\\\"human_readable_size\\\":\\\"30.00 GB\\\",\\\"sectors\\\":0,\\\"sas_device_handle\\\":\\\"\\\",\\\"rev\\\":\\\"\\\",\\\"sas_address\\\":\\\"\\\",\\\"locked\\\":0,\\\"sectorsize\\\":\\\"512\\\",\\\"removable\\\":\\\"0\\\",\\\"path\\\":\\\"/dev/vdc\\\",\\\"support_discard\\\":\\\"0\\\",\\\"model\\\":\\\"\\\",\\\"ro\\\":\\\"0\\\",\\\"nr_requests\\\":\\\"128\\\",\\\"partitions\\\":{}},\\\"lvs\\\":[]}\",\"label\":\"\"}]\n</code></pre></p> </li> </ol>"},{"location":"quick-start/installation/","title":"Installation Guide","text":"<p>This section provides instructions on how to install Pelagia that will deploy a Ceph cluster managed by Rook on a Kubernetes cluster, for example, deployed with k0s.</p>"},{"location":"quick-start/installation/#requirements-and-prerequisites","title":"Requirements and prerequisites","text":"<ul> <li>A Kubernetes cluster, for example, deployed with k0s.</li> <li>Enough resources for a Rook installation. For details, see   Rook prerequisites.</li> <li>Enough nodes in the cluster to run Ceph control and data daemons. Nodes must have at least   4 GB of RAM for each Ceph daemon (monitors, managers, metadata servers, osd, rgw).   The minimum recommended requirements are 6 nodes: 3 nodes for control daemons   (Ceph Managers and Ceph Monitors) and 3 nodes for data daemons (Ceph OSDs). For details about   hardware recommendations, see   Ceph documentation: Hardware recommendations.</li> <li>At least one disk per data node to be used as Ceph OSD. Miminal disk size is <code>5G</code> but Mirantis recommends   using disks with at least <code>100G</code> size.</li> </ul>"},{"location":"quick-start/installation/#installation","title":"Installation","text":"<p>To install Pelagia, use the Helm chart provided in the repository:</p> <pre><code>export PELAGIA_VERSION=\"&lt;version&gt;\"\nhelm upgrade --install pelagia-ceph oci://registry.mirantis.com/pelagia/pelagia-ceph --version ${PELAGIA_VERSION} -n pelagia --create-namespace\n</code></pre> <p>Substitute <code>&lt;version&gt;</code> with the latest stable version of Pelagia Helm chart.</p> <p>This command installs Pelagia controllers in the <code>pelagia</code> namespace. If the namespace does not exist, Helm creates it. As a result, the following controllers appear in <code>pelagia</code> namespace:</p> <pre><code>$ kubectl -n pelagia get pods\n\nNAME                                             READY   STATUS    RESTARTS   AGE\npelagia-deployment-controller-5bcb5c6464-7d8qf   2/2     Running   0          3m52s\npelagia-deployment-controller-5bcb5c6464-n2vzt   2/2     Running   0          3m52s\npelagia-deployment-controller-5bcb5c6464-vvzv2   2/2     Running   0          3m52s\npelagia-lcm-controller-6f858b6c5-4bqqr           3/3     Running   0          3m52s\npelagia-lcm-controller-6f858b6c5-6thhn           3/3     Running   0          3m52s\npelagia-lcm-controller-6f858b6c5-whq7x           3/3     Running   0          3m52s\n</code></pre> <p>Also, Pelagia Helm chart deploys the following Rook components in the <code>rook-ceph</code> namespace: <pre><code>$ kubectl -n rook-ceph get pods\n\nNAME                                  READY   STATUS    RESTARTS   AGE\nrook-ceph-operator-8495877b67-m7lf5   1/1     Running   0          4m13s\n</code></pre></p> <p>Currently, Pelagia Deployment Controller does not support integration with the existing Rook Ceph cluster, only Pelagia Lifecycle Management Controller does. To install Pelagia in LCM-only mode, please refer to LCM-only Installation Guide.</p>"},{"location":"quick-start/installation/#post-installation","title":"Post-installation","text":"<p>After the installation, you can deploy a Ceph cluster using the Pelagia <code>CephDeployment</code> custom resource. It will create Rook resources for Rook Operator to deploy a Ceph cluster. After Rook Operator deploys Ceph cluster, it could be managed by <code>CephDeployment</code> resource.</p> Simple example of a `CephDeployment` resource <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephDeployment\nmetadata:\n  name: pelagia-ceph\n  namespace: pelagia\nspec:\n  dashboard: false\n  network:\n    publicNet: ${CEPH_PUBLIC_NET}\n    clusterNet: ${CEPH_CLUSTER_NET}\n  nodes:\n  - name: ${CEPH_NODE_CP_0}\n    roles: [ \"mon\", \"mgr\", \"mds\" ]\n  - name: ${CEPH_NODE_CP_1}\n    roles: [ \"mon\", \"mgr\", \"mds\" ]\n  - name: ${CEPH_NODE_CP_2}\n    roles: [ \"mon\", \"mgr\", \"mds\" ]\n  - name: ${CEPH_NODE_WORKER_0}\n    devices:\n    - config:\n        deviceClass: hdd\n      fullPath: /dev/disk/by-id/${CEPH_OSD_DEVICE_0}\n  - name: ${CEPH_NODE_WORKER_1}\n    devices:\n    - config:\n        deviceClass: hdd\n      fullPath: /dev/disk/by-id/${CEPH_OSD_DEVICE_1}\n  - name: ${CEPH_NODE_WORKER_2}\n    devices:\n    - config:\n        deviceClass: hdd\n      fullPath: /dev/disk/by-id/${CEPH_OSD_DEVICE_2}\n  pools:\n  - name: kubernetes\n    deviceClass: hdd\n    default: true\n    replicated:\n      size: 3\n  objectStorage:\n    rgw:\n      name: rgw-store\n      dataPool:\n        deviceClass: hdd\n        replicated:\n          size: 3\n      metadataPool:\n        deviceClass: hdd\n        replicated:\n          size: 3\n      gateway:\n        allNodes: false\n        instances: 3\n        port: 8081\n        securePort: 8443\n      preservePoolsOnDelete: false\n  sharedFilesystem:\n    cephFS:\n    - name: cephfs-store\n      dataPools:\n      - name: cephfs-pool-1\n        deviceClass: hdd\n        replicated:\n          size: 3\n      metadataPool:\n        deviceClass: hdd\n        replicated:\n          size: 3\n      metadataServer:\n        activeCount: 1\n        activeStandby: false\n</code></pre> <p>The example above contains <code>3</code> control plane nodes and <code>3</code> worker nodes in a Ceph cluster. You can change the number of nodes and their roles according to your needs. As a result, you will have a Ceph cluster with RBD pool, CephFS filesystem, and RGW object storage deployed in your Kubernetes cluster. <pre><code>$ kubectl -n rook-ceph get pods\n\nNAME                                                              READY   STATUS      RESTARTS   AGE\ncsi-cephfsplugin-lgm9v                                            2/2     Running     0          36m\ncsi-cephfsplugin-lr26d                                            2/2     Running     0          36m\ncsi-cephfsplugin-provisioner-766fddb5c8-rm4r7                     5/5     Running     0          37m\ncsi-cephfsplugin-provisioner-766fddb5c8-wxtpl                     5/5     Running     0          37m\ncsi-cephfsplugin-s67pp                                            2/2     Running     0          36m\ncsi-rbdplugin-provisioner-649fb94cf4-fr8nk                        5/5     Running     0          37m\ncsi-rbdplugin-provisioner-649fb94cf4-vcj45                        5/5     Running     0          37m\ncsi-rbdplugin-rjnx4                                               2/2     Running     0          36m\ncsi-rbdplugin-xb8qg                                               2/2     Running     0          36m\ncsi-rbdplugin-z48m9                                               2/2     Running     0          36m\npelagia-ceph-toolbox-8688f94564-xmtjn                             1/1     Running     0          61m\nrook-ceph-crashcollector-009b4c37cd6752487fafef7c44bc6a4d-mxd5c   1/1     Running     0          56m\nrook-ceph-crashcollector-2df4fa7c50e45baccd8ffa967ce36ec6-flxjn   1/1     Running     0          107s\nrook-ceph-crashcollector-7a0999af02371dae358b7fcd28b3ae35-lp9jx   1/1     Running     0          111s\nrook-ceph-crashcollector-8490eae0de0b41f6fcd6a61239515f12-47h5d   1/1     Running     0          56m\nrook-ceph-crashcollector-cac4771a1b293acb738842052eb60bc2-qxbvf   1/1     Running     0          42s\nrook-ceph-crashcollector-d7732aeb2025beef8dda88e8195f704b-fgtsh   1/1     Running     0          104s\nrook-ceph-exporter-009b4c37cd6752487fafef7c44bc6a4d-995c84hb26h   1/1     Running     0          56m\nrook-ceph-exporter-2df4fa7c50e45baccd8ffa967ce36ec6-7cf86d8zb24   1/1     Running     0          107s\nrook-ceph-exporter-7a0999af02371dae358b7fcd28b3ae35-fb667bd7kc2   1/1     Running     0          111s\nrook-ceph-exporter-8490eae0de0b41f6fcd6a61239515f12-676975jxmvc   1/1     Running     0          56m\nrook-ceph-exporter-cac4771a1b293acb738842052eb60bc2-8c6f56988l8   1/1     Running     0          40s\nrook-ceph-exporter-d7732aeb2025beef8dda88e8195f704b-5ff7ccbmpzd   1/1     Running     0          104s\nrook-ceph-mds-cephfs-store-a-69b45f8c48-4s29t                     1/1     Running     0          56m\nrook-ceph-mds-cephfs-store-b-5cd87b8765-8mqdb                     1/1     Running     0          56m\nrook-ceph-mgr-a-6bf4b5b794-fhcmt                                  2/2     Running     0          57m\nrook-ceph-mgr-b-56c698c596-gvtnd                                  2/2     Running     0          57m\nrook-ceph-mon-a-85d5fbd55-6pchz                                   1/1     Running     0          58m\nrook-ceph-mon-b-5b4668f949-s898d                                  1/1     Running     0          57m\nrook-ceph-mon-c-64b6fb474b-d5zwd                                  1/1     Running     0          57m\nrook-ceph-operator-8495877b67-48ztd                               1/1     Running     0          2m55s\nrook-ceph-osd-0-5884bc8bcd-b4pgw                                  1/1     Running     0          111s\nrook-ceph-osd-1-59844c4c76-t8kxp                                  1/1     Running     0          107s\nrook-ceph-osd-2-54956b47f4-bdjgs                                  1/1     Running     0          104s\nrook-ceph-osd-prepare-2df4fa7c50e45baccd8ffa967ce36ec6-njt49      0/1     Completed   0          2m4s\nrook-ceph-osd-prepare-7a0999af02371dae358b7fcd28b3ae35-kllhb      0/1     Completed   0          2m8s\nrook-ceph-osd-prepare-d7732aeb2025beef8dda88e8195f704b-7548x      0/1     Completed   0          2m\nrook-ceph-rgw-rgw-store-a-5cbb8785ff-5bcvw                        1/1     Running     0          42s\nrook-ceph-rgw-rgw-store-a-5cbb8785ff-9dzz4                        1/1     Running     0          42s\nrook-discover-27b74                                               1/1     Running     0          66m\nrook-discover-2t882                                               1/1     Running     0          66m\nrook-discover-nqrk4                                               1/1     Running     0          66m\n</code></pre></p> <p>Pelagia deploys <code>StorageClass</code> resources for each Ceph pool to be used in workloads: <pre><code>$ kubectl get sc\n\nNAME                         PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\ncephfs-store-cephfs-pool-1   rook-ceph.cephfs.csi.ceph.com   Delete          Immediate           true                   62m\nkubernetes-hdd (default)     rook-ceph.rbd.csi.ceph.com      Delete          Immediate           false                  62m\nrgw-storage-class            rook-ceph.ceph.rook.io/bucket   Delete          Immediate           false                  65m\n</code></pre></p>"},{"location":"quick-start/installation/#see-also","title":"See also","text":"<p>For the Pelagia architecture and overview, refer to the Architecture Guide.</p> <p>For the detailed Ceph cluster configuration and day-2 operations, refer to the Architecture: CephDeployment.</p> <p>For the Ceph OSD automated lifecycle management, refer to Ops Guide: Automated Lifecycle Management.</p>"},{"location":"quick-start/lcm-installation/","title":"LCM-only Installation Guide","text":"<p>This section provides instructions on how to install Pelagia in lifecycle-management-only (lcm-only) mode. This mode allows you to automatically remove Rook Ceph OSD disks and nodes in your Kubernetes cluster.</p>"},{"location":"quick-start/lcm-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Kubernetes cluster, for example, deployed with k0s.</li> <li>A deployed Ceph cluster managed by Rook.</li> </ul>"},{"location":"quick-start/lcm-installation/#installation","title":"Installation","text":"<p>To install Pelagia in lcm-only mode, use the Helm chart provided in the repository:</p> <pre><code>export PELAGIA_VERSION=\"&lt;version&gt;\"\nhelm upgrade --install pelagia-ceph oci://registry.mirantis.com/pelagia/pelagia-ceph --version ${PELAGIA_VERSION} --set cephDeployment.enabled=false,rookConfig.configureRook=false -n pelagia --create-namespace\n</code></pre> <p>Substitute <code>&lt;version&gt;</code> with the latest stable version of Pelagia Helm chart.</p> <p>This command installs Pelagia LCM controllers in the <code>pelagia</code> namespace. If the namespace does not exist, Helm will create it. As a result, the following controllers appear in the <code>pelagia</code> namespace:</p> <pre><code>$ kubectl -n pelagia get pods\n\nNAME                                     READY   STATUS    RESTARTS   AGE\npelagia-lcm-controller-6f858b6c5-4bqqr   3/3     Running   0          9m40s\npelagia-lcm-controller-6f858b6c5-6thhn   3/3     Running   0          9m40s\npelagia-lcm-controller-6f858b6c5-whq7x   3/3     Running   0          9m40s\n</code></pre> <p>The installation command above installs Pelagia in lcm-only mode for an existing Rook Ceph cluster placed in the <code>rook-ceph</code> namespace. If you want to manage a Rook Ceph cluster in a different namespace, set the <code>rookConfig.rookNamespace</code> chart value. For example: <pre><code>export PELAGIA_VERSION=\"&lt;version&gt;\"\nhelm upgrade --install pelagia-ceph oci://registry.mirantis.com/pelagia/pelagia-ceph --version ${PELAGIA_VERSION} --set cephDeployment.enabled=false,rookConfig.configureRook=false,rookConfig.rookNamespace=new-rook-ceph -n pelagia --create-namespace\n</code></pre></p> <p>Substitute <code>&lt;version&gt;</code> with the latest stable version of Pelagia Helm chart.</p>"},{"location":"quick-start/lcm-installation/#post-installation","title":"Post-installation","text":"<p>After the installation, create empty <code>CephDeploymentHealth</code> custom resource. It stores Ceph cluster state and Rook API status.</p> <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephDeploymentHealth\nmetadata:\n  name: pelagia-ceph\n  namespace: pelagia\nstatus: {}\n</code></pre> <p>Now you can manage Rook Ceph OSD disks and nodes using the Pelagia <code>CephOsdRemoveTask</code> custom resource. For example:</p> <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephOsdRemoveTask\nmetadata:\n    name: remove-osd-4\n    namespace: pelagia\nspec:\n  nodes:\n    storage-worker-1:\n      cleanupByOsd:\n      - id: 4\n</code></pre> <p>This example will dry-run the removal of the OSD with ID 4 on the <code>storage-worker-1</code> node. After the dry-run, the <code>status</code> field of the <code>CephOsdRemoveTask</code> resource will be updated with the results of the dry-run.</p> <p>To trigger the OSD removal, set the <code>spec.approve</code> field to <code>true</code> and apply the resource again. For example:</p> <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephOsdRemoveTask\nmetadata:\n    name: remove-osd-4\n    namespace: pelagia\nspec:\n  approve: true\n  nodes:\n    storage-worker-1:\n      cleanupByOsd:\n      - id: 4\n</code></pre> <p>This action triggers the removal of the OSD with ID 4 on the <code>storage-worker-1</code> node.</p> <p>The <code>CephOsdRemoveTask</code> workflow is as follows:</p> <ol> <li>The OSD daemon is stopped and data is rebalanced out from the OSD.</li> <li>The OSD disk is cleaned up on the node.</li> <li>The OSD daemon is removed from the Ceph cluster.</li> </ol>"},{"location":"quick-start/lcm-installation/#see-also","title":"See also","text":"<p>For the Pelagia architecture and overview, refer to the Architecture Guide.</p> <p>For the detailed OSD automated lifecycle management, refer to Ops Guide: Automated Lifecycle Management.</p>"},{"location":"quick-start/upgrade/","title":"Upgrade Pelagia Guide","text":"<p>This section provides instructions on how to upgrade Pelagia on an existing Kubernetes cluster.</p>"},{"location":"quick-start/upgrade/#upgrade-chart","title":"Upgrade chart","text":"<p>To upgrade Pelagia, use a new Helm chart version provided in the repository:</p> <pre><code>helm upgrade --install pelagia-ceph oci://registry.mirantis.com/pelagia/pelagia-ceph --version &lt;new-version&gt; -n pelagia\n</code></pre> <p>This command upgrades Pelagia controllers in the <code>pelagia</code> namespace.</p> <p>Besides its own controllers, Pelagia can deliver updated Rook and Ceph manifests along with images. If <code>cephRelease</code> is not pinned in Pelagia values, Pelagia will automatically update a Ceph version if a new version is available.</p> <p>To pin the Ceph version, specify it in the <code>cephRelease</code> field of the Pelagia values file. For example:</p> <pre><code>helm upgrade --install pelagia-ceph oci://registry.mirantis.com/pelagia/pelagia-ceph --version &lt;new-version&gt; -n pelagia --set cephRelease=squid\n</code></pre> <p>However, Mirantis does not recommend pinning the Ceph version to ensure you obtain important updates and bug fixes.</p>"},{"location":"troubleshoot/ceph-disaster-recover/","title":"Ceph Cluster disaster recovery","text":""},{"location":"troubleshoot/ceph-disaster-recover/#ceph-disaster-recovery","title":"Ceph disaster recovery","text":"<p>This section describes how to recover a failed or accidentally removed Ceph cluster in the following cases:</p> <ul> <li>If Pelagia Controller underlying a running Rook Ceph cluster has failed, and you   want to install a new Pelagia Helm release and recover the failed   Ceph cluster onto the new Pelagia Controllers.</li> <li>To migrate the data of an existing Ceph cluster to a new deployment in case   downtime can be tolerated.</li> </ul> <p>Consider the common state of a failed or removed Ceph cluster:</p> <ul> <li>The Rook namespace does not contain pods, or they are in the <code>Terminating</code> state.</li> <li>The Rook or/and Pelagia namespaces are in the <code>Terminating</code> state.</li> <li>Pelagia Helm release is failed or absent.</li> <li>The Rook <code>CephCluster</code>, <code>CephBlockPool</code>, <code>CephObjectStore</code> CRs in the   Rook namespace cannot be found or have the <code>deletionTimestamp</code> parameter in the <code>metadata</code> section.</li> </ul> <p>Note</p> <p>Prior to recovering the Ceph cluster, verify that your deployment meets the following prerequisites:</p> <ol> <li>The Ceph cluster <code>fsid</code> exists.</li> <li>The Ceph cluster Monitor keyrings exist.</li> <li>The Ceph cluster devices exist and include the data previously handled by    Ceph OSDs.</li> </ol>"},{"location":"troubleshoot/ceph-disaster-recover/#ceph-cluster-recovery-workflow","title":"Ceph cluster recovery workflow","text":"<ol> <li>Create a backup of the remaining data and resources.</li> <li>Clean up the failed or removed Pelagia Helm release.</li> <li>Deploy a new Pelagia Helm release with the previously used <code>CephDeployment</code> custom resource (CR)    and one Ceph Monitor.</li> <li>Replace the <code>ceph-mon</code> data with the old cluster data.</li> <li>Replace <code>fsid</code> in <code>secrets/rook-ceph-mon</code> with the old one.</li> <li>Fix the Monitor map in the <code>ceph-mon</code> database.</li> <li>Fix the Ceph Monitor authentication key and disable authentication.</li> <li>Start the restored cluster and inspect the recovery.</li> <li>Fix the admin authentication key and enable authentication.</li> <li>Restart the cluster.</li> </ol>"},{"location":"troubleshoot/ceph-disaster-recover/#recover-a-failed-or-removed-ceph-cluster","title":"Recover a failed or removed Ceph cluster","text":"<ol> <li> <p>Back up the remaining resources. Skip the commands for the resources that    have already been removed:    <pre><code>kubectl -n rook-ceph get cephcluster &lt;clusterName&gt; -o yaml &gt; backup/cephcluster.yaml\n# perform this for each cephblockpool\nkubectl -n rook-ceph get cephblockpool &lt;cephBlockPool-i&gt; -o yaml &gt; backup/&lt;cephBlockPool-i&gt;.yaml\n# perform this for each client\nkubectl -n rook-ceph get cephclient &lt;cephclient-i&gt; -o yaml &gt; backup/&lt;cephclient-i&gt;.yaml\nkubectl -n rook-ceph get cephobjectstore &lt;cephObjectStoreName&gt; -o yaml &gt; backup/&lt;cephObjectStoreName&gt;.yaml\nkubectl -n rook-ceph get cephfilesystem &lt;cephfilesystemName&gt; -o yaml &gt; backup/&lt;cephfilesystemName&gt;.yaml\n# perform this for each secret\nkubectl -n rook-ceph get secret &lt;secret-i&gt; -o yaml &gt; backup/&lt;secret-i&gt;.yaml\n# perform this for each configMap\nkubectl -n rook-ceph get cm &lt;cm-i&gt; -o yaml &gt; backup/&lt;cm-i&gt;.yaml\n</code></pre></p> </li> <li> <p>SSH to each node where the Ceph Monitors or Ceph OSDs were placed before the    failure and back up the valuable data:    <pre><code>mv /var/lib/rook /var/lib/rook.backup\nmv /etc/ceph /etc/ceph.backup\nmv /etc/rook /etc/rook.backup\n</code></pre></p> <p>Once done, close the SSH connection.</p> </li> <li> <p>Clean up the previous installation of Pelagia Helm release. For details of Rook cleanup, see    Rook documentation: Cleaning up a cluster.</p> <ol> <li> <p>Delete all deployments in the Pelagia namespace:     <pre><code>kubectl -n pelagia delete deployment --all\n</code></pre></p> </li> <li> <p>Delete all deployments, DaemonSets, and jobs from the Rook namespace, if any:     <pre><code>kubectl -n rook-ceph delete deployment --all\nkubectl -n rook-ceph delete daemonset --all\nkubectl -n rook-ceph delete job --all\n</code></pre></p> </li> <li> <p>Edit the <code>CephDeployment</code>, <code>CephDeploymentHealth</code>, and <code>CephDeploymentSecret</code> CRs of the     Pelagia namespace and remove the <code>finalizer</code> parameter from the <code>metadata</code> section:     <pre><code>kubectl -n pelagia edit cephdpl\nkubectl -n pelagia edit cephdeploymenthealth\n</code></pre></p> </li> <li> <p>Edit the <code>CephCluster</code>, <code>CephBlockPool</code>, <code>CephClient</code>, and     <code>CephObjectStore</code> CRs of the Rook namespace and remove the     <code>finalizer</code> parameter from the <code>metadata</code> section:     <pre><code>kubectl -n rook-ceph edit cephclusters\nkubectl -n rook-ceph edit cephblockpools\nkubectl -n rook-ceph edit cephclients\nkubectl -n rook-ceph edit cephobjectstores\nkubectl -n rook-ceph edit cephobjectusers\n</code></pre></p> </li> <li> <p>Remove Pelagia Helm release:     <pre><code>helm uninstall &lt;releaseName&gt;\n</code></pre></p> <p>where <code>&lt;releaseName&gt;</code> is Pelagia Helm release, for example, <code>pelagia-ceph</code>.</p> </li> </ol> </li> <li> <p>Create the <code>CephDeployment</code> CR template and edit the roles of nodes. The entire    <code>nodes</code> spec must contain only one <code>mon</code> and one <code>mgr</code> role. Save the <code>CephDeployment</code>    template after editing:    <pre><code>apiVersion: lcm.mirantis.com/v1alpha1\nkind: CephDeployment\nmetadata:\n  name: pelagia-ceph\n  namespace: pelagia\nspec:\n  nodes:\n  - name: &lt;nodeName&gt;\n    roles:\n    - mon\n    - mgr\n</code></pre></p> <p>Substitute <code>&lt;nodeName&gt;</code> with node name of the node where monitor is placed.</p> </li> <li> <p>Install Pelagia Helm release:    <pre><code>helm upgrade --install pelagia-ceph oci://registry.mirantis.com/pelagia/pelagia-ceph --version &lt;version&gt; -n pelagia --create-namespace\n</code></pre></p> <p>where <code>&lt;version&gt;</code> is Pelagia Helm chart version, for example, <code>1.0.0</code>.</p> </li> <li> <p>Verify that the Pelagia Helm release is deployed:</p> <ol> <li> <p>Inspect the Rook Ceph Operator logs and wait until the orchestration has     settled:     <pre><code>kubectl -n rook-ceph logs -l app=rook-ceph-operator\n</code></pre></p> </li> <li> <p>Verify that the pods in the Rook namespace have <code>rook-ceph-mon-a</code>, <code>rook-ceph-mgr-a</code>,     and all the auxiliary pods are up and running, and no <code>rook-ceph-osd-ID-xxxxxx</code> are running:     <pre><code>kubectl -n rook-ceph get pod\n</code></pre></p> </li> <li> <p>Verify the Ceph state. The output must indicate that one <code>mon</code> and one     <code>mgr</code> are running, all Ceph OSDs are down, and all PGs are in the     <code>Unknown</code> state.     <pre><code>kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph -s\n</code></pre></p> <p>Note</p> <p>Rook should not start any Ceph OSD daemon because all devices belong to the old cluster that has a different <code>fsid</code>. To verify the Ceph OSD daemons, inspect the <code>osd-prepare</code> pods logs: <pre><code>kubectl -n rook-ceph logs -l app=rook-ceph-osd-prepare\n</code></pre></p> </li> </ol> </li> <li> <p>Connect to the terminal of the <code>rook-ceph-mon-a</code> pod:    <pre><code>kubectl -n rook-ceph exec -it deploy/rook-ceph-mon-a -- bash\n</code></pre></p> </li> <li> <p>Output the <code>keyring</code> file and save it for further usage:    <pre><code>cat /etc/ceph/keyring-store/keyring\nexit\n</code></pre></p> </li> <li> <p>Obtain and save the <code>nodeName</code> of <code>mon-a</code> for further usage:     <pre><code>kubectl -n rook-ceph get pod $(kubectl -n rook-ceph get pod \\\n-l app=rook-ceph-mon -o jsonpath='{.items[0].metadata.name}') -o jsonpath='{.spec.nodeName}'\n</code></pre></p> </li> <li> <p>Obtain and save the <code>DEPLOYMENT_CEPH_IMAGE</code> used in the Ceph cluster for further     usage:     <pre><code>kubectl -n pelagia get cm pelagia-lcmconfig -o jsonpath='{.data.DEPLOYMENT_CEPH_IMAGE}'\n</code></pre></p> </li> <li> <p>Stop all deployments in the Pelagia namespace:     <pre><code>kubectl -n pelagia scale deploy --all --replicas 0\n</code></pre></p> </li> <li> <p>Stop Rook Ceph Operator and scale the deployment replicas to <code>0</code>:     <pre><code>kubectl -n rook-ceph scale deploy rook-ceph-operator --replicas 0\n</code></pre></p> </li> <li> <p>Remove the Rook deployments generated with Rook Operator:     <pre><code>kubectl -n rook-ceph delete deploy -l app=rook-ceph-mon\nkubectl -n rook-ceph delete deploy -l app=rook-ceph-mgr\nkubectl -n rook-ceph delete deploy -l app=rook-ceph-osd\nkubectl -n rook-ceph delete deploy -l app=rook-ceph-crashcollector\n</code></pre></p> </li> <li> <p>Using the saved <code>nodeName</code>, SSH to the host where <code>rook-ceph-mon-a</code> in     the new Kubernetes cluster is placed and perform the following steps:</p> <ol> <li> <p>Remove <code>/var/lib/rook/mon-a</code> or copy it to another folder:      <pre><code>mv /var/lib/rook/mon-a /var/lib/rook/mon-a.new\n</code></pre></p> </li> <li> <p>Pick a healthy <code>rook-ceph-mon-ID</code> directory (<code>/var/lib/rook.backup/mon-ID</code>) in the previous backup, copy to      <code>/var/lib/rook/mon-a</code>:      <pre><code>cp -rp /var/lib/rook.backup/mon-&lt;ID&gt; /var/lib/rook/mon-a\n</code></pre></p> <p>Substitute <code>ID</code> with any healthy <code>mon</code> node ID of the old cluster.</p> </li> <li> <p>Replace <code>/var/lib/rook/mon-a/keyring</code> with the previously saved      keyring, preserving only the <code>[mon.]</code> section. Remove the      <code>[client.admin]</code> section.</p> </li> <li> <p>Run the <code>DEPLOYMENT_CEPH_IMAGE</code> Docker container using the previously saved      <code>DEPLOYMENT_CEPH_IMAGE</code> image:      <pre><code>docker run -it --rm -v /var/lib/rook:/var/lib/rook &lt;DEPLOYMENT_CEPH_IMAGE&gt; bash\n</code></pre></p> </li> <li> <p>Inside the container, create <code>/etc/ceph/ceph.conf</code> for a stable operation of <code>ceph-mon</code>:      <pre><code>touch /etc/ceph/ceph.conf\n</code></pre></p> </li> <li> <p>Change the directory to <code>/var/lib/rook</code> and edit <code>monmap</code> by      replacing the existing <code>mon</code> hosts with the new <code>mon-a</code> endpoints:      <pre><code>cd /var/lib/rook\nrm /var/lib/rook/mon-a/data/store.db/LOCK # Make sure the quorum lock file does not exist\nceph-mon --extract-monmap monmap --mon-data ./mon-a/data  # Extract monmap from old ceph-mon db and save as monmap\nmonmaptool --print monmap  # Print the monmap content, which reflects the old cluster ceph-mon configuration.\nmonmaptool --rm a monmap  # Delete `a` from monmap.\nmonmaptool --rm b monmap  # Repeat and delete `b` from monmap.\nmonmaptool --rm c monmap  # Repeat this pattern until all the old ceph-mons are removed and monmap is empty\nmonmaptool --addv a [v2:&lt;nodeIP&gt;:3300,v1:&lt;nodeIP&gt;:6789] monmap   # Replace it with the rook-ceph-mon-a address you obtained from the previous command.\nceph-mon --inject-monmap monmap --mon-data ./mon-a/data  # Replace monmap in ceph-mon db with our modified version.\nrm monmap\nexit\n</code></pre></p> <p>Substitute <code>&lt;nodeIP&gt;</code> with the IP address of the current <code>&lt;nodeName&gt;</code> node.</p> </li> <li> <p>Close the SSH connection.</p> </li> </ol> </li> <li> <p>Change <code>fsid</code> to the original one to run Rook as an old cluster:     <pre><code>kubectl -n rook-ceph edit secret/rook-ceph-mon\n</code></pre></p> <p>Note</p> <p>The <code>fsid</code> is <code>base64</code> encoded and must not contain a trailing carriage return. For example: <pre><code>echo -n a811f99a-d865-46b7-8f2c-f94c064e4356 | base64  # Replace with the fsid from the old cluster.\n</code></pre></p> </li> <li> <p>Disable authentication:</p> <ol> <li> <p>Open the <code>rook-config-override</code> ConfigMap for editing:      <pre><code>kubectl -n rook-ceph edit cm/rook-config-override\n</code></pre></p> </li> <li> <p>Add the following content:      <pre><code>data:\n  config: |\n    [global]\n    ...\n    auth cluster required = none\n    auth service required = none\n    auth client required = none\n    auth supported = none\n</code></pre></p> </li> </ol> </li> <li> <p>Start Rook Operator by scaling its deployment replicas to <code>1</code>:     <pre><code>kubectl -n rook-ceph scale deploy rook-ceph-operator --replicas 1\n</code></pre></p> </li> <li> <p>Inspect the Rook Operator logs and wait until the orchestration has settled:     <pre><code>kubectl -n rook-ceph logs -l app=rook-ceph-operator\n</code></pre></p> </li> <li> <p>Verify that the pods in the <code>rook-ceph</code> namespace have the     <code>rook-ceph-mon-a</code>, <code>rook-ceph-mgr-a</code>, and all the auxiliary pods are up     and running, and all <code>rook-ceph-osd-ID-xxxxxx</code> greater than zero are     running:     <pre><code>kubectl -n rook-ceph get pod\n</code></pre></p> </li> <li> <p>Verify the Ceph state. The output must indicate that one <code>mon</code>, one     <code>mgr</code>, and all Ceph OSDs must be up and running and all PGs are either in     the <code>Active</code> or <code>Degraded</code> state:     <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- ceph -s\n</code></pre></p> </li> <li> <p>Enter the <code>pelagia-ceph-toolbox</code> pod and import the authentication key:     <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- bash\nvi key\n[paste keyring content saved before, preserving only `[client admin]` section]\nceph auth import -i key\nrm key\nexit\n</code></pre></p> </li> <li> <p>Stop Rook Ceph Operator by scaling the deployment to <code>0</code> replicas:     <pre><code>kubectl -n rook-ceph scale deploy rook-ceph-operator --replicas 0\n</code></pre></p> </li> <li> <p>Re-enable authentication:</p> <ol> <li> <p>Open the <code>rook-config-override</code> ConfigMap for editing:      <pre><code>kubectl -n rook-ceph edit cm/rook-config-override\n</code></pre></p> </li> <li> <p>Remove the following content:      <pre><code>data:\n  config: |\n    [global]\n    ...\n    auth cluster required = none\n    auth service required = none\n    auth client required = none\n    auth supported = none\n</code></pre></p> </li> </ol> </li> <li> <p>Remove all Rook deployments generated with Rook Ceph Operator:     <pre><code>kubectl -n rook-ceph delete deploy -l app=rook-ceph-mon\nkubectl -n rook-ceph delete deploy -l app=rook-ceph-mgr\nkubectl -n rook-ceph delete deploy -l app=rook-ceph-osd\nkubectl -n rook-ceph delete deploy -l app=rook-ceph-crashcollector\n</code></pre></p> </li> <li> <p>Start Pelagia Controllers by scaling its deployment replicas to <code>1</code>:     <pre><code>kubectl -n pelagia scale deployment --all --replicas 1\n</code></pre></p> </li> <li> <p>Start Rook Ceph Operator by scaling its deployment replicas to <code>1</code>:     <pre><code>kubectl -n rook-ceph scale deploy rook-ceph-operator --replicas 1\n</code></pre></p> </li> <li> <p>Inspect the Rook Ceph Operator logs and wait until the orchestration has settled:     <pre><code>kubectl -n rook-ceph logs -l app=rook-ceph-operator\n</code></pre></p> </li> <li> <p>Verify that the pods in the Rook namespace have the     <code>rook-ceph-mon-a</code>, <code>rook-ceph-mgr-a</code>, and all the auxiliary pods are up     and running, and all <code>rook-ceph-osd-ID-xxxxxx</code> greater than zero are     running:     <pre><code>kubectl -n rook-ceph get pod\n</code></pre></p> </li> <li> <p>Verify the Ceph state. The output must indicate that one <code>mon</code>, one     <code>mgr</code>, and all Ceph OSDs must be up and running and the overall stored     data size equals to the old cluster data size.     <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- ceph -s\n</code></pre></p> </li> <li> <p>Edit the <code>CephDeployment</code> CR and add two more <code>mon</code> and <code>mgr</code> roles to the     corresponding nodes:     <pre><code>kubectl -n pelagia edit cephdpl\n</code></pre></p> </li> <li> <p>Inspect the Rook namespace and wait until all Ceph Monitors are in the     <code>Running</code> state:     <pre><code>kubectl -n rook-ceph get pod -l app=rook-ceph-mon\n</code></pre></p> </li> <li> <p>Verify the Ceph state. The output must indicate that three <code>mon</code> (three in     quorum), one <code>mgr</code>, and all Ceph OSDs must be up and running and the     overall stored data size equals to the old cluster data size.     <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- ceph -s\n</code></pre></p> </li> </ol> <p>Once done, the data from the failed or removed Ceph cluster is restored and ready to use.</p>"},{"location":"troubleshoot/ceph-exporter-crash/","title":"Rook Ceph exporter crash","text":""},{"location":"troubleshoot/ceph-exporter-crash/#the-ceph-exporter-pods-are-present-in-the-ceph-crash-list","title":"The ceph-exporter pods are present in the Ceph crash list","text":"<p>After a managed cluster update, the <code>ceph-exporter</code> pods may be present in the ceph crash ls or ceph health detail list while <code>rook-ceph-exporter</code> attempts to obtain the port that is still in use. For example: <pre><code>$ ceph health detail\n\nHEALTH_WARN 1 daemons have recently crashed\n[WRN] RECENT_CRASH: 1 daemons have recently crashed\n    client.ceph-exporter crashed on host kaas-node-b59f5e63-2bfd-43aa-bc80-42116d71188c at 2024-10-01T16:43:31.311563Z\n</code></pre></p> <p>The issue does not block the managed cluster update. Once the port becomes available, <code>rook-ceph-exporter</code> obtains the port and the issue disappears.</p> <p>To apply the issue resolution, run the following command to remove <code>ceph-exporter</code> pods from the Ceph crash list: <pre><code>ceph crash archive-all\n</code></pre></p>"},{"location":"troubleshoot/ceph-mon-recover/","title":"Ceph Monitor disaster recovery","text":""},{"location":"troubleshoot/ceph-mon-recover/#ceph-monitors-recovery","title":"Ceph Monitors recovery","text":"<p>This section describes how to recover failed Ceph Monitors of an existing Ceph cluster in the following state:</p> <ul> <li>The Ceph cluster contains failed Ceph Monitors that cannot start and hang   in the <code>Error</code> or <code>CrashLoopBackOff</code> state.</li> <li> <p>The logs of the failed Ceph Monitor pods contain the following lines:   <pre><code>mon.g does not exist in monmap, will attempt to join an existing cluster\n...\nmon.g@-1(???) e11 not in monmap and have been in a quorum before; must have been removed\nmon.g@-1(???) e11 commit suicide!\n</code></pre></p> </li> <li> <p>Rook Ceph Operator failover procedure produces new Ceph Monitor Pods, which are failing to   <code>CrashLoopBackOff</code> status.</p> </li> <li>The Ceph cluster contains at least one <code>Running</code> Ceph Monitor and the   <code>ceph -s</code> command outputs one healthy <code>mon</code> and one healthy   <code>mgr</code> instance.</li> </ul> <p>Perform the following steps for all failed Ceph Monitors at a time if not stated otherwise.</p> <p>To recover failed Ceph Monitors:</p> <ol> <li> <p>Scale the Rook Ceph Operator deployment down to <code>0</code> replicas:    <pre><code>kubectl -n rook-ceph scale deploy rook-ceph-operator --replicas 0\n</code></pre></p> </li> <li> <p>Delete all failed Ceph Monitor deployments:</p> <ol> <li> <p>Identify the Ceph Monitor pods in the <code>Error</code> or <code>CrashLookBackOff</code>     state:     <pre><code>kubectl -n rook-ceph get pod -l 'app in (rook-ceph-mon,rook-ceph-mon-canary)'\n</code></pre></p> </li> <li> <p>Verify that the affected pods contain the failure logs described above:     <pre><code>kubectl -n rook-ceph logs &lt;failedMonPodName&gt;\n</code></pre></p> <p>Substitute <code>&lt;failedMonPodName&gt;</code> with the Ceph Monitor pod name. For   example, <code>rook-ceph-mon-g-845d44b9c6-fjc5d</code>.</p> </li> <li> <p>Save the identifying letters of failed Ceph Monitors for further usage.     For example, <code>f</code>, <code>e</code>, and so on.</p> </li> <li> <p>Delete all corresponding deployments of these pods:</p> <ol> <li> <p>Identify the affected Ceph Monitor pod deployments:      <pre><code>kubectl -n rook-ceph get deploy -l 'app in (rook-ceph-mon,rook-ceph-mon-canary)'\n</code></pre></p> </li> <li> <p>Delete the affected Ceph Monitor pod deployments. For example, if the      Ceph cluster has the <code>rook-ceph-mon-c-845d44b9c6-fjc5d</code> pod in the      <code>CrashLoopBackOff</code> state, remove the corresponding <code>rook-ceph-mon-c</code>:      <pre><code>kubectl -n rook-ceph delete deploy rook-ceph-mon-c\n</code></pre></p> <p>Canary <code>mon</code> deployments have the suffix <code>-canary</code>.</p> </li> </ol> </li> </ol> </li> <li> <p>Remove all corresponding entries of Ceph Monitors from the MON map:</p> <ol> <li> <p>Enter the <code>pelagia-ceph-toolbox</code> pod:     <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- bash\n</code></pre></p> </li> <li> <p>Inspect the current MON map and save the IP addresses of the failed Ceph     monitors for further usage:     <pre><code>ceph mon dump\n</code></pre></p> </li> <li> <p>Remove all entries of failed Ceph Monitors using the previously saved     letters:     <pre><code>ceph mon rm &lt;monLetter&gt;\n</code></pre></p> <p>Substitute <code>&lt;monLetter&gt;</code> with the corresponding letter of a failed Ceph Monitor.</p> </li> <li> <p>Exit the <code>pelagia-ceph-toolbox</code> pod.</p> </li> </ol> </li> <li> <p>Remove all failed Ceph Monitors entries from the Rook <code>mon</code> endpoints    ConfigMap:</p> <ol> <li> <p>Open the <code>rook-ceph/rook-ceph-mon-endpoints</code> ConfigMap for editing:     <pre><code>kubectl -n rook-ceph edit cm rook-ceph-mon-endpoints\n</code></pre></p> </li> <li> <p>Remove all entries of failed Ceph Monitors from the ConfigMap data and     update the <code>maxMonId</code> value with the current number of <code>Running</code>     Ceph Monitors. For example, <code>rook-ceph-mon-endpoints</code> has the     following <code>data</code>:     <pre><code>data:\n  csi-cluster-config-json: '[{\"clusterID\":\"rook-ceph\",\"monitors\":[\"172.0.0.222:6789\",\"172.0.0.223:6789\",\"172.0.0.224:6789\",\"172.16.52.217:6789\",\"172.16.52.216:6789\"]}]'\n  data: a=172.0.0.222:6789,b=172.0.0.223:6789,c=172.0.0.224:6789,f=172.0.0.217:6789,e=172.0.0.216:6789\n  mapping: '{\"node\":{\n      \"a\":{\"Name\":\"node-21465871-42d0-4d56-911f-7b5b95cb4d34\",\"Hostname\":\"node-21465871-42d0-4d56-911f-7b5b95cb4d34\",\"Address\":\"172.16.52.222\"},\n      \"b\":{\"Name\":\"node-43991b09-6dad-40cd-93e7-1f02ed821b9f\",\"Hostname\":\"node-43991b09-6dad-40cd-93e7-1f02ed821b9f\",\"Address\":\"172.16.52.223\"},\n      \"c\":{\"Name\":\"node-15225c81-3f7a-4eba-b3e4-a23fd86331bd\",\"Hostname\":\"node-15225c81-3f7a-4eba-b3e4-a23fd86331bd\",\"Address\":\"172.16.52.224\"},\n      \"e\":{\"Name\":\"node-ba3bfa17-77d2-467c-91eb-6291fb219a80\",\"Hostname\":\"node-ba3bfa17-77d2-467c-91eb-6291fb219a80\",\"Address\":\"172.16.52.216\"},\n      \"f\":{\"Name\":\"node-6f669490-f0c7-4d19-bf73-e51fbd6c7672\",\"Hostname\":\"node-6f669490-f0c7-4d19-bf73-e51fbd6c7672\",\"Address\":\"172.16.52.217\"}}\n  }'\n  maxMonId: \"5\"\n</code></pre></p> <p>If <code>e</code> and <code>f</code> are the letters of failed Ceph Monitors, the resulting   ConfigMap data must be as follows:   <pre><code>data:\n  csi-cluster-config-json: '[{\"clusterID\":\"rook-ceph\",\"monitors\":[\"172.0.0.222:6789\",\"172.0.0.223:6789\",\"172.0.0.224:6789\"]}]'\n  data: a=172.0.0.222:6789,b=172.0.0.223:6789,c=172.0.0.224:6789\n  mapping: '{\"node\":{\n      \"a\":{\"Name\":\"node-21465871-42d0-4d56-911f-7b5b95cb4d34\",\"Hostname\":\"node-21465871-42d0-4d56-911f-7b5b95cb4d34\",\"Address\":\"172.16.52.222\"},\n      \"b\":{\"Name\":\"node-43991b09-6dad-40cd-93e7-1f02ed821b9f\",\"Hostname\":\"node-43991b09-6dad-40cd-93e7-1f02ed821b9f\",\"Address\":\"172.16.52.223\"},\n      \"c\":{\"Name\":\"node-15225c81-3f7a-4eba-b3e4-a23fd86331bd\",\"Hostname\":\"node-15225c81-3f7a-4eba-b3e4-a23fd86331bd\",\"Address\":\"172.16.52.224\"}}\n  }'\n  maxMonId: \"3\"\n</code></pre></p> </li> </ol> </li> <li> <p>Back up the data of the failed Ceph Monitors one by one:</p> <ol> <li>SSH to the node of a failed Ceph Monitor using the previously saved IP     address.</li> <li> <p>Move the Ceph Monitor data directory to another place:     <pre><code>mv /var/lib/rook/mon-&lt;letter&gt; /var/lib/rook/mon-&lt;letter&gt;.backup\n</code></pre></p> </li> <li> <p>Close the SSH connection.</p> </li> </ol> </li> <li> <p>Scale the Rook Ceph Operator deployment up to <code>1</code> replica:    <pre><code>kubectl -n rook-ceph scale deploy rook-ceph-operator --replicas 1\n</code></pre></p> </li> <li> <p>Wait until all Ceph Monitors are in the <code>Running</code> state:    <pre><code>kubectl -n rook-ceph get pod -l app=rook-ceph-mon -w\n</code></pre></p> </li> <li> <p>Restore the data from the backup for each recovered Ceph Monitor one by one:</p> <ol> <li> <p>Enter a recovered Ceph Monitor pod:     <pre><code>kubectl -n rook-ceph exec -it &lt;monPodName&gt; bash\n</code></pre></p> <p>Substitute <code>&lt;monPodName&gt;</code> with the recovered Ceph Monitor pod name. For   example, <code>rook-ceph-mon-g-845d44b9c6-fjc5d</code>.</p> </li> <li> <p>Recover the <code>mon</code> data backup for the current Ceph Monitor:     <pre><code>ceph-monstore-tool /var/lib/rook/mon-&lt;letter&gt;.backup/data store-copy /var/lib/rook/mon-&lt;letter&gt;/data/\n</code></pre></p> <p>Substitute <code>&lt;letter&gt;</code> with the current Ceph Monitor pod letter, for example, <code>e</code>.</p> </li> </ol> </li> <li> <p>Verify the Ceph state. The output must indicate the desired number of Ceph    Monitors, and all of them must be in quorum.    <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- ceph -s\n</code></pre></p> </li> </ol>"},{"location":"troubleshoot/ceph-mon-store-size-grow/","title":"Ceph Monitor size rapidly grows","text":""},{"location":"troubleshoot/ceph-mon-store-size-grow/#ceph-monitors-storedb-size-rapidly-growing","title":"Ceph Monitors store.db size rapidly growing","text":"<p>The <code>MON_DISK_LOW</code> Ceph Cluster health message indicates that the <code>store.db</code> size of the Ceph Monitor is rapidly growing and the <code>compaction</code> procedure is not working. In most cases, <code>store.db</code> starts storing a number of <code>logm</code> keys that are buffered due to Ceph OSD shadow errors.</p> <p>To verify whether the store.db size is rapidly growing:</p> <ol> <li> <p>Identify the Ceph Monitors <code>store.db</code> size:    <pre><code>for pod in $(kubectl get pods -n rook-ceph | grep mon | awk '{print $1}'); \\\ndo printf \"$pod:\\n\"; kubectl exec -n rook-ceph \"$pod\" -it -c mon -- \\\ndu -cms /var/lib/ceph/mon/ ; done\n</code></pre></p> </li> <li> <p>Repeat the previous step two or three times within the interval of 5\u201315    seconds.</p> </li> </ol> <p>If between the command runs the total size increases by more than 10 MB, perform the steps described below to resolve the issue.</p> <p>To apply the issue resolution:</p> <ol> <li> <p>Verify the original state of placement groups (PGs):    <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- ceph -s\n</code></pre></p> </li> <li> <p>Apply <code>clog_to_monitors</code> with the <code>false</code> value for all Ceph OSDs at    runtime:    <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- bash\nceph tell osd.* config set clog_to_monitors false\n</code></pre></p> </li> <li> <p>Restart Ceph OSDs one by one:</p> <ol> <li> <p>Restart one of the Ceph OSDs:     <pre><code>for pod in $(kubectl get pods -n rook-ceph -l app=rook-ceph-osd | \\\nawk 'FNR&gt;1{print $1}'); do printf \"$pod:\\n\"; kubectl -n rook-ceph \\\ndelete pod \"$pod\"; echo \"Continue?\"; read; done\n</code></pre></p> </li> <li> <p>Once prompted <code>Continue?</code>, first verify that rebalancing has finished     for the Ceph cluster, the Ceph OSD is <code>up</code> and <code>in</code>, and all PGs have     returned to their original state:     <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- ceph -s\nkubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- ceph osd tree\n</code></pre></p> <p>Once you are confident that the Ceph OSD restart and recovery is over, press <code>ENTER</code>.</p> </li> <li> <p>Restart the remaining Ceph OSDs.</p> <p>Note</p> <p>Periodically verify the Ceph Monitors <code>store.db</code> size:   <pre><code>for pod in $(kubectl get pods -n rook-ceph | grep mon | awk \\\n'{print $1}'); do printf \"$pod:\\n\"; kubectl exec -n rook-ceph \\\n\"$pod\" -it -c mon -- du -cms /var/lib/ceph/mon/ ; done\n</code></pre></p> </li> </ol> </li> </ol> <p>After some of the affected Ceph OSDs restart, Ceph Monitors will start decreasing the <code>store.db</code> size to the original 100\u2013300 MB. However, complete the restart of all Ceph OSDs.</p>"},{"location":"troubleshoot/ceph-osd-auth-failed/","title":"Ceph OSD auth failed","text":""},{"location":"troubleshoot/ceph-osd-auth-failed/#replaced-ceph-osd-fails-to-start-on-authorization","title":"Replaced Ceph OSD fails to start on authorization","text":"<p>In rare cases, when the replaced Ceph OSD has the same ID as the previous Ceph OSD and starts on a device with the same name as the previous Ceph OSD, Rook fails to update the keyring value, which is stored on a node in the corresponding host path. Thereby, Ceph OSD cannot start and fails with the following exemplary log output:</p> <pre><code>Defaulted container \"osd\" out of: osd, activate (init), expand-bluefs (init), chown-container-data-dir (init)\ndebug 2024-03-13T11:53:13.268+0000 7f8f790b4640 -1 monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [2]\ndebug 2024-03-13T11:53:13.268+0000 7f8f7a0b6640 -1 monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [2]\ndebug 2024-03-13T11:53:13.268+0000 7f8f798b5640 -1 monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [2]\nfailed to fetch mon config (--no-mon-config to skip)\n</code></pre> <p>To verify that the cluster is affected, compare the keyring values stored in the Ceph cluster and on a node in the corresponding host path:</p> <ol> <li> <p>Obtain the keyring of a Ceph OSD stored in the Ceph cluster:    <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- ceph auth get osd.&lt;ID&gt;\n</code></pre></p> <p>Substitute <code>&lt;ID&gt;</code> with the number of the required Ceph OSD.</p> <p>Example output:  <pre><code>[osd.3]\nkey = AQAcovBlqP4qHBAALK6943yZyazoup7nE1YpeQ==\ncaps mgr = \"allow profile osd\"\ncaps mon = \"allow profile osd\"\ncaps osd = \"allow *\"\n</code></pre></p> </li> <li> <p>Obtain the keyring value of the host path for the failed Ceph OSD:</p> <ol> <li>SSH on a node hosting the failed Ceph OSD.</li> <li>In <code>/var/lib/rook/rook-ceph</code>, search for a directory containing the     <code>keyring</code> and <code>whoami</code> files that have the number of failed Ceph OSD.     For example:     <pre><code># cat whoami\n3\n# cat keyring\n[osd.3]\nkey = AQD2k/BlcE+YJxAA/QsD/fIAL1qPrh3hjQ7AKQ==\n</code></pre></li> </ol> </li> </ol> <p>The cluster is affected if the keyring of the failed Ceph OSD on the host path differs from the one on the Ceph cluster. If so, proceed to fix them and unblock the failed Ceph OSD.</p> <p>To fix the keyring difference and unblock the Ceph OSD authorization:</p> <ol> <li> <p>Obtain the keyring value of the host path for this Ceph OSD:</p> <ol> <li>SSH on a node hosting the required Ceph OSD.</li> <li>In <code>/var/lib/rook/rook-ceph</code>, search for a directory containing     the <code>keyring</code> and <code>whoami</code> files that have the number of the     required Ceph OSD. For example:     <pre><code># cat whoami\n3\n# cat keyring\n[osd.3]\nkey = AQD2k/BlcE+YJxAA/QsD/fIAL1qPrh3hjQ7AKQ==\n</code></pre></li> </ol> </li> <li> <p>Enter the <code>pelagia-ceph-toolbox</code> pod:    <pre><code>kubectl -n rook-ceph exec -it deploy/pelagia-ceph-toolbox -- bash\n</code></pre></p> </li> <li> <p>Export the current Ceph OSD keyring stored in the Ceph cluster:    <pre><code>ceph auth get osd.&lt;ID&gt; -o /tmp/key\n</code></pre></p> </li> <li> <p>Replace the exported key with the value from <code>keyring</code>. For example:    <pre><code>vi /tmp/key\n# replace the key with the one from the keyring file\n[osd.3]\nkey = AQD2k/BlcE+YJxAA/QsD/fIAL1qPrh3hjQ7AKQ==\ncaps mgr = \"allow profile osd\"\ncaps mon = \"allow profile osd\"\ncaps osd = \"allow *\"\n</code></pre></p> </li> <li> <p>Import the replaced Ceph OSD keyring to the Ceph cluster:    <pre><code>ceph auth import -i /tmp/key\n</code></pre></p> </li> <li> <p>Restart the failed Ceph OSD pod:    <pre><code>kubectl -n rook-ceph scale deploy rook-ceph-osd-&lt;ID&gt; --replicas 0\nkubectl -n rook-ceph scale deploy rook-ceph-osd-&lt;ID&gt; --replicas 1\n</code></pre></p> </li> </ol>"},{"location":"troubleshoot/cephosdremovetask-timeout/","title":"CephOsdRemoveTask rebalance timeout","text":""},{"location":"troubleshoot/cephosdremovetask-timeout/#cephosdremovetask-failure-with-a-timeout-during-rebalance","title":"CephOsdRemoveTask failure with a timeout during rebalance","text":"<p>Ceph OSD removal procedure includes the Ceph OSD <code>out</code> action that starts the Ceph PGs rebalancing process. The total time for rebalancing depends on a cluster hardware configuration: network bandwidth, Ceph PGs placement, number of Ceph OSDs, and so on. The default rebalance timeout is limited by <code>30</code> minutes, which applies to standard cluster configurations.</p> <p>If the rebalance takes more than 30 minutes, the <code>CephOsdRemoveTask</code> resources created for removing Ceph OSDs or nodes fail with the following example message:</p> <pre><code>status:\n  messages:\n  - Timeout (30m0s) reached for waiting pg rebalance for osd 2\n</code></pre> <p>To apply the issue resolution, increase the timeout for all future <code>CephOsdRemoveTask</code> resources:</p> <p>Update <code>pelagia-lcmconfig</code> ConfigMap in Pelagia namespace with the key <code>TASK_OSD_PG_REBALANCE_TIMEOUT_MIN</code> and desired timeout in minutes in string format:</p> <pre><code>kubectl -n pelagia edit cm pelagia-lcmconfig\n</code></pre> <p>Example configuration: <pre><code>data:\n  TASK_OSD_PG_REBALANCE_TIMEOUT_MIN: \"180\"\n</code></pre></p> <p>where <code>\"180\"</code> means 180 minutes before timeout.</p> <p>If you have an existing <code>CephOsdRemoveTask</code> resource with issues in <code>messages</code> to process:</p> <ol> <li>In the failed <code>CephOsdRemoveTask</code> resource, copy the <code>spec</code> section.</li> <li>Create a new <code>CephOsdRemoveTask</code> with a different name. For details,    see Creating a Ceph OSD remove task.</li> <li>Paste the previously copied <code>spec</code> section of the failed    <code>CephOsdRemoveTask</code> resource to the new one.</li> <li>Remove the failed <code>CephOsdRemoveTask</code> resource.</li> </ol>"}]}